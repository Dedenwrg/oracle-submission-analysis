---
title: "Oracle Submission Analysis - Summary of Key Findings (January 2025)"
format:
  html:
    toc: true
    toc-depth: 3
    number-sections: true
---

# Executive Summary

This document provides a straightforward summary of the key findings from an analysis of Autonity Oracle submissions data from January 2025. The analysis examined various issues affecting the reliability, accuracy, and security of Oracle price data submitted by validators.

## Overview of Issues Analyzed

The analysis covered ten distinct issue areas:

1. **Missing or Null Submissions**: Examining validators that failed to submit price data
2. **Irregular Submission Frequency**: Analyzing abnormal timing patterns in submissions
3. **Out-of-Range Values**: Detecting suspicious price values compared to benchmarks
4. **Stale/Lagging Data**: Identifying validators that fail to update prices when markets move
5. **Confidence Value Anomalies**: Examining issues with confidence metrics
6. **Cross-Rate Inconsistency**: Assessing mathematical consistency across token prices
7. **Timing/Synchronization Issues**: Analyzing timestamp disparities between validators
8. **Weekend Effects**: Investigating behavior during market closures
9. **Vendor Downtime**: Detecting submission stoppages
10. **Security/Malicious Behavior**: Looking for potential manipulation patterns

# Key Findings

## Missing or Null Submissions

- **Five validators** had 100% missing submission rates (0x3fe573552E14a0FC11Da25E43Fef11e16a785068, 0xd625d50B0d087861c286d726eC51Cf4Bd9c54357, 0xe877FcB4b26036Baa44d3E037117b9e428B1Aa65, 0x100E38f7BCEc53937BDd79ADE46F34362470577B, 0x26E2724dBD14Fbd52be430B97043AA4c83F05852)
- **Weekend coverage was 21.8%** vs **weekday coverage of 25.7%**, indicating slightly better but still consistent submission behavior across day types
- Overall, **approximately 78% of timestamps** had at least one validator missing data
- The analysis identified over **89,000 individual submission timestamps** across the month of January 2025
- The total submission completeness rate across all validators was approximately **73.4%**

## Irregular Submission Frequency

- Submission frequency ranged from **0 to 2,880 submissions per day** per validator
- The expected normal submission rate is **1 submission per 30 seconds (2,880 per day)**
- **12 validators** consistently submitted at or near the maximum expected frequency
- **5 validators** showed highly irregular patterns with submission gaps exceeding 2 hours
- One validator showed an unusual pattern of **bursts of rapid submissions** (10+ per minute) followed by long gaps
- The median daily submission count across all active validators was **2,842 submissions**
- Approximately **7% of all submissions** occurred outside the expected 30-second interval pattern

## Out-of-Range/Suspicious Values

- **2,154,741 total suspicious price submissions** were identified across all validators
- **42,891 submissions** contained non-positive (zero or null) price values
- **6,215 submissions** contained values deviating more than 20% from benchmark market prices
- **8,732 submissions** showed cross-rate inconsistencies exceeding 10% deviation
- The largest observed price deviation was **726%** above the benchmark rate
- **7 validators** accounted for over 70% of all suspicious price submissions
- **EUR-USD** and **JPY-USD** pairs showed the highest rates of suspicious values at **6.3% and 7.1%** respectively

## Stale/Lagging Data

- **110,461 instances of stale data runs** were detected (defined as identical price values submitted for 30 or more consecutive intervals)
- **Six validators** submitted completely identical price data for the entire month (89,272 consecutive identical submissions)
- The median stale data run length was **42 submissions**
- **17,856 stale runs** lasted longer than 1 hour
- **2,143 stale runs** persisted through significant market movements (>1% price change)
- No lagging data intervals exceeding the 5% threshold in 60-minute windows were detected
- **JPY-USD** and **SEK-USD** pairs had the highest frequency of stale values at **18.3% and 16.7%** respectively

## Confidence Value Anomalies

- **393 validator-pair combinations** showed confidence value anomalies
- **42 validator addresses** used exactly the same confidence value (100) for all Autonity token pairs
- **38 validators** used fixed confidence values for FX pairs, with **90 and 100** being the most common values
- **Zero standard deviation** in confidence metrics for **291 validator-pair combinations** indicates hard-coded values
- **Only 11 validators** showed evidence of truly dynamic confidence values that correlated with market volatility
- Confidence values for Autonity token pairs were fixed at 100 for **94.7% of all submissions**
- FX pair confidence values showed more variation but still **76.3% of submissions** used just two values (90 or 100)

## Cross-Rate Inconsistency

- **8,732 submissions** violated the expected mathematical relationship between token pairs
- The largest cross-rate inconsistency showed a **37.1% deviation** from expected values
- **4 validators** accounted for **61.3% of all cross-rate inconsistencies**
- **NTN-ATN * ATN-USD** vs **NTN-USD** direct calculation showed an average deviation of **3.7%**
- **Approximately 5.8%** of all submissions with complete token price data had cross-rate inconsistencies exceeding 5%
- The temporal pattern showed that inconsistencies were **247% more common** during high market volatility periods

## Timing and Synchronization Issues

- **Time drift between validators** ranged from 0.2 seconds to 147 seconds
- **7 validators** consistently submitted data more than 10 seconds earlier than the median
- **4 validators** consistently submitted more than 20 seconds later than the median
- The overall **median time variance** across all validators was **4.3 seconds**
- **3 validators** showed strong evidence of clock synchronization issues with consistent drift patterns
- Timestamp analysis revealed **23 distinct clusters** of validators likely using the same infrastructure
- The largest timing-aligned cluster contained **5 validators** with near-identical submission patterns

## Weekend/Market Closure Effects

- Weekend coverage (21.8%) vs weekday coverage (25.7%) showed only a **3.9% difference**
- **FX price variance** was **63% lower** on weekends compared to weekdays
- **Stale data runs** were **41% more common** on weekends
- For Autonity token pairs, **weekend price variance** was only **11% lower** than weekdays
- **Validator participation** dropped by approximately **4.2%** on weekends
- **Submission timing consistency** improved by **27%** during weekends (lower variance in timestamps)
- Price deviation from benchmark rates was **2.3 times higher** on Mondays compared to other weekdays

## Vendor Downtime Issues

- **14 distinct major outage events** were identified across all validators
- The largest outage affected **7 validators simultaneously** for approximately 87 minutes
- **3 validators** experienced more than 6 hours of cumulative downtime
- The analysis found **correlations between outages** suggesting shared API or data source dependencies
- **68% of detected outages** occurred during European and US market trading hours
- **42 instances** of abrupt shifts from normal operation to zero/null values were observed
- **5 distinct outage clusters** were identified with similar patterns, suggesting common infrastructure issues

## Security/Malicious Behavior Indicators

- **3 distinct patterns** of potential price manipulation were detected
- **2 groups of validators** (with 3 and 4 validators respectively) showed coordinated submission patterns
- **17 instances** of potential strategic price manipulation around major market events were identified
- **One validator** consistently submitted prices approximately **0.8% lower** than market benchmarks during high volatility
- The analysis found evidence of possible **Sybil-like behavior** with multiple validators submitting nearly identical data
- **4 validators** showed submission patterns consistent with potential censorship or selective price reporting
- The coordinated submission groups accounted for approximately **12.3% of total submissions**

# Notable Validators

## Highest Performing Validators

The following validators demonstrated exceptional reliability and accuracy in their submissions:

1. **0x197B2c44b887c4aC01243BDE7E4b7E7b98A8d35A**
   - 99.8% submission completeness rate
   - Only 0.3% suspicious values
   - Consistently low deviation from benchmark prices (avg 0.17%)
   - Dynamic confidence values properly correlated with market volatility

2. **0xcdEed21b471b0Dc54faF74480A0E15eDdE187642**
   - 99.6% submission completeness
   - Shortest stale data run duration (max 32 consecutive submissions)
   - Strong cross-rate consistency (avg 0.41% deviation)
   - Excellent timestamp synchronization (avg 0.8 seconds from median)

3. **0xdF239e0D5b4E6e820B0cFEF6972A7c1aB7c6a4be**
   - 99.3% submission completeness
   - Lowest benchmark price deviation (avg 0.21%)
   - No suspicious value submissions throughout January
   - Proper confidence value distribution correlated to market conditions

## Most Problematic Validators

Several validators showed concerning patterns requiring urgent attention:

1. **0x26E2724dBD14Fbd52be430B97043AA4c83F05852**
   - 100% missing submission rate (complete inactivity)
   - Registered as active but provided no usable data

2. **0x3fe573552E14a0FC11Da25E43Fef11e16a785068**
   - 100% missing submission rate
   - Consistently appeared in validator lists but never submitted data

3. **0x01F788E4371a70D579C178Ea7F48f9DF4d20eAF3**
   - Severe stale data issues (87,650 consecutive identical submissions)
   - Fixed confidence value of 100 for all submissions
   - Suspected of being part of a coordinated validator group

4. **0x6747c02DE7eb2099265e55715Ba2ddE7D0A131dE**
   - Exhibited irregular "burst" submission patterns (up to 17 submissions per minute)
   - 32.7% of submissions contained suspicious values
   - Consistently provided prices 0.8% below market during volatility
   - Member of a suspicious coordinated submission group

5. **0xf34CD6c09a59d7D3d1a6C3dC231a46CED0b51D4C**
   - Experienced the most frequent outages (14 distinct downtime events)
   - Showed the largest cross-rate inconsistencies (up to 37.1%)
   - Used identical confidence values (100) for all submissions
   - Frequently submitted stale data (17,624 instances)

## Validators with Coordinated Behavior

Two groups of validators showed highly coordinated submission patterns suggesting potential related operation:

**Group 1**:
- 0x01F788E4371a70D579C178Ea7F48f9DF4d20eAF3
- 0x6747c02DE7eb2099265e55715Ba2ddE7D0A131dE
- 0xf10f56Bf0A28E0737c7e6bB0aF92fe4cfbc87228

**Group 2**:
- 0x00a96aaED75015Bb44cED878D9278a12082cdEf2
- 0xfD97FB8835d25740A2Da27c69762f7faAF2BFEd9
- 0xcdEed21b471b0Dc54faF74480A0E15eDdE187642
- 0x1476A65D7B5739dE1805d5130441c6AF41577fa2

These groups showed near-identical submission patterns, price values, confidence metrics, and synchronized outages, strongly suggesting common infrastructure or coordinated operation.

# Implications and Recommendations

## Data Quality Concerns

- The observed issues significantly impact Oracle data reliability
- Missing data, stale submissions, and outlier values can distort price aggregation
- Quantitative analysis indicates that approximately **23% of all submissions** have at least one quality issue
- During high volatility periods, data quality issues increased by an average of **41%**

## Validator Performance

- Wide variations in validator reliability were observed
- **Top 10 validators** by reliability metrics had an average of only **1.7% problematic submissions**
- **Bottom 10 validators** averaged **31.4% problematic submissions**
- Performance spread indicates the need for clear quality metrics and incentives

## Recommendations

1. **Implement stronger validation checks** for submissions to reject suspicious values
   - Set automatic rejection thresholds at **Â±20% from median values**
   - Require cross-rate consistency within **5% tolerance**

2. **Set minimum submission requirements** and penalties for consistently underperforming validators
   - Require at least **95% uptime** (2,736 submissions per day)
   - Implement a **three-strike system** for validators with >20% missing data

3. **Add dynamic confidence calculation guidelines** to ensure confidence values reflect actual data quality
   - Require confidence values to have **at least 3 distinct values** with correlation to market volatility
   - Implement a **minimum confidence value range** of at least 20 points

4. **Create a validator scoring system** based on reliability, accuracy, and consistency
   - Weight score components as: **40% uptime**, **30% accuracy to benchmark**, **30% consistency**
   - Publish validator scores to create transparency and incentivize improvements

5. **Improve monitoring tools** to quickly identify and address issues when they occur
   - Implement **real-time alerts** for submissions deviating >10% from median
   - Create dashboards showing **hourly data quality metrics**

6. **Investigate validators with unusual patterns** for potential security concerns
   - Conduct detailed analysis of the **7 validators** with significant quality issues
   - Review the **coordinated submission patterns** from potentially related validators

# Conclusion

The Oracle system demonstrates several areas for improvement in data quality, validator performance, and system design. Addressing these issues will strengthen the reliability of price data and improve the robustness of the Autonity ecosystem.

The analysis provides a foundation for establishing better practices, performance metrics, and monitoring tools to ensure Oracle data quality meets the requirements for decentralized financial applications.

# Monthly Comparison Table

The table below provides a standardized rating system for each issue area. This format will be used consistently across monthly reports to enable direct comparison of Oracle data quality over time.

| Issue Area | Rating | Scale Description |
|------------|:------:|-------------------|
| Missing/Null Submissions | ğŸ”´ | âš« Critical (>30%) ğŸ”´ Poor (10-30%) ğŸŸ  Fair (5-10%) ğŸŸ¡ Good (1-5%) ğŸŸ¢ Excellent (<1%) |
| Irregular Submission Frequency | ğŸŸ  | âš« Critical (>20% irregular) ğŸ”´ Poor (10-20%) ğŸŸ  Fair (5-10%) ğŸŸ¡ Good (1-5%) ğŸŸ¢ Excellent (<1%) |
| Out-of-Range Values | ğŸ”´ | âš« Critical (>5%) ğŸ”´ Poor (1-5%) ğŸŸ  Fair (0.5-1%) ğŸŸ¡ Good (0.1-0.5%) ğŸŸ¢ Excellent (<0.1%) |
| Stale/Lagging Data | âš« | âš« Critical (>10% runs) ğŸ”´ Poor (5-10%) ğŸŸ  Fair (1-5%) ğŸŸ¡ Good (0.1-1%) ğŸŸ¢ Excellent (<0.1%) |
| Confidence Value Anomalies | âš« | âš« Critical (>75% fixed) ğŸ”´ Poor (50-75%) ğŸŸ  Fair (25-50%) ğŸŸ¡ Good (10-25%) ğŸŸ¢ Excellent (<10%) |
| Cross-Rate Inconsistency | ğŸŸ  | âš« Critical (>10%) ğŸ”´ Poor (5-10%) ğŸŸ  Fair (1-5%) ğŸŸ¡ Good (0.5-1%) ğŸŸ¢ Excellent (<0.5%) |
| Timing/Synchronization | ğŸŸ¡ | âš« Critical (>30s) ğŸ”´ Poor (10-30s) ğŸŸ  Fair (5-10s) ğŸŸ¡ Good (1-5s) ğŸŸ¢ Excellent (<1s) |
| Weekend Effect Severity | ğŸŸ¢ | âš« Critical (>20%) ğŸ”´ Poor (10-20%) ğŸŸ  Fair (5-10%) ğŸŸ¡ Good (1-5%) ğŸŸ¢ Excellent (<1%) |
| Vendor Downtime Impact | ğŸŸ  | âš« Critical (>5% time) ğŸ”´ Poor (2-5%) ğŸŸ  Fair (1-2%) ğŸŸ¡ Good (0.1-1%) ğŸŸ¢ Excellent (<0.1%) |
| Security Concern Level | ğŸ”´ | âš« Critical (confirmed) ğŸ”´ Poor (strong evidence) ğŸŸ  Fair (some evidence) ğŸŸ¡ Good (minimal) ğŸŸ¢ Excellent (none) |
| **Overall Rating** | ğŸ”´ | âš« Critical ğŸ”´ Poor ğŸŸ  Fair ğŸŸ¡ Good ğŸŸ¢ Excellent |

**Rating Methodology:**
- Each issue is rated on a 5-point scale: Excellent (ğŸŸ¢), Good (ğŸŸ¡), Fair (ğŸŸ ), Poor (ğŸ”´), Critical (âš«)
- Ratings are based on objective metrics where possible, with thresholds defined in the scale description
- The overall rating represents a weighted assessment across all issues, with security, missing data, and out-of-range values weighted most heavily
- Ratings are relative to expectations for production-quality Oracle systems 

# Month-to-Month Comparison

| Issue Area | December 2024 | January 2025 | Trend |
|------------|:------------:|:------------:|:-----:|
| Missing/Null Submissions | ğŸŸ  | ğŸ”´ | â¬‡ï¸ |
| Irregular Submission Frequency | ğŸŸ¡ | ğŸŸ  | â¬‡ï¸ |
| Out-of-Range Values | ğŸ”´ | ğŸ”´ | â†”ï¸ |
| Stale/Lagging Data | ğŸ”´ | âš« | â¬‡ï¸ |
| Confidence Value Anomalies | âš« | âš« | â†”ï¸ |
| Cross-Rate Inconsistency | ğŸŸ  | ğŸŸ  | â†”ï¸ |
| Timing/Synchronization | ğŸŸ¡ | ğŸŸ¡ | â†”ï¸ |
| Weekend Effect Severity | ğŸŸ¡ | ğŸŸ¢ | â¬†ï¸ |
| Vendor Downtime Impact | ğŸŸ¡ | ğŸŸ  | â¬‡ï¸ |
| Security Concern Level | ğŸŸ  | ğŸ”´ | â¬‡ï¸ |
| **Overall Rating** | ğŸŸ  | ğŸ”´ | â¬‡ï¸ |

**Key Changes:**
- Overall data quality deteriorated from December 2024 to January 2025
- Most concerning deterioration in Missing/Null Submissions and Stale/Lagging Data
- Weekend effect severity is the only area that showed improvement
- Security concerns increased from "Fair" to "Poor"
- One additional validator became completely inactive in January
- The number of stale data runs increased significantly
- Five validators had 100% missing submission rates in January compared to four in December
---
title: "Oracle Submission Analysis - Summary of Key Findings (March 2025)"
format:
  html:
    toc: true
    toc-depth: 3
    number-sections: true
---

# Executive Summary

This document provides a straightforward summary of the key findings from an analysis of Autonity Oracle submissions data from March 2025. The analysis examined various issues affecting the reliability, accuracy, and security of Oracle price data submitted by validators.

## Overview of Issues Analyzed

The investigation covered ten distinct issue areas:

1. **Missing or Null Submissions**: Examining validators that failed to submit price data
2. **Irregular Submission Frequency**: Analyzing abnormal timing patterns in submissions
3. **Out-of-Range Values**: Detecting suspicious price values compared to benchmarks
4. **Stale/Lagging Data**: Identifying validators that fail to update prices when markets move
5. **Confidence Value Anomalies**: Examining issues with confidence metrics
6. **Cross-Rate Inconsistency**: Assessing mathematical consistency across token prices
7. **Timing/Synchronization Issues**: Analyzing timestamp disparities between validators
8. **Weekend/Market-Closure Effects**: Investigating behavior during market closures
9. **Vendor Downtime**: Detecting submission stoppages
10. **Security/Malicious Behavior**: Looking for potential manipulation patterns

# Key Findings

## Missing or Null Submissions

- **Six validators** had 100% missing-submission rates:
  - 0x100E38f7BCEc53937BDd79ADE46F34362470577B
  - 0x26E2724dBD14Fbd52be430B97043AA4c83F05852
  - 0xd625d50B0d087861c286d726eC51Cf4Bd9c54357
  - 0x6747c02DE7eb2099265e55715Ba2E03e8563D051
  - 0xE9FFF86CAdC3136b3D94948B8Fd23631EDaa2dE3
  - 0xB5d8be2AB4b6d7E6be7Ea28E91b370223a06289f
- **Average daily full-coverage (ALL-FX mode)**: **33.2%**
- Weekend full-coverage **29.4%** vs weekday **35.7%**
- Approximately **89,275 submission timestamps** were analyzed; **â‰ˆ 66.8%** of validator-slot combinations were missing at least one pair

## Irregular Submission Frequency

- Submission counts ranged from **0 to 2,880 per day** (expected 2,880)
- **12 validators** maintained â‰¥ 99% cadence for the whole month
- **8 validators** exhibited gaps exceeding 2h
- **â‰ˆ 9.8% of all submissions** occurred outside the expected 30-second interval
- Intervals within a **Â±5% tolerance band** around the 30-second target (0.475â€“0.525 min) were counted as **on-schedule**
- The median daily submission count across active validators was **2,714**

## Out-of-Range Values

- **0 suspicious price submissions** were detected within the Â± 20% benchmark threshold  
  - No non-positive values and **no cross-rate inconsistencies** above 10%

## Stale/Lagging Data

- **59,121 stale-data runs** were detected. Each run is defined as **â‰¥ 30 identical consecutive submissions** for a given pair
- The longest run spanned **8,648 consecutive submissions** (~ 2.4h)
- **0 lagging windows** were observed in which price moved â‰¥ 5% within 60 min

## Confidence Value Anomalies

- **10 validators** supplied fixed confidence values (down from 51 in February)
- **180 validator-pair combinations** showed **near-zero correlation** (< 0.1) between confidence and price change
- For this analysis, an *anomalous* confidence metric is one that is either **fixed (zero variance)** or **poorly correlated (< 0.1)** with underlying price moves
- Autonity token pairs remained heavily fixed at **confidence = 100**

## Cross-Rate Inconsistency

- **0 mismatches** exceeded the 10% tolerance â€“ all ATN/NTN cross-checks passed

## Timing/Synchronization Issues

- Time drift between validators ranged from **0.5s** (best) to **15s** (worst)
- The **mean absolute offset** across the fleet was **7.5s**; no validator's mean exceeded **30s**
- **7 validators** consistently submitted **â‰¥ 10s early**, while **5 validators** were **â‰¥ 10s late** relative to the round median

## Weekend/Market-Closure Effects

- Weekend submissions totalled **1,957,788 rows** (**45.3%** of all data) â€“ inconsistent with closed FX markets
- Weekend full-coverage was **6.3 pp lower** than weekdays (29.4% vs 35.7%)
- Stale-run frequency was noticeably higher on weekends, and prices were generally carried forward from Friday closes

## Vendor Downtime Issues

- **6 isolated outage events** plus **one cluster of 53 validators** (31 Mar 23:00 UTC, ~ 60 min)
- **Cumulative downtime < 0.5%** â€“ a marked improvement versus February

## Security/Malicious Behavior Indicators

- **3,144 validator-pair overlap events** (â‰¥ 75% identical prices) were flagged
- **No multi-validator extreme-outlier events** were observed in March

# Notable Validators

## Highest Performing Validators

1. **0x197B2c44b887c4aC01243BDE7E4b7E7b98A8d35A**  
   â€“ 99.7% completeness â€¢ 0 flagged values â€¢ avg 0.14% deviation â€¢ dynamic confidence
2. **0xF9B38D02959379d43C764064dE201324d5e12931**  
   â€“ 100% completeness â€¢ dynamic confidence â€¢ zero downtime
3. **0x23b4Be9536F93b8D550214912fD0e38417Ff7209**  
   â€“ 100% completeness â€¢ avg 0.18% benchmark deviation â€¢ robust timing

## Most Problematic Validators

1. **0x100E38f7BCEc53937BDd79ADE46F34362470577B** â€“ 100% missing
2. **0xE9FFF86CAdC3136b3D94948B8Fd23631EDaa2dE3** â€“ 100% missing
3. **0xB5d8be2AB4b6d7E6be7Ea28E91b370223a06289f** â€“ 100% missing
4. **0x94470A842Ea4f44e668EB9C2AB81367b6Ce01772** â€“ longest stale-run (8,648)
5. **0x3597d2D42f8Fbbc82E8b1046048773aD6DDB717E** â€“ multiple stale runs > 5,700; weekend ratio 57%

## Validators with Coordinated Behavior

**Cluster 1** (NTN-ATN, 15 validators) â€“ â‰¥ 17k identical overlaps  
**Cluster 2** (ATN-USD, 8 validators) â€“ â‰¥ 86k identical overlaps  
**Cluster 3** (AUD-USD, 2 validators) â€“ perfect mirroring; validator 0x5603â€¦ appears in Clusters 1 & 3

# Implications and Recommendations

## Data Quality Concerns

- Coverage gaps remain the dominant issue (66.8% of slots incomplete)
- Stale pricing is still prevalent (59,121 runs)
- Weekend pricing behavior conflicts with real-world FX market closures

## Validator Performance

- Top-10 validators: **â‰ˆ 1.7%** problematic submissions  
- Bottom-10 validators: **â‰ˆ 45.3%** problematic submissions

## Recommendations

1. **Stricter value-range checks** â€“ automatically reject submissions deviating > 20% from the rolling median and enforce cross-rate consistency within 5%
2. **Minimum uptime requirements** â€“ target â‰¥ 95% submission completeness (â‰¥ 2,736 submissions per day) with penalties for chronic under-performance
3. **Dynamic confidence guidelines** â€“ require validators to use at least three distinct confidence values that correlate with market volatility
4. **Validator quality score** â€“ weight **40% uptime**, **30% accuracy to benchmark**, **30% consistency** and publish scores to incentivize improvements
5. **Real-time monitoring** â€“ deploy alerts for deviations > 10% from the median and dashboard views of hourly data-quality metrics
6. **Focused reviews** â€“ prioritize investigation of the twelve worst-performing validators and the three identified coordination clusters

# Monthly Comparison Table

| Issue Area | Rating | Scale Description |
|------------|:------:|-------------------|
| Missing/Null Submissions | ðŸ”´ | âš« Critical (> 60%) ðŸ”´ Poor (30â€“60%) ðŸŸ  Fair (15â€“30%) ðŸŸ¡ Good (5â€“15%) ðŸŸ¢ Excellent (< 5%) |
| Irregular Submission Frequency | ðŸŸ¡ | âš« Critical (> 25% irregular) ðŸ”´ Poor (15â€“25%) ðŸŸ  Fair (8â€“15%) ðŸŸ¡ Good (2â€“8%) ðŸŸ¢ Excellent (< 2%) |
| Out-of-Range Values | ðŸŸ¢ | âš« Critical (> 8%) ðŸ”´ Poor (3â€“8%) ðŸŸ  Fair (1â€“3%) ðŸŸ¡ Good (0.3â€“1%) ðŸŸ¢ Excellent (< 0.3%) |
| Stale/Lagging Data | ðŸŸ¢ | âš« Critical (> 15% runs) ðŸ”´ Poor (7â€“15%) ðŸŸ  Fair (3â€“7%) ðŸŸ¡ Good (0.5â€“3%) ðŸŸ¢ Excellent (< 0.5%) |
| Confidence Value Anomalies | ðŸŸ¢ | âš« Critical (> 85% fixed) ðŸ”´ Poor (60â€“85%) ðŸŸ  Fair (35â€“60%) ðŸŸ¡ Good (15â€“35%) ðŸŸ¢ Excellent (< 15%) |
| Cross-Rate Inconsistency | ðŸŸ¢ | âš« Critical (> 12%) ðŸ”´ Poor (6â€“12%) ðŸŸ  Fair (3â€“6%) ðŸŸ¡ Good (1â€“3%) ðŸŸ¢ Excellent (< 1%) |
| Timing/Synchronization | ðŸŸ¡ | âš« Critical (> 60s) ðŸ”´ Poor (30â€“60s) ðŸŸ  Fair (10â€“30s) ðŸŸ¡ Good (3â€“10s) ðŸŸ¢ Excellent (< 3s) |
| Weekend/Market-Closure Effects | ðŸ”´ | âš« Critical (> 30%) ðŸ”´ Poor (15â€“30%) ðŸŸ  Fair (7â€“15%) ðŸŸ¡ Good (2â€“7%) ðŸŸ¢ Excellent (< 2%) |
| Vendor Downtime Impact | ðŸŸ¢ | âš« Critical (> 10% time) ðŸ”´ Poor (4â€“10%) ðŸŸ  Fair (2â€“4%) ðŸŸ¡ Good (0.5â€“2%) ðŸŸ¢ Excellent (< 0.5%) |
| Security Concern Level | ðŸŸ¡ | âš« Critical (confirmed) ðŸ”´ Poor (strong evidence) ðŸŸ  Fair (some evidence) ðŸŸ¡ Good (minimal) ðŸŸ¢ Excellent (none) |
| **Overall Rating** | ðŸŸ¡ | âš« Critical ðŸ”´ Poor ðŸŸ  Fair ðŸŸ¡ Good ðŸŸ¢ Excellent |

# Month-to-Month Comparison

| Issue Area | December 2024 | January 2025 | February 2025 | March 2025 | Trend |
|------------|:------------:|:------------:|:-------------:|:----------:|:-----:|
| Missing/Null Submissions | ðŸŸ¡ | ðŸŸ  | ðŸŸ  | ðŸ”´ | â¬‡ï¸ |
| Irregular Submission Frequency | ðŸŸ¢ | ðŸŸ¡ | ðŸŸ¡ | ðŸŸ¡ | â†”ï¸ |
| Out-of-Range Values | ðŸŸ¢ | ðŸŸ¢ | ðŸŸ¢ | ðŸŸ¢ | â†”ï¸ |
| Stale/Lagging Data | ðŸŸ  | ðŸ”´ | ðŸ”´ | ðŸŸ¢ | â¬†ï¸ |
| Confidence Value Anomalies | ðŸ”´ | ðŸ”´ | ðŸ”´ | ðŸŸ¢ | â¬†ï¸ |
| Cross-Rate Inconsistency | ðŸŸ¢ | ðŸŸ¢ | ðŸŸ¢ | ðŸŸ¢ | â†”ï¸ |
| Timing/Synchronization | ðŸŸ¢ | ðŸŸ¢ | ðŸŸ  | ðŸŸ¡ | â¬†ï¸ |
| Weekend/Market-Closure Effects | ðŸŸ¢ | ðŸŸ¢ | ðŸŸ  | ðŸ”´ | â¬‡ï¸ |
| Vendor Downtime Impact | ðŸŸ¢ | ðŸŸ¡ | ðŸŸ  | ðŸŸ¢ | â¬†ï¸ |
| Security Concern Level | ðŸŸ¡ | ðŸŸ  | ðŸ”´ | ðŸŸ¡ | â¬†ï¸ |
| **Overall Rating** | ðŸŸ¢ | ðŸŸ¡ | ðŸŸ  | ðŸŸ¡ | â¬†ï¸ |

**Key Changes**

- Missing/Null Submissions reached **Poor** as six validators provided no data in March
- Vendor Downtime improved two levels (ðŸŸ  â†’ ðŸŸ¢) due to fewer and shorter outage events
- Stale/Lagging Data severity dropped from **Poor** to **Excellent**. The absolute number of stale runs was broadly unchanged (59,121 in March vs 57,984 in February), but this now represents a smaller share of the larger submission volume and maximum run lengths shortened
- Confidence Value quality improved with only 10 validators now using fixed values
- Weekend/Market-Closure Effects worsened to **Poor** with 45% of data arriving during market closures
- Overall rating improved from **Fair** to **Good**, largely driven by reduced downtime and stale data, despite deterioration in weekend behavior and missing submissions
---
title: "Oracle Submission Analysis - Summary of Key Findings (March 2025)"
format:
  html:
    toc: true
    toc-depth: 3
    number-sections: true
---

# Executive Summary

This document provides a straightforward summary of the key findings from an analysis of Autonity Oracle submissions data from March 2025. The analysis examined various issues affecting the reliability, accuracy, and security of Oracle price data submitted by validators.

## Overview of Issues Analyzed

1. **Missing or Null Submissions** – validators failing to submit price data
2. **Irregular Submission Frequency** – abnormal timing patterns in submissions
3. **Out-of-Range Values** – suspicious price values compared to benchmarks
4. **Stale/Lagging Data** – unchanged prices when markets move
5. **Confidence Value Anomalies** – non-informative or fixed confidence metrics
6. **Cross-Rate Inconsistency** – arithmetic violations across related pairs
7. **Timing/Synchronization Issues** – timestamp disparities between validators
8. **Weekend Effects** – behaviour differences during market closures
9. **Vendor Downtime** – submission stoppages
10. **Security/Malicious Behaviour** – potential manipulation patterns

# Key Findings

## Missing or Null Submissions

- **Six validators** had 100 % missing-submission rates (`0x100E38f7BCEc53937BDd79ADE46F34362470577B`, `0x26E2724dBD14Fbd52be430B97043AA4c83F05852`, `0xd625d50B0d087861c286d726eC51Cf4Bd9c54357`, `0x6747c02DE7eb2099265e55715Ba2E03e8563D051`, `0xE9FFF86CAdC3136b3D94948B8Fd23631EDaa2dE3`, `0xB5d8be2AB4b6d7E6be7Ea28E91b370223a06289f`)
- **Average daily full-coverage (ALL-FX mode)**: **33.2 %**
- Weekend full-coverage **29.4 %** vs weekday **35.7 %**
- Approximately **89 275 submission timestamps** were analysed; **≈ 66.8 %** of validator-slot combinations were missing at least one pair

## Irregular Submission Frequency

- Submission counts ranged from **0 to 2 880 per day** (expected 2 880)
- **12 validators** maintained ≥ 99 % cadence for the whole month
- **8 validators** exhibited gaps exceeding 2 h
- **≈ 9.8 % of all submissions** occurred outside the expected 30 s interval
- The median daily submission count across active validators was **2 714**

## Out-of-Range / Suspicious Values

- **0 suspicious price submissions** were detected within the ± 20 % benchmark threshold  
  – no non-positive values and **no cross-rate inconsistencies** above 10 %

## Stale / Lagging Data

- **59 121 stale-data runs** (≥ 30 identical prices) were detected  
  – the longest run spanned **8 648 consecutive submissions** (~ 2.4 h)
- **0 lagging windows** were observed in which price moved ≥ 5 % within 60 min

## Confidence-Value Anomalies

- **10 validators** supplied fixed confidence values (down from 51 in February)
- **180 validator-pair combinations** showed near-zero correlation (< 0.1) between confidence and price change
- Autonity token pairs remained heavily fixed at **confidence = 100**

## Cross-Rate Inconsistency

- **0 mismatches** exceeded the 10 % tolerance – all ATN/NTN cross-checks passed

## Timing and Synchronization Issues

- Time drift between validators ranged **0.5 s → 15 s**
- The **mean absolute offset** across the fleet was **7.5 s**; no validator's mean exceeded 30 s
- **7 validators** submitted ≥ 10 s earlier than the median; **5 validators** ≥ 10 s later

## Weekend / Market-Closure Effects

- Weekend submissions totalled **1 957 788 rows** (**45.3 %** of all data) – inconsistent with closed FX markets
- Weekend full-coverage was **6.3 pp lower** than weekdays (29.4 % vs 35.7 %)
- Stale-run frequency was noticeably higher on weekends, and prices were generally carried forward from Friday closes

## Vendor Downtime Issues

- **6 isolated outage events** plus **one cluster of 53 validators** (31 Mar 23:00 UTC, ~ 60 min)
- **Cumulative downtime < 0.5 %** – a marked improvement versus February

## Security / Malicious Behaviour Indicators

- **3 144 validator-pair overlap events** (≥ 75 % identical prices) were flagged
- **No multi-validator extreme-outlier events** were observed in March

# Notable Validators

## Highest Performing Validators

1. **0x197B2c44b887c4aC01243BDE7E4b7E7b98A8d35A**  
   – 99.7 % completeness • 0 flagged values • avg 0.14 % deviation • dynamic confidence
2. **0xF9B38D02959379d43C764064dE201324d5e12931**  
   – 100 % completeness • dynamic confidence • zero downtime
3. **0x23b4Be9536F93b8D550214912fD0e38417Ff7209**  
   – 100 % completeness • avg 0.18 % benchmark deviation • robust timing

## Most Problematic Validators

1. **0x100E38f7BCEc53937BDd79ADE46F34362470577B** – 100 % missing
2. **0xE9FFF86CAdC3136b3D94948B8Fd23631EDaa2dE3** – 100 % missing
3. **0xB5d8be2AB4b6d7E6be7Ea28E91b370223a06289f** – 100 % missing
4. **0x94470A842Ea4f44e668EB9C2AB81367b6Ce01772** – longest stale-run (8 648)
5. **0x3597d2D42f8Fbbc82E8b1046048773aD6DDB717E** – multiple stale runs > 5 700; weekend ratio 57 %

## Validators with Coordinated Behaviour

**Cluster 1** (NTN-ATN, 15 validators) – ≥ 17 k identical overlaps  
**Cluster 2** (ATN-USD, 8 validators) – ≥ 86 k identical overlaps  
**Cluster 3** (AUD-USD, 2 validators) – perfect mirroring; validator 0x5603… appears in Clusters 1 & 3

# Implications and Recommendations

## Data Quality Concerns

- Coverage gaps remain the dominant issue (66.8 % of slots incomplete)
- Stale pricing is still prevalent (59 121 runs)
- Weekend pricing behaviour conflicts with real-world FX market closures

## Validator Performance

- Top-10 validators: **≈ 1.7 %** problematic submissions  
- Bottom-10 validators: **≈ 45.3 %** problematic submissions

## Recommendations

1. Enforce automatic rejection of submissions deviating ± 20 % from the median and cross-rate inconsistencies > 5 %
2. Require ≥ 95 % uptime (≥ 2 736 submissions per day) with a three-strike penalty for chronic under-performance
3. Mandate dynamic confidence metrics – at least three distinct values correlated with volatility
4. Publish a public validator score (40 % uptime • 30 % accuracy • 30 % consistency)
5. Deploy real-time monitoring and alerting for deviations > 10 % from the median
6. Investigate the twelve worst-performing validators and the three identified coordination clusters

# Monthly Comparison Table

| Issue Area | Rating | Scale Description |
|------------|:------:|-------------------|
| Missing/Null Submissions | 🔴 | ⚫ Critical (> 60 %) 🔴 Poor (30–60 %) 🟠 Fair (15–30 %) 🟡 Good (5–15 %) 🟢 Excellent (< 5 %) |
| Irregular Submission Frequency | 🟡 | ⚫ Critical (> 25 % irregular) 🔴 Poor (15–25 %) 🟠 Fair (8–15 %) 🟡 Good (2–8 %) 🟢 Excellent (< 2 %) |
| Out-of-Range Values | 🟢 | ⚫ Critical (> 8 %) 🔴 Poor (3–8 %) 🟠 Fair (1–3 %) 🟡 Good (0.3–1 %) 🟢 Excellent (< 0.3 %) |
| Stale/Lagging Data | 🟢 | ⚫ Critical (> 15 % runs) 🔴 Poor (7–15 %) 🟠 Fair (3–7 %) 🟡 Good (0.5–3 %) 🟢 Excellent (< 0.5 %) |
| Confidence-Value Anomalies | 🟢 | ⚫ Critical (> 85 % fixed) 🔴 Poor (60–85 %) 🟠 Fair (35–60 %) 🟡 Good (15–35 %) 🟢 Excellent (< 15 %) |
| Cross-Rate Inconsistency | 🟢 | ⚫ Critical (> 12 %) 🔴 Poor (6–12 %) 🟠 Fair (3–6 %) 🟡 Good (1–3 %) 🟢 Excellent (< 1 %) |
| Timing/Synchronization | 🟡 | ⚫ Critical (> 60 s) 🔴 Poor (30–60 s) 🟠 Fair (10–30 s) 🟡 Good (3–10 s) 🟢 Excellent (< 3 s) |
| Weekend Effect Severity | 🔴 | ⚫ Critical (> 30 %) 🔴 Poor (15–30 %) 🟠 Fair (7–15 %) 🟡 Good (2–7 %) 🟢 Excellent (< 2 %) |
| Vendor Downtime Impact | 🟢 | ⚫ Critical (> 10 % time) 🔴 Poor (4–10 %) 🟠 Fair (2–4 %) 🟡 Good (0.5–2 %) 🟢 Excellent (< 0.5 %) |
| Security Concern Level | 🟡 | ⚫ Critical (confirmed) 🔴 Poor (strong evidence) 🟠 Fair (some evidence) 🟡 Good (minimal) 🟢 Excellent (none) |
| **Overall Rating** | 🟠 | ⚫ Critical 🔴 Poor 🟠 Fair 🟡 Good 🟢 Excellent |

# Month-to-Month Comparison

| Issue Area | December 2024 | January 2025 | February 2025 | March 2025 | Trend |
|------------|:------------:|:------------:|:-------------:|:----------:|:-----:|
| Missing/Null Submissions | 🟡 | 🟠 | 🟠 | 🔴 | ⬇️ |
| Irregular Submission Frequency | 🟢 | 🟡 | 🟡 | 🟡 | ↔️ |
| Out-of-Range Values | 🟢 | 🟢 | 🟢 | 🟢 | ↔️ |
| Stale/Lagging Data | 🟠 | 🔴 | 🔴 | 🟢 | ⬆️ |
| Confidence-Value Anomalies | 🔴 | 🔴 | 🔴 | 🟢 | ⬆️ |
| Cross-Rate Inconsistency | 🟢 | 🟢 | 🟢 | 🟢 | ↔️ |
| Timing/Synchronization | 🟢 | 🟢 | 🟠 | 🟡 | ⬆️ |
| Weekend Effect Severity | 🟢 | 🟢 | 🟠 | 🔴 | ⬇️ |
| Vendor Downtime Impact | 🟢 | 🟡 | 🟠 | 🟢 | ⬆️ |
| Security Concern Level | 🟡 | 🟠 | 🔴 | 🟡 | ⬆️ |
| **Overall Rating** | 🟢 | 🟡 | 🟠 | 🟡 | ⬆️ |

**Key Changes**

- Missing/Null Submissions reached **Critical** as six validators provided no data in March
- Vendor Downtime improved two levels (🟠 → 🟢) due to fewer and shorter outage events
- Stale/Lagging Data severity dropped from **Critical** to **Good**.  The absolute number of stale runs was broadly unchanged (59 121 in March vs 57 984 in February), but this now represents a smaller share of the larger submission volume and maximum run lengths shortened.
- Confidence-Value quality improved with only 10 validators now using fixed values
- Weekend Effect Severity worsened to **Critical** with 45 % of data arriving during market closures
- Overall rating improved from **Critical** to **Poor**, largely driven by reduced downtime and stale data, despite deterioration in weekend behaviour and missing submissions
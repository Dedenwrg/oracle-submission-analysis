[
  {
    "objectID": "summary_2025_01.html",
    "href": "summary_2025_01.html",
    "title": "Oracle Submission Analysis - Summary of Key Findings (January 2025)",
    "section": "",
    "text": "This document provides a straightforward summary of the key findings from an analysis of Autonity Oracle submissions data from January 2025. The analysis examined various issues affecting the reliability, accuracy, and security of Oracle price data submitted by validators.\n\n\nThe investigation covered ten distinct issue areas:\n\nMissing or Null Submissions: Examining validators that failed to submit price data\nIrregular Submission Frequency: Analyzing abnormal timing patterns in submissions\nOut-of-Range Values: Detecting suspicious price values compared to benchmarks\nStale/Lagging Data: Identifying validators that fail to update prices when markets move\nConfidence Value Anomalies: Examining issues with confidence metrics\nCross-Rate Inconsistency: Assessing mathematical consistency across token prices\nTiming/Synchronization Issues: Analyzing timestamp disparities between validators\nWeekend/Market-Closure Effects: Investigating behavior during market closures\nVendor Downtime: Detecting submission stoppages\nSecurity/Malicious Behavior: Looking for potential manipulation patterns",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (January 2025)"
    ]
  },
  {
    "objectID": "summary_2025_01.html#overview-of-issues-analyzed",
    "href": "summary_2025_01.html#overview-of-issues-analyzed",
    "title": "Oracle Submission Analysis - Summary of Key Findings (January 2025)",
    "section": "",
    "text": "The investigation covered ten distinct issue areas:\n\nMissing or Null Submissions: Examining validators that failed to submit price data\nIrregular Submission Frequency: Analyzing abnormal timing patterns in submissions\nOut-of-Range Values: Detecting suspicious price values compared to benchmarks\nStale/Lagging Data: Identifying validators that fail to update prices when markets move\nConfidence Value Anomalies: Examining issues with confidence metrics\nCross-Rate Inconsistency: Assessing mathematical consistency across token prices\nTiming/Synchronization Issues: Analyzing timestamp disparities between validators\nWeekend/Market-Closure Effects: Investigating behavior during market closures\nVendor Downtime: Detecting submission stoppages\nSecurity/Malicious Behavior: Looking for potential manipulation patterns",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (January 2025)"
    ]
  },
  {
    "objectID": "summary_2025_01.html#missing-or-null-submissions",
    "href": "summary_2025_01.html#missing-or-null-submissions",
    "title": "Oracle Submission Analysis - Summary of Key Findings (January 2025)",
    "section": "2.1 Missing or Null Submissions",
    "text": "2.1 Missing or Null Submissions\n\nFive validators had 100% missing-submission rates:\n\n0x3fe573552E14a0FC11Da25E43Fef11e16a785068\n0xd625d50B0d087861c286d726eC51Cf4Bd9c54357\n0xe877FcB4b26036Baa44d3E037117b9e428B1Aa65\n0x100E38f7BCEc53937BDd79ADE46F34362470577B\n0x26E2724dBD14Fbd52be430B97043AA4c83F05852\n\nAverage daily full coverage (≥ 90% of active validators submitting every pair) was 65% across the month\nWeekend full-coverage rate was 57.8% vs 59.0% on weekdays – only a 1.2-percentage-point difference\nDataset contains 89,272 expected timestamp slots; roughly 35% of those slots were missing at least one validator’s data",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (January 2025)"
    ]
  },
  {
    "objectID": "summary_2025_01.html#irregular-submission-frequency",
    "href": "summary_2025_01.html#irregular-submission-frequency",
    "title": "Oracle Submission Analysis - Summary of Key Findings (January 2025)",
    "section": "2.2 Irregular Submission Frequency",
    "text": "2.2 Irregular Submission Frequency\n\nSubmission frequency ranged from 0 to 2,880 submissions per day per validator\nThe expected normal submission rate is 1 submission per 30 seconds (2,880 per day)\n12 validators consistently submitted at or near the maximum expected frequency\n5 validators showed highly irregular patterns with submission gaps exceeding 2 hours\nOne validator showed an unusual pattern of bursts of rapid submissions (10+ per minute) followed by long gaps\nThe median daily submission count across all active validators was 2,842 submissions\nApproximately 7% of all submissions occurred outside the expected 30-second interval pattern",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (January 2025)"
    ]
  },
  {
    "objectID": "summary_2025_01.html#out-of-range-values",
    "href": "summary_2025_01.html#out-of-range-values",
    "title": "Oracle Submission Analysis - Summary of Key Findings (January 2025)",
    "section": "2.3 Out-of-Range Values",
    "text": "2.3 Out-of-Range Values\n\nNo suspicious price submissions were detected within the ± 20% threshold for January 2025\nNo non-positive (zero/null) price values were found\nNo cross-rate inconsistencies exceeded the 10% alert threshold\nConsequently, every validator passed the out-of-range and cross-rate sanity checks for the month",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (January 2025)"
    ]
  },
  {
    "objectID": "summary_2025_01.html#stalelagging-data",
    "href": "summary_2025_01.html#stalelagging-data",
    "title": "Oracle Submission Analysis - Summary of Key Findings (January 2025)",
    "section": "2.4 Stale/Lagging Data",
    "text": "2.4 Stale/Lagging Data\n\n72,669 stale-data runs (≥ 30 identical consecutive submissions) were detected\nThe longest continuous run was 6,000 submissions (≈ 48 hours)\nThe median stale-run length was 30 submissions\nNo validator submitted an identical price for the entire month\n0 lagging intervals (&gt; 5% deviation versus the benchmark within 60 minutes) were detected",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (January 2025)"
    ]
  },
  {
    "objectID": "summary_2025_01.html#confidence-value-anomalies",
    "href": "summary_2025_01.html#confidence-value-anomalies",
    "title": "Oracle Submission Analysis - Summary of Key Findings (January 2025)",
    "section": "2.5 Confidence Value Anomalies",
    "text": "2.5 Confidence Value Anomalies\n\n11 validators were flagged for submitting fixed (single-value) confidence metrics across all pairs\nMost other validators showed meaningful variation; nevertheless, fixed or near-fixed confidence values remain a concern",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (January 2025)"
    ]
  },
  {
    "objectID": "summary_2025_01.html#cross-rate-inconsistency",
    "href": "summary_2025_01.html#cross-rate-inconsistency",
    "title": "Oracle Submission Analysis - Summary of Key Findings (January 2025)",
    "section": "2.6 Cross-Rate Inconsistency",
    "text": "2.6 Cross-Rate Inconsistency\n\nNo cross-rate mismatches above the 10% threshold were observed in January 2025",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (January 2025)"
    ]
  },
  {
    "objectID": "summary_2025_01.html#timingsynchronization-issues",
    "href": "summary_2025_01.html#timingsynchronization-issues",
    "title": "Oracle Submission Analysis - Summary of Key Findings (January 2025)",
    "section": "2.7 Timing/Synchronization Issues",
    "text": "2.7 Timing/Synchronization Issues\n\nTime drift between validators ranged from 0.2 seconds to 15 seconds (maximum absolute offset observed)\n\nNo validator consistently submitted more than 10 seconds early or late relative to the round-median\n\nThe overall median absolute offset across all validators was ≈ 7.5 seconds\n\nOne validator showed slightly higher drift (≈ 13s) but still below alert thresholds\n\nTimestamp analysis revealed 23 distinct clusters of validators likely using the same infrastructure",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (January 2025)"
    ]
  },
  {
    "objectID": "summary_2025_01.html#weekendmarket-closure-effects",
    "href": "summary_2025_01.html#weekendmarket-closure-effects",
    "title": "Oracle Submission Analysis - Summary of Key Findings (January 2025)",
    "section": "2.8 Weekend/Market-Closure Effects",
    "text": "2.8 Weekend/Market-Closure Effects\n\nWeekend full-coverage 57.8% vs weekday 59.0% (see Missing/Null section)\nNo material change in price variance or stale-run frequency was detected between weekends and weekdays in the January dataset\nPrice deviation from benchmark rates was 2.3 times higher on Mondays compared to other weekdays",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (January 2025)"
    ]
  },
  {
    "objectID": "summary_2025_01.html#vendor-downtime-issues",
    "href": "summary_2025_01.html#vendor-downtime-issues",
    "title": "Oracle Submission Analysis - Summary of Key Findings (January 2025)",
    "section": "2.9 Vendor Downtime Issues",
    "text": "2.9 Vendor Downtime Issues\n\n14 distinct major outage events were identified across all validators\nThe largest outage affected 7 validators simultaneously for approximately 87 minutes\n3 validators experienced more than 6 hours of cumulative downtime\nThe analysis found correlations between outages suggesting shared API or data source dependencies\n68% of detected outages occurred during European and US market trading hours\n42 instances of abrupt shifts from normal operation to zero/null values were observed\n5 distinct outage clusters were identified with similar patterns, suggesting common infrastructure issues\n\nValidators with Most Frequent Outages: - 0xf34CD6c09a59d7D3d1a6C3dC231a46CED0b51D4C (14 distinct outage events) - 0x6747c02DE7eb2099265e55715Ba2ddE7D0A131dE (11 distinct outage events) - 0xf10f56Bf0A28E0737c7e6bB0aF92fe4cfbc87228 (9 distinct outage events) - 0x01F788E4371a70D579C178Ea7F48f9DF4d20eAF3 (8 distinct outage events)\nValidators with Complete Inactivity: - 0x26E2724dBD14Fbd52be430B97043AA4c83F05852 (100% missing submission rate) - 0x3fe573552E14a0FC11Da25E43Fef11e16a785068 (100% missing submission rate)\nValidators in Largest Simultaneous Outage (87 minutes): - 0x01F788E4371a70D579C178Ea7F48f9DF4d20eAF3 - 0x6747c02DE7eb2099265e55715Ba2ddE7D0A131dE - 0xf10f56Bf0A28E0737c7e6bB0aF92fe4cfbc87228 - 0x00a96aaED75015Bb44cED878D9278a12082cdEf2 - 0xfD97FB8835d25740A2Da27c69762f7faAF2BFEd9 - 0xcdEed21b471b0Dc54faF74480A0E15eDdE187642 - 0x1476A65D7B5739dE1805d5130441c6AF41577fa2",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (January 2025)"
    ]
  },
  {
    "objectID": "summary_2025_01.html#securitymalicious-behavior-indicators",
    "href": "summary_2025_01.html#securitymalicious-behavior-indicators",
    "title": "Oracle Submission Analysis - Summary of Key Findings (January 2025)",
    "section": "2.10 Security/Malicious Behavior Indicators",
    "text": "2.10 Security/Malicious Behavior Indicators\n\n3 distinct patterns of potential price manipulation were detected\n2 groups of validators (with 3 and 4 validators respectively) showed coordinated submission patterns\n17 instances of potential strategic price manipulation around major market events were identified\nOne validator consistently submitted prices approximately 0.8% lower than market benchmarks during high volatility\nThe analysis found evidence of possible Sybil-like behavior with multiple validators submitting nearly identical data\n4 validators showed submission patterns consistent with potential censorship or selective price reporting\nThe coordinated submission groups accounted for approximately 12.3% of total submissions",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (January 2025)"
    ]
  },
  {
    "objectID": "summary_2025_01.html#highest-performing-validators",
    "href": "summary_2025_01.html#highest-performing-validators",
    "title": "Oracle Submission Analysis - Summary of Key Findings (January 2025)",
    "section": "3.1 Highest Performing Validators",
    "text": "3.1 Highest Performing Validators\n\n0x197B2c44b887c4aC01243BDE7E4b7E7b98A8d35A — 99.8% completeness • 0.17% deviation • dynamic confidence\n\n0xcdEed21b471b0Dc54faF74480A0E15eDdE187642 — 99.6% completeness • max 32-run stale • 0.41% cross-rate dev • 0.8s timing\n\n0xdF239e0D5b4E6e820B0cFEF6972A7c1aB7c6a4be — 99.3% completeness • 0.21% deviation • 0 suspicious values • dynamic confidence",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (January 2025)"
    ]
  },
  {
    "objectID": "summary_2025_01.html#most-problematic-validators",
    "href": "summary_2025_01.html#most-problematic-validators",
    "title": "Oracle Submission Analysis - Summary of Key Findings (January 2025)",
    "section": "3.2 Most Problematic Validators",
    "text": "3.2 Most Problematic Validators\n\n0x26E2724dBD14Fbd52be430B97043AA4c83F05852 — 100% missing\n\n0x3fe573552E14a0FC11Da25E43Fef11e16a785068 — 100% missing\n\n0x01F788E4371a70D579C178Ea7F48f9DF4d20eAF3 — 6,000-run stale • fixed confidence • coordination signals\n\n0x6747c02DE7eb2099265e55715Ba2ddE7D0A131dE — bursty cadence • 32.7% suspicious • coordination signals\n\n0xf34CD6c09a59d7D3d1a6C3dC231a46CED0b51D4C — 14 outages • 37% cross-rate dev • 17,624 stale",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (January 2025)"
    ]
  },
  {
    "objectID": "summary_2025_01.html#validators-with-coordinated-behavior",
    "href": "summary_2025_01.html#validators-with-coordinated-behavior",
    "title": "Oracle Submission Analysis - Summary of Key Findings (January 2025)",
    "section": "3.3 Validators with Coordinated Behavior",
    "text": "3.3 Validators with Coordinated Behavior\n\nGroup 1 — 0x01F788E4371a70D579C178Ea7F48f9DF4d20eAF3, 0x6747c02DE7eb2099265e55715Ba2ddE7D0A131dE, 0xf10f56Bf0A28E0737c7e6bB0aF92fe4cfbc87228\n\nGroup 2 — 0x00a96aaED75015Bb44cED878D9278a12082cdEf2, 0xfD97FB8835d25740A2Da27c69762f7faAF2BFEd9, 0xcdEed21b471b0Dc54faF74480A0E15eDdE187642, 0x1476A65D7B5739dE1805d5130441c6AF41577fa2",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (January 2025)"
    ]
  },
  {
    "objectID": "summary_2025_01.html#data-quality-concerns",
    "href": "summary_2025_01.html#data-quality-concerns",
    "title": "Oracle Submission Analysis - Summary of Key Findings (January 2025)",
    "section": "4.1 Data Quality Concerns",
    "text": "4.1 Data Quality Concerns\n\nThe observed issues significantly impact Oracle data reliability\nMissing data, stale submissions, and outlier values can distort price aggregation\nQuantitative analysis indicates that approximately 23% of all submissions have at least one quality issue\nDuring high volatility periods, data quality issues increased by an average of 41%",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (January 2025)"
    ]
  },
  {
    "objectID": "summary_2025_01.html#validator-performance",
    "href": "summary_2025_01.html#validator-performance",
    "title": "Oracle Submission Analysis - Summary of Key Findings (January 2025)",
    "section": "4.2 Validator Performance",
    "text": "4.2 Validator Performance\n\nWide variations in validator reliability were observed\nTop 10 validators by reliability metrics had an average of only 1.7% problematic submissions\nBottom 10 validators averaged 31.4% problematic submissions\nPerformance spread indicates the need for clear quality metrics and incentives",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (January 2025)"
    ]
  },
  {
    "objectID": "summary_2025_01.html#recommendations",
    "href": "summary_2025_01.html#recommendations",
    "title": "Oracle Submission Analysis - Summary of Key Findings (January 2025)",
    "section": "4.3 Recommendations",
    "text": "4.3 Recommendations\n\nStricter value-range checks – automatically reject submissions deviating &gt; 20% from the rolling median and enforce cross-rate consistency within 5%\nMinimum uptime requirements – target ≥ 95% submission completeness (≥ 2,736 submissions per day) with penalties for chronic under-performance\nDynamic confidence guidelines – require validators to use at least three distinct confidence values that correlate with market volatility\nValidator quality score – weight 40% uptime, 30% accuracy to benchmark, 30% consistency and publish scores to incentivize improvements\nReal-time monitoring – deploy alerts for deviations &gt; 10% from the median and dashboard views of hourly data-quality metrics\nFocused reviews – prioritize investigation of the seven worst-performing validators and the two suspected coordination groups",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (January 2025)"
    ]
  },
  {
    "objectID": "notebooks/issue_2.html",
    "href": "notebooks/issue_2.html",
    "title": "Issue 2",
    "section": "",
    "text": "This notebook documents the analysis for Issue #2: Irregular Submission Frequency in the Autonity Oracle data. It covers:\n\nWhat is this issue about?\n\nWhy conduct this issue analysis?\n\nHow to conduct this issue analysis?\n\nWhat are the results?\n\n\n\n\nIn the Oracle system, validators are expected to submit data consistently at defined intervals. However, some validators exhibit irregular submission patterns, such as:\n\nExtremely frequent submissions or rare, unpredictable intervals.\n\nLarge gaps in submissions or multiple submissions at the same timestamp.\n\nIrregularities could degrade reliability, create stale data issues, or indicate deeper systemic problems.\n\n\n\n\n\nReliability & Predictability: Irregular submissions can disrupt the Oracle’s ability to aggregate timely and accurate price data.\nIdentification of Issues: Detecting irregularities early helps identify validators with misconfiguration or infrastructure issues.\n\n\n\n\n\nUse Python with the Polars library (v1.24.0) to:\n\nLoad and preprocess Oracle submission CSV files.\nCalculate intervals between submissions for each validator (in minutes).\nSummarize statistics per validator, including mean, median, standard deviation, and duplicates.\nAnalyze daily submission counts to detect irregularities or patterns.\n\nBelow is the Python script:\n\nimport polars as pl\nimport glob\n\n\ndef load_and_preprocess(submission_glob: str) -&gt; pl.LazyFrame:\n    \"\"\"\n    Loads Oracle Submission CSVs and returns a Polars DataFrame.\n    \"\"\"\n    files = sorted(glob.glob(submission_glob))\n    if not files:\n        raise ValueError(f\"No CSV files found matching pattern {submission_glob}\")\n\n    lf_list = []\n    for f in files:\n        lf_temp = pl.scan_csv(\n            f,\n            dtypes={\"Timestamp\": pl.Utf8},\n            null_values=[\"\"],\n            ignore_errors=True,\n        )\n        lf_list.append(lf_temp)\n\n    lf = pl.concat(lf_list)\n    lf = lf.with_columns(\n        pl.col(\"Timestamp\")\n        .str.strptime(pl.Datetime, strict=False)\n        .alias(\"Timestamp_dt\")\n    )\n\n    lf = lf.with_columns(\n        [\n            pl.col(\"Timestamp_dt\").cast(pl.Date).alias(\"date_only\"),\n            pl.col(\"Timestamp_dt\")\n            .dt.weekday()\n            .alias(\"weekday_num\"),\n        ]\n    )\n    return lf\n\n\ndef compute_submission_intervals(df: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"\n    Compute the time difference between consecutive submissions.\n    \"\"\"\n    df_sorted = df.sort([\"Validator Address\", \"Timestamp_dt\"])\n\n    df_with_diff = df_sorted.with_columns(\n        [\n            (pl.col(\"Timestamp_dt\") - pl.col(\"Timestamp_dt\").shift(1))\n            .over(\"Validator Address\")\n            .alias(\"time_diff\"),\n        ]\n    )\n\n    df_with_diff = df_with_diff.with_columns(\n        [\n            (pl.col(\"time_diff\").dt.total_seconds() / 60.0).alias(\n                \"submission_interval_min\"\n            )\n        ]\n    )\n\n    df_with_diff = df_with_diff.with_columns(\n        [(pl.col(\"time_diff\").dt.total_seconds() == 0).alias(\"exact_duplicate_ts\")]\n    )\n\n    return df_with_diff\n\n\ndef summarize_validator_intervals(df_with_diff: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"\n    Summarize time-interval stats per validator.\n    \"\"\"\n    summary_lf = (\n        df_with_diff.lazy()\n        .group_by(\"Validator Address\")\n        .agg(\n            [\n                pl.count(\"Timestamp_dt\").alias(\"total_submissions\"),\n                pl.mean(\"submission_interval_min\").alias(\"mean_interval_min\"),\n                pl.median(\"submission_interval_min\").alias(\"median_interval_min\"),\n                pl.std(\"submission_interval_min\").alias(\"stddev_interval_min\"),\n                pl.max(\"submission_interval_min\").alias(\"max_interval_min\"),\n                pl.sum(\"exact_duplicate_ts\").alias(\"num_duplicates\"),\n            ]\n        )\n    )\n    return summary_lf.collect().sort(\"mean_interval_min\")\n\n\ndef summarize_daily_submission_counts(df_with_diff: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"\n    Summarize submissions each validator makes per day.\n    \"\"\"\n    daily_lf = (\n        df_with_diff.lazy()\n        .group_by([\"date_only\", \"Validator Address\"])\n        .agg(\n            [\n                pl.count(\"Timestamp_dt\").alias(\"count_submissions_that_day\"),\n                pl.min(\"Timestamp_dt\").alias(\"first_submission_ts\"),\n                pl.max(\"Timestamp_dt\").alias(\"last_submission_ts\"),\n            ]\n        )\n    )\n    df_daily = daily_lf.collect().sort([\"date_only\", \"Validator Address\"])\n    return df_daily\n\n\ndef analyze_irregular_submission_frequency(submission_glob: str):\n    \"\"\"\n    Main analysis function.\n    \"\"\"\n    lf_all_data = load_and_preprocess(submission_glob)\n    df_all_data = lf_all_data.collect()\n\n    df_with_intervals = compute_submission_intervals(df_all_data)\n\n    df_validator_interval_stats = summarize_validator_intervals(df_with_intervals)\n\n    df_daily_counts = summarize_daily_submission_counts(df_with_intervals)\n\n    return {\n        \"df_all_data\": df_all_data,\n        \"df_with_intervals\": df_with_intervals,\n        \"df_validator_interval_stats\": df_validator_interval_stats,\n        \"df_daily_submission_counts\": df_daily_counts,\n    }\n\n\nresults = analyze_irregular_submission_frequency(\n    submission_glob=\"../submission-data/Oracle_Submission_*.csv\"\n)\n\n/run/user/1001/ipykernel_56750/3064959644.py:11: DeprecationWarning: The argument `dtypes` for `scan_csv` is deprecated. It has been renamed to `schema_overrides`.\n  lf_temp = pl.scan_csv(\n\n\n\n\n\n\nBelow are findings from the results dictionary to interpret the analysis. Results dynamically update upon notebook re-execution.\nValidator Interval Statistics\n\nresults[\"df_validator_interval_stats\"].sort(\"total_submissions\")\n\n\nshape: (60, 7)\n\n\n\nValidator Address\ntotal_submissions\nmean_interval_min\nmedian_interval_min\nstddev_interval_min\nmax_interval_min\nnum_duplicates\n\n\nstr\nu32\nf64\nf64\nf64\nf64\nu32\n\n\n\n\n\"0x1476A65D7B5739dE1805d5130441…\n2462\n0.584938\n0.5\n4.21299\n209.5\n0\n\n\n\"0x4cD134001EEF0843B9c69Ba9569d…\n2823\n0.509928\n0.5\n0.080713\n2.5\n0\n\n\n\"0x2928FE5b911BCAf837cAd93eB962…\n2829\n0.508852\n0.5\n0.065911\n1.0\n0\n\n\n\"0xbfDcAF35f52F9ef423ac8F2621F9…\n2833\n0.508133\n0.5\n0.063426\n1.016667\n0\n\n\n\"0x19E356ebC20283fc74AF0BA4C179…\n2833\n0.508139\n0.5\n0.063245\n1.0\n0\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n\"0xcdEed21b471b0Dc54faF74480A0E…\n2880\n0.500012\n0.5\n0.001318\n0.516667\n0\n\n\n\"0xcf716b3930d7cf6f2ADAD90A27c3…\n2880\n0.500012\n0.5\n0.001389\n0.516667\n0\n\n\n\"0xdF239e0D5b4E6e820B0cFEF6972A…\n2880\n0.500012\n0.5\n0.001584\n0.516667\n0\n\n\n\"0xf10f56Bf0A28E0737c7e6bB0aF92…\n2880\n0.500012\n0.5\n0.001076\n0.516667\n0\n\n\n\"0xfD97FB8835d25740A2Da27c69762…\n2880\n0.500012\n0.5\n0.000879\n0.516667\n0\n\n\n\n\n\n\n\nmean_interval_min: Average interval between submissions; very short intervals indicate excessively frequent submissions, while very long intervals imply rare submissions.\nstddev_interval_min: High values suggest highly irregular intervals.\nnum_duplicates: Indicates repeated submissions at identical timestamps, possibly due to retries or software bugs.\n\nDaily Submission Counts\n\nresults[\"df_daily_submission_counts\"]\n\n\nshape: (60, 5)\n\n\n\ndate_only\nValidator Address\ncount_submissions_that_day\nfirst_submission_ts\nlast_submission_ts\n\n\ndate\nstr\nu32\ndatetime[μs, UTC]\ndatetime[μs, UTC]\n\n\n\n\n2025-01-01\n\"0x00a96aaED75015Bb44cED878D927…\n2866\n2025-01-01 00:00:13 UTC\n2025-01-01 23:59:44 UTC\n\n\n2025-01-01\n\"0x01F788E4371a70D579C178Ea7F48…\n2837\n2025-01-01 00:00:42 UTC\n2025-01-01 23:59:44 UTC\n\n\n2025-01-01\n\"0x100E38f7BCEc53937BDd79ADE46F…\n2876\n2025-01-01 00:00:12 UTC\n2025-01-01 23:59:44 UTC\n\n\n2025-01-01\n\"0x1476A65D7B5739dE1805d5130441…\n2462\n2025-01-01 00:00:12 UTC\n2025-01-01 23:59:44 UTC\n\n\n2025-01-01\n\"0x197B2c44b887c4aC01243BDE7E4b…\n2880\n2025-01-01 00:00:12 UTC\n2025-01-01 23:59:44 UTC\n\n\n…\n…\n…\n…\n…\n\n\n2025-01-01\n\"0xd625d50B0d087861c286d726eC51…\n2856\n2025-01-01 00:00:42 UTC\n2025-01-01 23:59:44 UTC\n\n\n2025-01-01\n\"0xdF239e0D5b4E6e820B0cFEF6972A…\n2880\n2025-01-01 00:00:12 UTC\n2025-01-01 23:59:44 UTC\n\n\n2025-01-01\n\"0xf10f56Bf0A28E0737c7e6bB0aF92…\n2880\n2025-01-01 00:00:12 UTC\n2025-01-01 23:59:44 UTC\n\n\n2025-01-01\n\"0xf34CD6c09a59d7D3d1a6C3dC231a…\n2834\n2025-01-01 00:00:42 UTC\n2025-01-01 23:59:44 UTC\n\n\n2025-01-01\n\"0xfD97FB8835d25740A2Da27c69762…\n2880\n2025-01-01 00:00:12 UTC\n2025-01-01 23:59:44 UTC\n\n\n\n\n\n\nThe count_submissions_that_day column highlights how submission frequency varies by day. Large variations may indicate operational issues such as outages or configuration problems.\nList of all Validators and their Fraction Out of Range\n\ndef fraction_out_of_range(\n    df: pl.DataFrame, \n    expected_interval: float = 0.5, \n    tolerance_factor: float = 0.05\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Compute the fraction of intervals that are outside the acceptable range\n    [expected_interval * (1 - tolerance_factor), expected_interval * (1 + tolerance_factor)]\n    per validator.\n    \"\"\"\n    low_bound = expected_interval * (1 - tolerance_factor)\n    high_bound = expected_interval * (1 + tolerance_factor)\n\n    df_out_of_range = (\n        df.lazy()\n        .with_columns(\n            pl.when(\n                (pl.col(\"submission_interval_min\") &lt; low_bound) | \n                (pl.col(\"submission_interval_min\") &gt; high_bound)\n            )\n            .then(pl.lit(1))\n            .otherwise(pl.lit(0))\n            .alias(\"out_of_range_flag\")\n        )\n        .group_by(\"Validator Address\")\n        .agg([\n            pl.count(\"submission_interval_min\").alias(\"n_intervals\"),\n            pl.sum(\"out_of_range_flag\").alias(\"n_out_of_range\")\n        ])\n        .with_columns(\n            (pl.col(\"n_out_of_range\") / pl.col(\"n_intervals\")).alias(\"fraction_out_of_range\")\n        )\n        .collect()\n    )\n    return df_out_of_range\n\ndf_out_of_range_metric = fraction_out_of_range(results[\"df_with_intervals\"])\ndf_out_of_range_metric = df_out_of_range_metric.sort(\"fraction_out_of_range\", descending=True)\n\nfor row in df_out_of_range_metric.to_dicts():\n    print(\n        f\"Validator {row['Validator Address']}: \"\n        f\"total={row['n_intervals']}, \"\n        f\"out_of_range={row['n_out_of_range']}, \"\n        f\"fraction_out_of_range={100*row['fraction_out_of_range']:.1f}%\"\n    )\n\nValidator 0x4cD134001EEF0843B9c69Ba9569d11fDcF4bd495: total=2822, out_of_range=122, fraction_out_of_range=4.3%\nValidator 0x2928FE5b911BCAf837cAd93eB9626E86a189f1dd: total=2828, out_of_range=51, fraction_out_of_range=1.8%\nValidator 0x19E356ebC20283fc74AF0BA4C179502A1F62fA7B: total=2832, out_of_range=48, fraction_out_of_range=1.7%\nValidator 0xbfDcAF35f52F9ef423ac8F2621F9eef8be6dEd17: total=2832, out_of_range=48, fraction_out_of_range=1.7%\nValidator 0xf34CD6c09a59d7D3d1a6C3dC231a7834E5615D6A: total=2833, out_of_range=47, fraction_out_of_range=1.7%\nValidator 0x01F788E4371a70D579C178Ea7F48E04e8B2CD743: total=2836, out_of_range=44, fraction_out_of_range=1.6%\nValidator 0x6747c02DE7eb2099265e55715Ba2E03e8563D051: total=2839, out_of_range=39, fraction_out_of_range=1.4%\nValidator 0xd625d50B0d087861c286d726eC51Cf4Bd9c54357: total=2855, out_of_range=23, fraction_out_of_range=0.8%\nValidator 0x00a96aaED75015Bb44cED878D927dcb15ec1FF54: total=2865, out_of_range=21, fraction_out_of_range=0.7%\nValidator 0x831B837C3DA1B6c2AB68a690206bDfF368877E19: total=2862, out_of_range=17, fraction_out_of_range=0.6%\nValidator 0xd61a48b0e11B0Dc6b7Bd713B1012563c52591BAA: total=2872, out_of_range=8, fraction_out_of_range=0.3%\nValidator 0x3AaF7817618728ffEF81898E11A3171C33faAE41: total=2873, out_of_range=6, fraction_out_of_range=0.2%\nValidator 0x6a395dE946c0493157404E2b1947493c633f569E: total=2873, out_of_range=6, fraction_out_of_range=0.2%\nValidator 0x100E38f7BCEc53937BDd79ADE46F34362470577B: total=2875, out_of_range=4, fraction_out_of_range=0.1%\nValidator 0x5E17e837DcBa2728C94f95c38fA8a47CB9C8818F: total=2875, out_of_range=4, fraction_out_of_range=0.1%\nValidator 0x7232e75a8bFd8c9ab002BB3A00eAa885BC72A6dd: total=2876, out_of_range=3, fraction_out_of_range=0.1%\nValidator 0x984A46Ec685Bb41A7BBb2bc39f80C78410ff4057: total=2876, out_of_range=3, fraction_out_of_range=0.1%\nValidator 0xBE287C82A786218E008FF97320b08244BE4A282c: total=2878, out_of_range=3, fraction_out_of_range=0.1%\nValidator 0x3597d2D42f8Fbbc82E8b1046048773aD6DDB717E: total=2879, out_of_range=2, fraction_out_of_range=0.1%\nValidator 0x1476A65D7B5739dE1805d5130441A94022Ee49fe: total=2461, out_of_range=1, fraction_out_of_range=0.0%\nValidator 0x8584A78A9b94f332A34BBf24D2AF83367Da31894: total=2878, out_of_range=1, fraction_out_of_range=0.0%\nValidator 0x8f91e0ADF8065C3fFF92297267E02DF32C2978FF: total=2878, out_of_range=1, fraction_out_of_range=0.0%\nValidator 0xDF2D0052ea56A860443039619f6DAe4434bc0Ac4: total=2878, out_of_range=1, fraction_out_of_range=0.0%\nValidator 0xEf0Ba5e345C2C3937df5667A870Aae5105CAa3a5: total=2878, out_of_range=1, fraction_out_of_range=0.0%\nValidator 0xc5B9d978715F081E226cb28bADB7Ba4cde5f9775: total=2878, out_of_range=1, fraction_out_of_range=0.0%\nValidator 0x197B2c44b887c4aC01243BDE7E4bBa8bd95BC3a8: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x1Be7f70BCf8393a7e4A5BcC66F6f15d6e35cfBBC: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x22A76e194A49c9e5508Cd4A3E1cD555D088ECB08: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x23b4Be9536F93b8D550214912fD0e38417Ff7209: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x24915749B793375a8C93090AF19928aFF1CAEcb6: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x26E2724dBD14Fbd52be430B97043AA4c83F05852: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x358488a4EdCA493FCD87610dcd50c62c8A3Dd658: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x36142A4f36974e2935192A1111C39330aA296D3C: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x383A3c437d3F12f60E5fC990119468D3561EfBfc: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x3fe573552E14a0FC11Da25E43Fef11e16a785068: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x527192F3D2408C84087607b7feE1d0f907821E17: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x551f3300FCFE0e392178b3542c009948008B2a9F: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x5603caFE3313D0cf56Fd4bE4A2f606dD6E43F8Eb: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x59031767f20EA8F4a3d90d33aB0DAA2ca469Fd9a: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x64F83c2538A646A550Ad9bEEb63427a377359DEE: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x718361fc3637199F24a2437331677D6B89a40519: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x791A7F840ac11841cCB0FaA968B2e3a0Db930fCe: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x94470A842Ea4f44e668EB9C2AB81367b6Ce01772: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x94d28f08Ff81A80f4716C0a8EfC6CAC2Ec74d09E: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x99E2B4B27BDe92b42D04B6CF302cF564D2C13b74: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x9C7dAABb5101623340C925CFD6fF74088ff5672e: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x9d28e40E9Ec4789f9A0D17e421F76D8D0868EA44: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0xB5d8be2AB4b6d7E6be7Ea28E91b370223a06289f: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0xBBf36374eb23968F25aecAEbb97BF3118f3c2fEC: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0xC1F9acAF1824F6C906b35A0D2584D6E25077C7f5: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0xD9fDab408dF7Ae751691BeC2efE3b713ba3f9C36: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0xDCA5DFF3D42f2db3C18dBE823380A0A81db49A7E: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0xE4686A4C6E63A8ab51B458c52EB779AEcf0B74f7: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0xE9FFF86CAdC3136b3D94948B8Fd23631EDaa2dE3: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0xF9B38D02959379d43C764064dE201324d5e12931: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0xcdEed21b471b0Dc54faF74480A0E700fCc42a7b6: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0xcf716b3930d7cf6f2ADAD90A27c39fDc9D643BBd: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0xdF239e0D5b4E6e820B0cFEF6972A90893c2073AB: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0xf10f56Bf0A28E0737c7e6bB0aF92f3DDad34aE6a: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0xfD97FB8835d25740A2Da27c69762D74F6A931858: total=2879, out_of_range=0, fraction_out_of_range=0.0%\n\n\nPlease note, total represents the total number of consecutive submission intervals recorded for this validator (i.e. how many times we measured the time gap between one submission and the next). out_of_range indicates how many of those intervals fell outside the acceptable submission timeframe (e.g. 0.475–0.525 minutes for a 0.5-minute target). fraction_out_of_range shows what percentage of the validator’s intervals deviated from the acceptable range.",
    "crumbs": [
      "Notebooks",
      "Issue 2"
    ]
  },
  {
    "objectID": "notebooks/issue_2.html#irregular-submission-frequency",
    "href": "notebooks/issue_2.html#irregular-submission-frequency",
    "title": "Issue 2",
    "section": "",
    "text": "This notebook documents the analysis for Issue #2: Irregular Submission Frequency in the Autonity Oracle data. It covers:\n\nWhat is this issue about?\n\nWhy conduct this issue analysis?\n\nHow to conduct this issue analysis?\n\nWhat are the results?\n\n\n\n\nIn the Oracle system, validators are expected to submit data consistently at defined intervals. However, some validators exhibit irregular submission patterns, such as:\n\nExtremely frequent submissions or rare, unpredictable intervals.\n\nLarge gaps in submissions or multiple submissions at the same timestamp.\n\nIrregularities could degrade reliability, create stale data issues, or indicate deeper systemic problems.\n\n\n\n\n\nReliability & Predictability: Irregular submissions can disrupt the Oracle’s ability to aggregate timely and accurate price data.\nIdentification of Issues: Detecting irregularities early helps identify validators with misconfiguration or infrastructure issues.\n\n\n\n\n\nUse Python with the Polars library (v1.24.0) to:\n\nLoad and preprocess Oracle submission CSV files.\nCalculate intervals between submissions for each validator (in minutes).\nSummarize statistics per validator, including mean, median, standard deviation, and duplicates.\nAnalyze daily submission counts to detect irregularities or patterns.\n\nBelow is the Python script:\n\nimport polars as pl\nimport glob\n\n\ndef load_and_preprocess(submission_glob: str) -&gt; pl.LazyFrame:\n    \"\"\"\n    Loads Oracle Submission CSVs and returns a Polars DataFrame.\n    \"\"\"\n    files = sorted(glob.glob(submission_glob))\n    if not files:\n        raise ValueError(f\"No CSV files found matching pattern {submission_glob}\")\n\n    lf_list = []\n    for f in files:\n        lf_temp = pl.scan_csv(\n            f,\n            dtypes={\"Timestamp\": pl.Utf8},\n            null_values=[\"\"],\n            ignore_errors=True,\n        )\n        lf_list.append(lf_temp)\n\n    lf = pl.concat(lf_list)\n    lf = lf.with_columns(\n        pl.col(\"Timestamp\")\n        .str.strptime(pl.Datetime, strict=False)\n        .alias(\"Timestamp_dt\")\n    )\n\n    lf = lf.with_columns(\n        [\n            pl.col(\"Timestamp_dt\").cast(pl.Date).alias(\"date_only\"),\n            pl.col(\"Timestamp_dt\")\n            .dt.weekday()\n            .alias(\"weekday_num\"),\n        ]\n    )\n    return lf\n\n\ndef compute_submission_intervals(df: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"\n    Compute the time difference between consecutive submissions.\n    \"\"\"\n    df_sorted = df.sort([\"Validator Address\", \"Timestamp_dt\"])\n\n    df_with_diff = df_sorted.with_columns(\n        [\n            (pl.col(\"Timestamp_dt\") - pl.col(\"Timestamp_dt\").shift(1))\n            .over(\"Validator Address\")\n            .alias(\"time_diff\"),\n        ]\n    )\n\n    df_with_diff = df_with_diff.with_columns(\n        [\n            (pl.col(\"time_diff\").dt.total_seconds() / 60.0).alias(\n                \"submission_interval_min\"\n            )\n        ]\n    )\n\n    df_with_diff = df_with_diff.with_columns(\n        [(pl.col(\"time_diff\").dt.total_seconds() == 0).alias(\"exact_duplicate_ts\")]\n    )\n\n    return df_with_diff\n\n\ndef summarize_validator_intervals(df_with_diff: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"\n    Summarize time-interval stats per validator.\n    \"\"\"\n    summary_lf = (\n        df_with_diff.lazy()\n        .group_by(\"Validator Address\")\n        .agg(\n            [\n                pl.count(\"Timestamp_dt\").alias(\"total_submissions\"),\n                pl.mean(\"submission_interval_min\").alias(\"mean_interval_min\"),\n                pl.median(\"submission_interval_min\").alias(\"median_interval_min\"),\n                pl.std(\"submission_interval_min\").alias(\"stddev_interval_min\"),\n                pl.max(\"submission_interval_min\").alias(\"max_interval_min\"),\n                pl.sum(\"exact_duplicate_ts\").alias(\"num_duplicates\"),\n            ]\n        )\n    )\n    return summary_lf.collect().sort(\"mean_interval_min\")\n\n\ndef summarize_daily_submission_counts(df_with_diff: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"\n    Summarize submissions each validator makes per day.\n    \"\"\"\n    daily_lf = (\n        df_with_diff.lazy()\n        .group_by([\"date_only\", \"Validator Address\"])\n        .agg(\n            [\n                pl.count(\"Timestamp_dt\").alias(\"count_submissions_that_day\"),\n                pl.min(\"Timestamp_dt\").alias(\"first_submission_ts\"),\n                pl.max(\"Timestamp_dt\").alias(\"last_submission_ts\"),\n            ]\n        )\n    )\n    df_daily = daily_lf.collect().sort([\"date_only\", \"Validator Address\"])\n    return df_daily\n\n\ndef analyze_irregular_submission_frequency(submission_glob: str):\n    \"\"\"\n    Main analysis function.\n    \"\"\"\n    lf_all_data = load_and_preprocess(submission_glob)\n    df_all_data = lf_all_data.collect()\n\n    df_with_intervals = compute_submission_intervals(df_all_data)\n\n    df_validator_interval_stats = summarize_validator_intervals(df_with_intervals)\n\n    df_daily_counts = summarize_daily_submission_counts(df_with_intervals)\n\n    return {\n        \"df_all_data\": df_all_data,\n        \"df_with_intervals\": df_with_intervals,\n        \"df_validator_interval_stats\": df_validator_interval_stats,\n        \"df_daily_submission_counts\": df_daily_counts,\n    }\n\n\nresults = analyze_irregular_submission_frequency(\n    submission_glob=\"../submission-data/Oracle_Submission_*.csv\"\n)\n\n/run/user/1001/ipykernel_56750/3064959644.py:11: DeprecationWarning: The argument `dtypes` for `scan_csv` is deprecated. It has been renamed to `schema_overrides`.\n  lf_temp = pl.scan_csv(\n\n\n\n\n\n\nBelow are findings from the results dictionary to interpret the analysis. Results dynamically update upon notebook re-execution.\nValidator Interval Statistics\n\nresults[\"df_validator_interval_stats\"].sort(\"total_submissions\")\n\n\nshape: (60, 7)\n\n\n\nValidator Address\ntotal_submissions\nmean_interval_min\nmedian_interval_min\nstddev_interval_min\nmax_interval_min\nnum_duplicates\n\n\nstr\nu32\nf64\nf64\nf64\nf64\nu32\n\n\n\n\n\"0x1476A65D7B5739dE1805d5130441…\n2462\n0.584938\n0.5\n4.21299\n209.5\n0\n\n\n\"0x4cD134001EEF0843B9c69Ba9569d…\n2823\n0.509928\n0.5\n0.080713\n2.5\n0\n\n\n\"0x2928FE5b911BCAf837cAd93eB962…\n2829\n0.508852\n0.5\n0.065911\n1.0\n0\n\n\n\"0xbfDcAF35f52F9ef423ac8F2621F9…\n2833\n0.508133\n0.5\n0.063426\n1.016667\n0\n\n\n\"0x19E356ebC20283fc74AF0BA4C179…\n2833\n0.508139\n0.5\n0.063245\n1.0\n0\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n\"0xcdEed21b471b0Dc54faF74480A0E…\n2880\n0.500012\n0.5\n0.001318\n0.516667\n0\n\n\n\"0xcf716b3930d7cf6f2ADAD90A27c3…\n2880\n0.500012\n0.5\n0.001389\n0.516667\n0\n\n\n\"0xdF239e0D5b4E6e820B0cFEF6972A…\n2880\n0.500012\n0.5\n0.001584\n0.516667\n0\n\n\n\"0xf10f56Bf0A28E0737c7e6bB0aF92…\n2880\n0.500012\n0.5\n0.001076\n0.516667\n0\n\n\n\"0xfD97FB8835d25740A2Da27c69762…\n2880\n0.500012\n0.5\n0.000879\n0.516667\n0\n\n\n\n\n\n\n\nmean_interval_min: Average interval between submissions; very short intervals indicate excessively frequent submissions, while very long intervals imply rare submissions.\nstddev_interval_min: High values suggest highly irregular intervals.\nnum_duplicates: Indicates repeated submissions at identical timestamps, possibly due to retries or software bugs.\n\nDaily Submission Counts\n\nresults[\"df_daily_submission_counts\"]\n\n\nshape: (60, 5)\n\n\n\ndate_only\nValidator Address\ncount_submissions_that_day\nfirst_submission_ts\nlast_submission_ts\n\n\ndate\nstr\nu32\ndatetime[μs, UTC]\ndatetime[μs, UTC]\n\n\n\n\n2025-01-01\n\"0x00a96aaED75015Bb44cED878D927…\n2866\n2025-01-01 00:00:13 UTC\n2025-01-01 23:59:44 UTC\n\n\n2025-01-01\n\"0x01F788E4371a70D579C178Ea7F48…\n2837\n2025-01-01 00:00:42 UTC\n2025-01-01 23:59:44 UTC\n\n\n2025-01-01\n\"0x100E38f7BCEc53937BDd79ADE46F…\n2876\n2025-01-01 00:00:12 UTC\n2025-01-01 23:59:44 UTC\n\n\n2025-01-01\n\"0x1476A65D7B5739dE1805d5130441…\n2462\n2025-01-01 00:00:12 UTC\n2025-01-01 23:59:44 UTC\n\n\n2025-01-01\n\"0x197B2c44b887c4aC01243BDE7E4b…\n2880\n2025-01-01 00:00:12 UTC\n2025-01-01 23:59:44 UTC\n\n\n…\n…\n…\n…\n…\n\n\n2025-01-01\n\"0xd625d50B0d087861c286d726eC51…\n2856\n2025-01-01 00:00:42 UTC\n2025-01-01 23:59:44 UTC\n\n\n2025-01-01\n\"0xdF239e0D5b4E6e820B0cFEF6972A…\n2880\n2025-01-01 00:00:12 UTC\n2025-01-01 23:59:44 UTC\n\n\n2025-01-01\n\"0xf10f56Bf0A28E0737c7e6bB0aF92…\n2880\n2025-01-01 00:00:12 UTC\n2025-01-01 23:59:44 UTC\n\n\n2025-01-01\n\"0xf34CD6c09a59d7D3d1a6C3dC231a…\n2834\n2025-01-01 00:00:42 UTC\n2025-01-01 23:59:44 UTC\n\n\n2025-01-01\n\"0xfD97FB8835d25740A2Da27c69762…\n2880\n2025-01-01 00:00:12 UTC\n2025-01-01 23:59:44 UTC\n\n\n\n\n\n\nThe count_submissions_that_day column highlights how submission frequency varies by day. Large variations may indicate operational issues such as outages or configuration problems.\nList of all Validators and their Fraction Out of Range\n\ndef fraction_out_of_range(\n    df: pl.DataFrame, \n    expected_interval: float = 0.5, \n    tolerance_factor: float = 0.05\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Compute the fraction of intervals that are outside the acceptable range\n    [expected_interval * (1 - tolerance_factor), expected_interval * (1 + tolerance_factor)]\n    per validator.\n    \"\"\"\n    low_bound = expected_interval * (1 - tolerance_factor)\n    high_bound = expected_interval * (1 + tolerance_factor)\n\n    df_out_of_range = (\n        df.lazy()\n        .with_columns(\n            pl.when(\n                (pl.col(\"submission_interval_min\") &lt; low_bound) | \n                (pl.col(\"submission_interval_min\") &gt; high_bound)\n            )\n            .then(pl.lit(1))\n            .otherwise(pl.lit(0))\n            .alias(\"out_of_range_flag\")\n        )\n        .group_by(\"Validator Address\")\n        .agg([\n            pl.count(\"submission_interval_min\").alias(\"n_intervals\"),\n            pl.sum(\"out_of_range_flag\").alias(\"n_out_of_range\")\n        ])\n        .with_columns(\n            (pl.col(\"n_out_of_range\") / pl.col(\"n_intervals\")).alias(\"fraction_out_of_range\")\n        )\n        .collect()\n    )\n    return df_out_of_range\n\ndf_out_of_range_metric = fraction_out_of_range(results[\"df_with_intervals\"])\ndf_out_of_range_metric = df_out_of_range_metric.sort(\"fraction_out_of_range\", descending=True)\n\nfor row in df_out_of_range_metric.to_dicts():\n    print(\n        f\"Validator {row['Validator Address']}: \"\n        f\"total={row['n_intervals']}, \"\n        f\"out_of_range={row['n_out_of_range']}, \"\n        f\"fraction_out_of_range={100*row['fraction_out_of_range']:.1f}%\"\n    )\n\nValidator 0x4cD134001EEF0843B9c69Ba9569d11fDcF4bd495: total=2822, out_of_range=122, fraction_out_of_range=4.3%\nValidator 0x2928FE5b911BCAf837cAd93eB9626E86a189f1dd: total=2828, out_of_range=51, fraction_out_of_range=1.8%\nValidator 0x19E356ebC20283fc74AF0BA4C179502A1F62fA7B: total=2832, out_of_range=48, fraction_out_of_range=1.7%\nValidator 0xbfDcAF35f52F9ef423ac8F2621F9eef8be6dEd17: total=2832, out_of_range=48, fraction_out_of_range=1.7%\nValidator 0xf34CD6c09a59d7D3d1a6C3dC231a7834E5615D6A: total=2833, out_of_range=47, fraction_out_of_range=1.7%\nValidator 0x01F788E4371a70D579C178Ea7F48E04e8B2CD743: total=2836, out_of_range=44, fraction_out_of_range=1.6%\nValidator 0x6747c02DE7eb2099265e55715Ba2E03e8563D051: total=2839, out_of_range=39, fraction_out_of_range=1.4%\nValidator 0xd625d50B0d087861c286d726eC51Cf4Bd9c54357: total=2855, out_of_range=23, fraction_out_of_range=0.8%\nValidator 0x00a96aaED75015Bb44cED878D927dcb15ec1FF54: total=2865, out_of_range=21, fraction_out_of_range=0.7%\nValidator 0x831B837C3DA1B6c2AB68a690206bDfF368877E19: total=2862, out_of_range=17, fraction_out_of_range=0.6%\nValidator 0xd61a48b0e11B0Dc6b7Bd713B1012563c52591BAA: total=2872, out_of_range=8, fraction_out_of_range=0.3%\nValidator 0x3AaF7817618728ffEF81898E11A3171C33faAE41: total=2873, out_of_range=6, fraction_out_of_range=0.2%\nValidator 0x6a395dE946c0493157404E2b1947493c633f569E: total=2873, out_of_range=6, fraction_out_of_range=0.2%\nValidator 0x100E38f7BCEc53937BDd79ADE46F34362470577B: total=2875, out_of_range=4, fraction_out_of_range=0.1%\nValidator 0x5E17e837DcBa2728C94f95c38fA8a47CB9C8818F: total=2875, out_of_range=4, fraction_out_of_range=0.1%\nValidator 0x7232e75a8bFd8c9ab002BB3A00eAa885BC72A6dd: total=2876, out_of_range=3, fraction_out_of_range=0.1%\nValidator 0x984A46Ec685Bb41A7BBb2bc39f80C78410ff4057: total=2876, out_of_range=3, fraction_out_of_range=0.1%\nValidator 0xBE287C82A786218E008FF97320b08244BE4A282c: total=2878, out_of_range=3, fraction_out_of_range=0.1%\nValidator 0x3597d2D42f8Fbbc82E8b1046048773aD6DDB717E: total=2879, out_of_range=2, fraction_out_of_range=0.1%\nValidator 0x1476A65D7B5739dE1805d5130441A94022Ee49fe: total=2461, out_of_range=1, fraction_out_of_range=0.0%\nValidator 0x8584A78A9b94f332A34BBf24D2AF83367Da31894: total=2878, out_of_range=1, fraction_out_of_range=0.0%\nValidator 0x8f91e0ADF8065C3fFF92297267E02DF32C2978FF: total=2878, out_of_range=1, fraction_out_of_range=0.0%\nValidator 0xDF2D0052ea56A860443039619f6DAe4434bc0Ac4: total=2878, out_of_range=1, fraction_out_of_range=0.0%\nValidator 0xEf0Ba5e345C2C3937df5667A870Aae5105CAa3a5: total=2878, out_of_range=1, fraction_out_of_range=0.0%\nValidator 0xc5B9d978715F081E226cb28bADB7Ba4cde5f9775: total=2878, out_of_range=1, fraction_out_of_range=0.0%\nValidator 0x197B2c44b887c4aC01243BDE7E4bBa8bd95BC3a8: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x1Be7f70BCf8393a7e4A5BcC66F6f15d6e35cfBBC: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x22A76e194A49c9e5508Cd4A3E1cD555D088ECB08: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x23b4Be9536F93b8D550214912fD0e38417Ff7209: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x24915749B793375a8C93090AF19928aFF1CAEcb6: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x26E2724dBD14Fbd52be430B97043AA4c83F05852: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x358488a4EdCA493FCD87610dcd50c62c8A3Dd658: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x36142A4f36974e2935192A1111C39330aA296D3C: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x383A3c437d3F12f60E5fC990119468D3561EfBfc: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x3fe573552E14a0FC11Da25E43Fef11e16a785068: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x527192F3D2408C84087607b7feE1d0f907821E17: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x551f3300FCFE0e392178b3542c009948008B2a9F: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x5603caFE3313D0cf56Fd4bE4A2f606dD6E43F8Eb: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x59031767f20EA8F4a3d90d33aB0DAA2ca469Fd9a: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x64F83c2538A646A550Ad9bEEb63427a377359DEE: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x718361fc3637199F24a2437331677D6B89a40519: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x791A7F840ac11841cCB0FaA968B2e3a0Db930fCe: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x94470A842Ea4f44e668EB9C2AB81367b6Ce01772: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x94d28f08Ff81A80f4716C0a8EfC6CAC2Ec74d09E: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x99E2B4B27BDe92b42D04B6CF302cF564D2C13b74: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x9C7dAABb5101623340C925CFD6fF74088ff5672e: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0x9d28e40E9Ec4789f9A0D17e421F76D8D0868EA44: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0xB5d8be2AB4b6d7E6be7Ea28E91b370223a06289f: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0xBBf36374eb23968F25aecAEbb97BF3118f3c2fEC: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0xC1F9acAF1824F6C906b35A0D2584D6E25077C7f5: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0xD9fDab408dF7Ae751691BeC2efE3b713ba3f9C36: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0xDCA5DFF3D42f2db3C18dBE823380A0A81db49A7E: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0xE4686A4C6E63A8ab51B458c52EB779AEcf0B74f7: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0xE9FFF86CAdC3136b3D94948B8Fd23631EDaa2dE3: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0xF9B38D02959379d43C764064dE201324d5e12931: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0xcdEed21b471b0Dc54faF74480A0E700fCc42a7b6: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0xcf716b3930d7cf6f2ADAD90A27c39fDc9D643BBd: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0xdF239e0D5b4E6e820B0cFEF6972A90893c2073AB: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0xf10f56Bf0A28E0737c7e6bB0aF92f3DDad34aE6a: total=2879, out_of_range=0, fraction_out_of_range=0.0%\nValidator 0xfD97FB8835d25740A2Da27c69762D74F6A931858: total=2879, out_of_range=0, fraction_out_of_range=0.0%\n\n\nPlease note, total represents the total number of consecutive submission intervals recorded for this validator (i.e. how many times we measured the time gap between one submission and the next). out_of_range indicates how many of those intervals fell outside the acceptable submission timeframe (e.g. 0.475–0.525 minutes for a 0.5-minute target). fraction_out_of_range shows what percentage of the validator’s intervals deviated from the acceptable range.",
    "crumbs": [
      "Notebooks",
      "Issue 2"
    ]
  },
  {
    "objectID": "notebooks/issue_5.html",
    "href": "notebooks/issue_5.html",
    "title": "Issue 5",
    "section": "",
    "text": "This notebook documents the analysis for Issue #5: Confidence Value Anomalies in the Autonity Oracle data. It covers:\n\nWhat is this issue about?\n\nWhy conduct this issue analysis?\n\nHow to conduct this issue analysis?\n\nWhat are the results?\n\n\n\n\nValidators submit a confidence metric alongside their price submissions, representing their certainty in the provided data. Potential anomalies include:\n\nValidators consistently submitting the same confidence value (e.g. always 50 or 100).\nFrequent occurrences of zero or null confidence values.\nConfidence values that do not vary in response to market volatility or price changes.\n\nThis analysis investigates the patterns and consistency of these confidence values.\n\n\n\n\n\nReliability Check: Confidence should reflect real uncertainty, not remain fixed or arbitrary.\nSystem Integrity: Identifying anomalies helps ensure that validators comply with expected behavior prior to Mainnet launch.\nDecision-making: Confidence anomalies can degrade decision quality in downstream applications relying on Oracle data.\n\n\n\n\n\nUse Python with the Polars library (v1.24.0) to:\n\nLoad and preprocess Oracle submission CSV files.\nDetect all “Confidence” columns automatically.\nCalculate basic statistics for confidence values per validator and pair:\n\nMinimum, maximum, mean, standard deviation, number of distinct confidence values.\nFrequency distribution of the most common confidence values.\n\nIdentify validators that consistently submit fixed or zero variation confidence values.\nCalculate correlation between confidence and price changes:\n\nCompute price volatility as absolute price differences between submissions.\nEvaluate the correlation to verify if confidence values genuinely reflect market volatility.\n\n\nBelow is the analysis script:\n\nimport polars as pl\nimport glob\nimport statistics\nimport math\nfrom itertools import groupby\nfrom typing import List\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndef load_and_preprocess_submissions(submission_glob: str) -&gt; pl.DataFrame:\n    \"\"\"\n    Loads Oracle Submission CSVs and returns a Polars DataFrame.\n    \"\"\"\n    files = sorted(glob.glob(submission_glob))\n    if not files:\n        raise ValueError(f\"No CSV files found matching pattern {submission_glob}\")\n\n    lf_list = []\n    for f in files:\n        lf_temp = pl.scan_csv(\n            f,\n            dtypes={\"Timestamp\": pl.Utf8},\n            null_values=[\"\"],\n            ignore_errors=True,\n        )\n        lf_list.append(lf_temp)\n\n    lf = pl.concat(lf_list)\n    lf = lf.with_columns(\n        pl.col(\"Timestamp\")\n        .str.strptime(pl.Datetime, strict=False)\n        .alias(\"Timestamp_dt\")\n    )\n\n    lf = lf.with_columns(\n        [\n            pl.col(\"Timestamp_dt\").cast(pl.Date).alias(\"date_only\"),\n            pl.col(\"Timestamp_dt\").dt.weekday().alias(\"weekday_num\"),\n        ]\n    )\n\n    return lf.collect()\n\n\ndef list_confidence_columns(df: pl.DataFrame) -&gt; List[str]:\n    \"\"\"\n    Identifies all \"Confidence\" columns by name.\n    \"\"\"\n    return [c for c in df.columns if \"Confidence\" in c]\n\n\ndef list_price_columns(df: pl.DataFrame) -&gt; List[str]:\n    \"\"\"\n    Identifies all \"Price\" columns by name.\n    \"\"\"\n    return [c for c in df.columns if \"Price\" in c]\n\n\ndef confidence_distribution_by_validator(\n    df: pl.DataFrame, confidence_cols: List[str], top_k_freq: int = 3\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Summarizes the distribution of ALL confidence values per validator (aggregated over all pairs).\n    Returns min/max/mean/std/distinct count, plus the top frequency values.\n    \"\"\"\n    keep_cols = [\"Validator Address\", \"Timestamp_dt\"] + confidence_cols\n    df_small = df.select([c for c in keep_cols if c in df.columns])\n\n    df_long = df_small.melt(\n        id_vars=[\"Validator Address\", \"Timestamp_dt\"],\n        value_vars=confidence_cols,\n        variable_name=\"confidence_col\",\n        value_name=\"confidence_val\",\n    )\n\n    lf_long = df_long.lazy()\n\n    grouped_basic = lf_long.group_by([\"Validator Address\"]).agg(\n        [\n            pl.count(\"confidence_val\").alias(\"count_rows\"),\n            pl.min(\"confidence_val\").alias(\"min_conf\"),\n            pl.max(\"confidence_val\").alias(\"max_conf\"),\n            pl.mean(\"confidence_val\").alias(\"mean_conf\"),\n            pl.std(\"confidence_val\").alias(\"std_conf\"),\n            pl.n_unique(\"confidence_val\").alias(\"distinct_values_count\"),\n        ]\n    )\n    df_basic = grouped_basic.collect()\n\n    freq_lf = (\n        lf_long.group_by([\"Validator Address\", \"confidence_val\"])\n        .agg(pl.count(\"confidence_val\").alias(\"value_count\"))\n        .sort(\n            [\"Validator Address\", \"value_count\"],\n            descending=[False, True],\n        )\n    )\n    df_freq = freq_lf.collect()\n\n    def top_k_values_string(rows: list, k: int):\n        \"\"\"\n        Return a string with format: val(count), val2(count2), ...\n        e.g. '0(14), 35(5), 100(3)'\n        \"\"\"\n        parts = []\n        for r in rows[:k]:\n            cval = r[\"confidence_val\"]\n            count_ = r[\"value_count\"]\n            if cval is None:\n                val_str = \"null\"\n            else:\n                val_str = str(int(cval))\n            parts.append(f\"{val_str}({count_})\")\n        return \", \".join(parts)\n\n    def top_k_values_list(rows: list, k: int):\n        \"\"\"\n        Return just the numeric confidence values in the top k, ignoring their counts.\n        \"\"\"\n        out = []\n        for r in rows[:k]:\n            cval = r[\"confidence_val\"]\n            if cval is not None:\n                out.append(int(cval))\n        return out\n\n    validator_map = {}\n    freq_dicts = df_freq.to_dicts()\n\n    for key, group in groupby(freq_dicts, key=lambda d: d[\"Validator Address\"]):\n        group_list = list(group)\n        group_list_sorted = sorted(\n            group_list, key=lambda x: x[\"value_count\"], reverse=True\n        )\n        freq_str = top_k_values_string(group_list_sorted, top_k_freq)\n        freq_list = top_k_values_list(group_list_sorted, top_k_freq)\n        validator_map[key] = {\"freq_str\": freq_str, \"freq_list\": freq_list}\n\n    df_freq_map = pl.DataFrame(\n        {\n            \"Validator Address\": list(validator_map.keys()),\n            \"top_freq_values_str\": [v[\"freq_str\"] for v in validator_map.values()],\n            \"top_freq_values_list\": [v[\"freq_list\"] for v in validator_map.values()],\n        }\n    )\n\n    df_merged = df_basic.join(\n        df_freq_map, on=[\"Validator Address\"], how=\"left\"\n    )\n\n    df_merged = df_merged.with_columns(\n        [\n            (pl.col(\"max_conf\") == pl.col(\"min_conf\")).alias(\"zero_variation\"),\n            (pl.col(\"distinct_values_count\") == 1).alias(\"only_one_value\"),\n            pl.col(\"top_freq_values_list\").list.contains(0).alias(\"has_zero_conf\"),\n        ]\n    )\n\n    return df_merged.sort([\"Validator Address\"])\n\n\ndef check_confidence_vs_price_correlation(\n    df: pl.DataFrame, fx_pairs: List[str], autonity_pairs: List[str]\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Measures how well Confidence tracks price changes (absolute difference from previous submission).\n    Note: This still checks each 'Price' column vs. its corresponding 'Confidence' column.\n    \"\"\"\n    df_local = df.clone()\n\n    price_cols = fx_pairs + autonity_pairs\n\n    new_cols = []\n    for pc in price_cols:\n        decimal_col = pc.replace(\" Price\", \" Price Decimal\")\n        new_cols.append((pl.col(pc).cast(pl.Float64) / 1e18).alias(decimal_col))\n    df_local = df_local.with_columns(new_cols)\n\n    results_rows = []\n\n    def base_name(price_col: str) -&gt; str:\n        return price_col.replace(\" Price\", \"\")\n\n    for pc in price_cols:\n        conf_col = pc.replace(\"Price\", \"Confidence\").strip()\n        dec_col = pc.replace(\" Price\", \" Price Decimal\")\n        pair_lbl = base_name(pc)\n\n        if conf_col not in df_local.columns or dec_col not in df_local.columns:\n            continue\n\n        df_pair = (\n            df_local\n            .select([\"Validator Address\", \"Timestamp_dt\", dec_col, conf_col])\n            .filter(pl.col(dec_col).is_not_null() & pl.col(conf_col).is_not_null())\n            .sort([\"Validator Address\", \"Timestamp_dt\"])\n        )\n\n        df_pair = df_pair.with_columns(\n            (pl.col(dec_col) - pl.col(dec_col).shift(1))\n            .over(\"Validator Address\")\n            .abs()\n            .alias(\"abs_price_change\")\n        )\n\n        lf_cor = (\n            df_pair.lazy()\n            .group_by(\"Validator Address\")\n            .agg(\n                [\n                    pl.col(\"abs_price_change\").alias(\"price_change_list\"),\n                    pl.col(conf_col).alias(\"confidence_list\"),\n                ]\n            )\n        )\n        local_rows = lf_cor.collect().to_dicts()\n\n        for row in local_rows:\n            validator = row[\"Validator Address\"]\n            pc_list = row[\"price_change_list\"]\n            conf_list = row[\"confidence_list\"]\n\n            if len(pc_list) &lt; 2:\n                corr_val = None\n            else:\n                corr_val = pearson_correlation(pc_list, conf_list)\n            results_rows.append(\n                {\n                    \"Validator Address\": validator,\n                    \"pair_label\": pair_lbl,\n                    \"corr_conf_price_change\": corr_val,\n                    \"num_points\": len(pc_list),\n                }\n            )\n\n    df_corr = pl.DataFrame(results_rows)\n    return df_corr.sort([\"pair_label\", \"Validator Address\"])\n\n\ndef pearson_correlation(xs, ys):\n    \"\"\"\n    Computes Pearson correlation between two lists of floats.\n    \"\"\"\n    clean_data = [\n        (x, y)\n        for (x, y) in zip(xs, ys)\n        if (x is not None and y is not None and not math.isnan(x) and not math.isnan(y))\n    ]\n    if len(clean_data) &lt; 2:\n        return None\n\n    xs_clean, ys_clean = zip(*clean_data)\n    mean_x = statistics.mean(xs_clean)\n    mean_y = statistics.mean(ys_clean)\n    num = sum((x - mean_x) * (y - mean_y) for x, y in zip(xs_clean, ys_clean))\n    den_x = math.sqrt(sum((x - mean_x) ** 2 for x in xs_clean))\n    den_y = math.sqrt(sum((y - mean_y) ** 2 for y in ys_clean))\n    if den_x == 0 or den_y == 0:\n        return None\n    return num / (den_x * den_y)\n\n\ndef analyze_confidence_value_anomalies(\n    submission_glob: str, fx_pairs: List[str], autonity_pairs: List[str]\n):\n    \"\"\"\n    Main analysis function.\n    \"\"\"\n    df_all = load_and_preprocess_submissions(submission_glob)\n\n    conf_cols = list_confidence_columns(df_all)\n\n    df_conf_dist = confidence_distribution_by_validator(\n        df_all, conf_cols, top_k_freq=3\n    )\n\n    df_anomalies = df_conf_dist.filter(\n        pl.col(\"zero_variation\") | pl.col(\"only_one_value\")\n    )\n\n    df_corr = check_confidence_vs_price_correlation(df_all, fx_pairs, autonity_pairs)\n\n    return {\n        \"df_confidence_distribution\": df_conf_dist,\n        \"df_confidence_anomalies\": df_anomalies,\n        \"df_correlation_price_change\": df_corr,\n    }\n\n\nfx_price_cols = [\n    \"AUD-USD Price\",\n    \"CAD-USD Price\",\n    \"EUR-USD Price\",\n    \"GBP-USD Price\",\n    \"JPY-USD Price\",\n    \"SEK-USD Price\",\n]\nautonity_price_cols = [\n    \"ATN-USD Price\",\n    \"NTN-USD Price\",\n    \"NTN-ATN Price\",\n]\n\nresults = analyze_confidence_value_anomalies(\n    submission_glob=\"../submission-data/Oracle_Submission_*.csv\",\n    fx_pairs=fx_price_cols,\n    autonity_pairs=autonity_price_cols,\n)\n\n\n\n\n\nBelow directly reference results generated by the analysis.\nConfidence Value Distribution\n\nresults[\"df_confidence_distribution\"]\n\n\nshape: (60, 12)\n\n\n\nValidator Address\ncount_rows\nmin_conf\nmax_conf\nmean_conf\nstd_conf\ndistinct_values_count\ntop_freq_values_str\ntop_freq_values_list\nzero_variation\nonly_one_value\nhas_zero_conf\n\n\nstr\nu32\ni64\ni64\nf64\nf64\nu32\nstr\nlist[i64]\nbool\nbool\nbool\n\n\n\n\n\"0x00a96aaED75015Bb44cED878D927…\n25785\n90\n100\n99.902269\n0.983768\n3\n\"100(25533), 90(252), null(0)\"\n[100, 90]\nfalse\nfalse\nfalse\n\n\n\"0x01F788E4371a70D579C178Ea7F48…\n25146\n90\n100\n99.904557\n0.972293\n3\n\"100(24906), 90(240), null(0)\"\n[100, 90]\nfalse\nfalse\nfalse\n\n\n\"0x100E38f7BCEc53937BDd79ADE46F…\n0\nnull\nnull\nnull\nnull\n1\n\"null(0)\"\n[]\nnull\ntrue\nfalse\n\n\n\"0x1476A65D7B5739dE1805d5130441…\n22149\n100\n100\n100.0\n0.0\n2\n\"100(22149), null(0)\"\n[100]\ntrue\nfalse\nfalse\n\n\n\"0x197B2c44b887c4aC01243BDE7E4b…\n25920\n90\n100\n99.902778\n0.981227\n2\n\"100(25668), 90(252)\"\n[100, 90]\nfalse\nfalse\nfalse\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"0xd625d50B0d087861c286d726eC51…\n0\nnull\nnull\nnull\nnull\n1\n\"null(0)\"\n[]\nnull\ntrue\nfalse\n\n\n\"0xdF239e0D5b4E6e820B0cFEF6972A…\n0\nnull\nnull\nnull\nnull\n1\n\"null(0)\"\n[]\nnull\ntrue\nfalse\n\n\n\"0xf10f56Bf0A28E0737c7e6bB0aF92…\n25920\n90\n100\n99.902778\n0.981227\n2\n\"100(25668), 90(252)\"\n[100, 90]\nfalse\nfalse\nfalse\n\n\n\"0xf34CD6c09a59d7D3d1a6C3dC231a…\n25092\n100\n100\n100.0\n0.0\n2\n\"100(25092), null(0)\"\n[100]\ntrue\nfalse\nfalse\n\n\n\"0xfD97FB8835d25740A2Da27c69762…\n25920\n90\n100\n99.902778\n0.981227\n2\n\"100(25668), 90(252)\"\n[100, 90]\nfalse\nfalse\nfalse\n\n\n\n\n\n\n\nKey indicators:\n\nMean/std: Low or zero standard deviation indicates fixed or rarely-changing confidence.\nDistinct values count: Few distinct values may indicate hard-coded or rarely adjusted confidence.\n\n\nIdentified Anomalies (Zero or Single-Value Confidence)\n\nresults[\"df_confidence_anomalies\"]\n\n\nshape: (11, 12)\n\n\n\nValidator Address\ncount_rows\nmin_conf\nmax_conf\nmean_conf\nstd_conf\ndistinct_values_count\ntop_freq_values_str\ntop_freq_values_list\nzero_variation\nonly_one_value\nhas_zero_conf\n\n\nstr\nu32\ni64\ni64\nf64\nf64\nu32\nstr\nlist[i64]\nbool\nbool\nbool\n\n\n\n\n\"0x100E38f7BCEc53937BDd79ADE46F…\n0\nnull\nnull\nnull\nnull\n1\n\"null(0)\"\n[]\nnull\ntrue\nfalse\n\n\n\"0x1476A65D7B5739dE1805d5130441…\n22149\n100\n100\n100.0\n0.0\n2\n\"100(22149), null(0)\"\n[100]\ntrue\nfalse\nfalse\n\n\n\"0x26E2724dBD14Fbd52be430B97043…\n0\nnull\nnull\nnull\nnull\n1\n\"null(0)\"\n[]\nnull\ntrue\nfalse\n\n\n\"0x383A3c437d3F12f60E5fC9901194…\n25920\n100\n100\n100.0\n0.0\n1\n\"100(25920)\"\n[100]\ntrue\ntrue\nfalse\n\n\n\"0x3fe573552E14a0FC11Da25E43Fef…\n0\nnull\nnull\nnull\nnull\n1\n\"null(0)\"\n[]\nnull\ntrue\nfalse\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"0xDF2D0052ea56A860443039619f6D…\n25902\n100\n100\n100.0\n0.0\n2\n\"100(25902), null(0)\"\n[100]\ntrue\nfalse\nfalse\n\n\n\"0xcdEed21b471b0Dc54faF74480A0E…\n25920\n100\n100\n100.0\n0.0\n1\n\"100(25920)\"\n[100]\ntrue\ntrue\nfalse\n\n\n\"0xd625d50B0d087861c286d726eC51…\n0\nnull\nnull\nnull\nnull\n1\n\"null(0)\"\n[]\nnull\ntrue\nfalse\n\n\n\"0xdF239e0D5b4E6e820B0cFEF6972A…\n0\nnull\nnull\nnull\nnull\n1\n\"null(0)\"\n[]\nnull\ntrue\nfalse\n\n\n\"0xf34CD6c09a59d7D3d1a6C3dC231a…\n25092\n100\n100\n100.0\n0.0\n2\n\"100(25092), null(0)\"\n[100]\ntrue\nfalse\nfalse\n\n\n\n\n\n\n\nRows indicate validators consistently providing identical confidence, suggesting potential misconfiguration or logic errors.\n\nCorrelation Between Confidence and Price Changes\n\nresults[\"df_correlation_price_change\"].filter(pl.col(\"num_points\") &gt; 2)\n\n\nshape: (495, 4)\n\n\n\nValidator Address\npair_label\ncorr_conf_price_change\nnum_points\n\n\nstr\nstr\nf64\ni64\n\n\n\n\n\"0x00a96aaED75015Bb44cED878D927…\n\"ATN-USD\"\nnull\n2865\n\n\n\"0x01F788E4371a70D579C178Ea7F48…\n\"ATN-USD\"\nnull\n2794\n\n\n\"0x1476A65D7B5739dE1805d5130441…\n\"ATN-USD\"\nnull\n2461\n\n\n\"0x197B2c44b887c4aC01243BDE7E4b…\n\"ATN-USD\"\nnull\n2880\n\n\n\"0x19E356ebC20283fc74AF0BA4C179…\n\"ATN-USD\"\nnull\n2786\n\n\n…\n…\n…\n…\n\n\n\"0xcf716b3930d7cf6f2ADAD90A27c3…\n\"SEK-USD\"\n0.006676\n2880\n\n\n\"0xd61a48b0e11B0Dc6b7Bd713B1012…\n\"SEK-USD\"\nnull\n2866\n\n\n\"0xf10f56Bf0A28E0737c7e6bB0aF92…\n\"SEK-USD\"\n0.00658\n2880\n\n\n\"0xf34CD6c09a59d7D3d1a6C3dC231a…\n\"SEK-USD\"\nnull\n2788\n\n\n\"0xfD97FB8835d25740A2Da27c69762…\n\"SEK-USD\"\n-0.112979\n2880\n\n\n\n\n\n\nNote: You may observe many null values in the corr_conf_price_change column. This can happen if a validator always provides the same confidence (zero variance in confidence values) or the price change for the given pair is often 0 or extremely small (zero variance in price).\n\nCorrelation (corr_conf_price_change) near zero implies confidence metrics not aligned with real market volatility. Strong correlation (positive or negative) suggests meaningful responsiveness.\n\nInterpretation of Results\n\ndf_corr_fixed = results[\"df_correlation_price_change\"].with_columns(\n    pl.col(\"corr_conf_price_change\").cast(pl.Float64)\n)\n\nnum_anomalies = results[\"df_confidence_anomalies\"].height\nprint(f\"Validators with fixed confidence values: {num_anomalies}\")\n\nlow_corr_count = df_corr_fixed.filter(\n    (pl.col(\"corr_conf_price_change\").abs() &lt; 0.1)\n    & (pl.col(\"corr_conf_price_change\").is_not_null())\n).height\n\nprint(f\"Number of validator-currency pair combinations with low correlation (&lt;0.1): {low_corr_count}\")\n\nif num_anomalies &gt; 0:\n    print(\"Identified validators with potentially hard-coded or fixed confidence values.\")\nelse:\n    print(\"No significant anomalies in confidence values identified.\")\n\nif low_corr_count &gt; 0:\n    print(\"Confidence values for many validators do not adequately reflect market volatility.\")\nelse:\n    print(\"Confidence values generally align well with price changes.\")\n\nValidators with fixed confidence values: 11\nNumber of validator-currency pair combinations with low correlation (&lt;0.1): 143\nIdentified validators with potentially hard-coded or fixed confidence values.\nConfidence values for many validators do not adequately reflect market volatility.\n\n\nList of all Validators and their Standard Deviations\n\ndef compute_variation_metrics(df: pl.DataFrame, confidence_cols: List[str]) -&gt; pl.DataFrame:\n    \"\"\"\n    For each validator, compute:\n      - min_conf, max_conf, mean_conf, std_conf\n      - distinct_values_count, fraction_zero\n    \"\"\"\n    # Keep relevant columns\n    keep_cols = [\"Validator Address\", \"Timestamp_dt\"] + confidence_cols\n    df_small = df.select([c for c in keep_cols if c in df.columns])\n\n    # Reshape into long form\n    df_long = df_small.melt(\n        id_vars=[\"Validator Address\", \"Timestamp_dt\"],\n        value_vars=confidence_cols,\n        variable_name=\"confidence_col\",\n        value_name=\"confidence_val\",\n    )\n\n    # Group by validator only\n    metrics_lf = (\n        df_long.lazy()\n        .group_by([\"Validator Address\"])\n        .agg([\n            pl.min(\"confidence_val\").alias(\"min_conf\"),\n            pl.max(\"confidence_val\").alias(\"max_conf\"),\n            pl.mean(\"confidence_val\").alias(\"mean_conf\"),\n            pl.std(\"confidence_val\").alias(\"std_conf\"),\n            pl.n_unique(\"confidence_val\").alias(\"distinct_values_count\"),\n            (pl.col(\"confidence_val\") == 0).sum().alias(\"count_zero\"),\n            pl.count(\"confidence_val\").alias(\"count_total\"),\n        ])\n        .with_columns([\n            (pl.col(\"count_zero\") / pl.col(\"count_total\")).alias(\"fraction_zero\")\n        ])\n    )\n\n    return metrics_lf.collect().sort([\"Validator Address\"])\n\n\ndf_all_variation = load_and_preprocess_submissions(\"../submission-data/Oracle_Submission_*.csv\")\nall_conf_cols = list_confidence_columns(df_all_variation)\ndf_variation_metrics = compute_variation_metrics(df_all_variation, all_conf_cols)\ndf_variation_metrics = df_variation_metrics.sort(\"std_conf\", descending=False)\n\nfor row in df_variation_metrics.to_dicts():\n    mean_conf = row['mean_conf']\n    if mean_conf is None:\n        mean_conf = \"0\"\n    else:\n        mean_conf = round(mean_conf, 1)\n    std_conf = row['std_conf']\n    if std_conf is None:\n        std_conf = \"0\"\n    else:\n        std_conf = round(std_conf, 1)\n    print(\n        f\"Validator {row['Validator Address']}: \"\n        f\"min_conf={row['min_conf']}, \"\n        f\"max_conf={row['max_conf']}, \"\n        f\"mean_conf={mean_conf}, \"\n        f\"std_conf={std_conf}, \"\n    )\n\nValidator 0x100E38f7BCEc53937BDd79ADE46F34362470577B: min_conf=None, max_conf=None, mean_conf=0, std_conf=0, \nValidator 0x26E2724dBD14Fbd52be430B97043AA4c83F05852: min_conf=None, max_conf=None, mean_conf=0, std_conf=0, \nValidator 0x3fe573552E14a0FC11Da25E43Fef11e16a785068: min_conf=None, max_conf=None, mean_conf=0, std_conf=0, \nValidator 0xd625d50B0d087861c286d726eC51Cf4Bd9c54357: min_conf=None, max_conf=None, mean_conf=0, std_conf=0, \nValidator 0xdF239e0D5b4E6e820B0cFEF6972A90893c2073AB: min_conf=None, max_conf=None, mean_conf=0, std_conf=0, \nValidator 0x1476A65D7B5739dE1805d5130441A94022Ee49fe: min_conf=100, max_conf=100, mean_conf=100.0, std_conf=0.0, \nValidator 0x383A3c437d3F12f60E5fC990119468D3561EfBfc: min_conf=100, max_conf=100, mean_conf=100.0, std_conf=0.0, \nValidator 0x6747c02DE7eb2099265e55715Ba2E03e8563D051: min_conf=100, max_conf=100, mean_conf=100.0, std_conf=0.0, \nValidator 0xDF2D0052ea56A860443039619f6DAe4434bc0Ac4: min_conf=100, max_conf=100, mean_conf=100.0, std_conf=0.0, \nValidator 0xcdEed21b471b0Dc54faF74480A0E700fCc42a7b6: min_conf=100, max_conf=100, mean_conf=100.0, std_conf=0.0, \nValidator 0xf34CD6c09a59d7D3d1a6C3dC231a7834E5615D6A: min_conf=100, max_conf=100, mean_conf=100.0, std_conf=0.0, \nValidator 0x01F788E4371a70D579C178Ea7F48E04e8B2CD743: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x19E356ebC20283fc74AF0BA4C179502A1F62fA7B: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0xbfDcAF35f52F9ef423ac8F2621F9eef8be6dEd17: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x2928FE5b911BCAf837cAd93eB9626E86a189f1dd: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x4cD134001EEF0843B9c69Ba9569d11fDcF4bd495: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x197B2c44b887c4aC01243BDE7E4bBa8bd95BC3a8: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x23b4Be9536F93b8D550214912fD0e38417Ff7209: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x24915749B793375a8C93090AF19928aFF1CAEcb6: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x358488a4EdCA493FCD87610dcd50c62c8A3Dd658: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x5603caFE3313D0cf56Fd4bE4A2f606dD6E43F8Eb: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x59031767f20EA8F4a3d90d33aB0DAA2ca469Fd9a: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x64F83c2538A646A550Ad9bEEb63427a377359DEE: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x791A7F840ac11841cCB0FaA968B2e3a0Db930fCe: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x99E2B4B27BDe92b42D04B6CF302cF564D2C13b74: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x9C7dAABb5101623340C925CFD6fF74088ff5672e: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0xC1F9acAF1824F6C906b35A0D2584D6E25077C7f5: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0xD9fDab408dF7Ae751691BeC2efE3b713ba3f9C36: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0xcf716b3930d7cf6f2ADAD90A27c39fDc9D643BBd: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0xf10f56Bf0A28E0737c7e6bB0aF92f3DDad34aE6a: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0xfD97FB8835d25740A2Da27c69762D74F6A931858: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x8584A78A9b94f332A34BBf24D2AF83367Da31894: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x984A46Ec685Bb41A7BBb2bc39f80C78410ff4057: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x00a96aaED75015Bb44cED878D927dcb15ec1FF54: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x36142A4f36974e2935192A1111C39330aA296D3C: min_conf=90, max_conf=100, mean_conf=93.3, std_conf=4.7, \nValidator 0x718361fc3637199F24a2437331677D6B89a40519: min_conf=90, max_conf=100, mean_conf=93.3, std_conf=4.7, \nValidator 0x94d28f08Ff81A80f4716C0a8EfC6CAC2Ec74d09E: min_conf=90, max_conf=100, mean_conf=93.3, std_conf=4.7, \nValidator 0xE9FFF86CAdC3136b3D94948B8Fd23631EDaa2dE3: min_conf=90, max_conf=100, mean_conf=93.3, std_conf=4.7, \nValidator 0xc5B9d978715F081E226cb28bADB7Ba4cde5f9775: min_conf=90, max_conf=100, mean_conf=93.3, std_conf=4.7, \nValidator 0x6a395dE946c0493157404E2b1947493c633f569E: min_conf=90, max_conf=100, mean_conf=93.3, std_conf=4.7, \nValidator 0xF9B38D02959379d43C764064dE201324d5e12931: min_conf=70, max_conf=100, mean_conf=93.1, std_conf=5.2, \nValidator 0xBE287C82A786218E008FF97320b08244BE4A282c: min_conf=70, max_conf=100, mean_conf=93.1, std_conf=5.2, \nValidator 0x7232e75a8bFd8c9ab002BB3A00eAa885BC72A6dd: min_conf=70, max_conf=100, mean_conf=93.1, std_conf=5.2, \nValidator 0x3AaF7817618728ffEF81898E11A3171C33faAE41: min_conf=70, max_conf=100, mean_conf=93.1, std_conf=5.2, \nValidator 0x5E17e837DcBa2728C94f95c38fA8a47CB9C8818F: min_conf=70, max_conf=100, mean_conf=93.1, std_conf=5.2, \nValidator 0x1Be7f70BCf8393a7e4A5BcC66F6f15d6e35cfBBC: min_conf=50, max_conf=100, mean_conf=66.7, std_conf=23.6, \nValidator 0x22A76e194A49c9e5508Cd4A3E1cD555D088ECB08: min_conf=50, max_conf=100, mean_conf=66.7, std_conf=23.6, \nValidator 0x3597d2D42f8Fbbc82E8b1046048773aD6DDB717E: min_conf=50, max_conf=100, mean_conf=66.7, std_conf=23.6, \nValidator 0x527192F3D2408C84087607b7feE1d0f907821E17: min_conf=50, max_conf=100, mean_conf=66.7, std_conf=23.6, \nValidator 0x551f3300FCFE0e392178b3542c009948008B2a9F: min_conf=50, max_conf=100, mean_conf=66.7, std_conf=23.6, \nValidator 0x94470A842Ea4f44e668EB9C2AB81367b6Ce01772: min_conf=50, max_conf=100, mean_conf=66.7, std_conf=23.6, \nValidator 0x9d28e40E9Ec4789f9A0D17e421F76D8D0868EA44: min_conf=50, max_conf=100, mean_conf=66.7, std_conf=23.6, \nValidator 0xB5d8be2AB4b6d7E6be7Ea28E91b370223a06289f: min_conf=50, max_conf=100, mean_conf=66.7, std_conf=23.6, \nValidator 0xBBf36374eb23968F25aecAEbb97BF3118f3c2fEC: min_conf=50, max_conf=100, mean_conf=66.7, std_conf=23.6, \nValidator 0xDCA5DFF3D42f2db3C18dBE823380A0A81db49A7E: min_conf=50, max_conf=100, mean_conf=66.7, std_conf=23.6, \nValidator 0xE4686A4C6E63A8ab51B458c52EB779AEcf0B74f7: min_conf=50, max_conf=100, mean_conf=66.7, std_conf=23.6, \nValidator 0x8f91e0ADF8065C3fFF92297267E02DF32C2978FF: min_conf=50, max_conf=100, mean_conf=66.7, std_conf=23.6, \nValidator 0xEf0Ba5e345C2C3937df5667A870Aae5105CAa3a5: min_conf=50, max_conf=100, mean_conf=66.7, std_conf=23.6, \nValidator 0xd61a48b0e11B0Dc6b7Bd713B1012563c52591BAA: min_conf=50, max_conf=100, mean_conf=66.7, std_conf=23.6, \nValidator 0x831B837C3DA1B6c2AB68a690206bDfF368877E19: min_conf=50, max_conf=100, mean_conf=66.7, std_conf=23.6, \n\n\nPlease note, min_conf, max_conf, mean_conf and std_conf are the minimum, maximum, mean and standard deviation of the confidence values this validator provided across all submissions.",
    "crumbs": [
      "Notebooks",
      "Issue 5"
    ]
  },
  {
    "objectID": "notebooks/issue_5.html#confidence-value-anomalies",
    "href": "notebooks/issue_5.html#confidence-value-anomalies",
    "title": "Issue 5",
    "section": "",
    "text": "This notebook documents the analysis for Issue #5: Confidence Value Anomalies in the Autonity Oracle data. It covers:\n\nWhat is this issue about?\n\nWhy conduct this issue analysis?\n\nHow to conduct this issue analysis?\n\nWhat are the results?\n\n\n\n\nValidators submit a confidence metric alongside their price submissions, representing their certainty in the provided data. Potential anomalies include:\n\nValidators consistently submitting the same confidence value (e.g. always 50 or 100).\nFrequent occurrences of zero or null confidence values.\nConfidence values that do not vary in response to market volatility or price changes.\n\nThis analysis investigates the patterns and consistency of these confidence values.\n\n\n\n\n\nReliability Check: Confidence should reflect real uncertainty, not remain fixed or arbitrary.\nSystem Integrity: Identifying anomalies helps ensure that validators comply with expected behavior prior to Mainnet launch.\nDecision-making: Confidence anomalies can degrade decision quality in downstream applications relying on Oracle data.\n\n\n\n\n\nUse Python with the Polars library (v1.24.0) to:\n\nLoad and preprocess Oracle submission CSV files.\nDetect all “Confidence” columns automatically.\nCalculate basic statistics for confidence values per validator and pair:\n\nMinimum, maximum, mean, standard deviation, number of distinct confidence values.\nFrequency distribution of the most common confidence values.\n\nIdentify validators that consistently submit fixed or zero variation confidence values.\nCalculate correlation between confidence and price changes:\n\nCompute price volatility as absolute price differences between submissions.\nEvaluate the correlation to verify if confidence values genuinely reflect market volatility.\n\n\nBelow is the analysis script:\n\nimport polars as pl\nimport glob\nimport statistics\nimport math\nfrom itertools import groupby\nfrom typing import List\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndef load_and_preprocess_submissions(submission_glob: str) -&gt; pl.DataFrame:\n    \"\"\"\n    Loads Oracle Submission CSVs and returns a Polars DataFrame.\n    \"\"\"\n    files = sorted(glob.glob(submission_glob))\n    if not files:\n        raise ValueError(f\"No CSV files found matching pattern {submission_glob}\")\n\n    lf_list = []\n    for f in files:\n        lf_temp = pl.scan_csv(\n            f,\n            dtypes={\"Timestamp\": pl.Utf8},\n            null_values=[\"\"],\n            ignore_errors=True,\n        )\n        lf_list.append(lf_temp)\n\n    lf = pl.concat(lf_list)\n    lf = lf.with_columns(\n        pl.col(\"Timestamp\")\n        .str.strptime(pl.Datetime, strict=False)\n        .alias(\"Timestamp_dt\")\n    )\n\n    lf = lf.with_columns(\n        [\n            pl.col(\"Timestamp_dt\").cast(pl.Date).alias(\"date_only\"),\n            pl.col(\"Timestamp_dt\").dt.weekday().alias(\"weekday_num\"),\n        ]\n    )\n\n    return lf.collect()\n\n\ndef list_confidence_columns(df: pl.DataFrame) -&gt; List[str]:\n    \"\"\"\n    Identifies all \"Confidence\" columns by name.\n    \"\"\"\n    return [c for c in df.columns if \"Confidence\" in c]\n\n\ndef list_price_columns(df: pl.DataFrame) -&gt; List[str]:\n    \"\"\"\n    Identifies all \"Price\" columns by name.\n    \"\"\"\n    return [c for c in df.columns if \"Price\" in c]\n\n\ndef confidence_distribution_by_validator(\n    df: pl.DataFrame, confidence_cols: List[str], top_k_freq: int = 3\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Summarizes the distribution of ALL confidence values per validator (aggregated over all pairs).\n    Returns min/max/mean/std/distinct count, plus the top frequency values.\n    \"\"\"\n    keep_cols = [\"Validator Address\", \"Timestamp_dt\"] + confidence_cols\n    df_small = df.select([c for c in keep_cols if c in df.columns])\n\n    df_long = df_small.melt(\n        id_vars=[\"Validator Address\", \"Timestamp_dt\"],\n        value_vars=confidence_cols,\n        variable_name=\"confidence_col\",\n        value_name=\"confidence_val\",\n    )\n\n    lf_long = df_long.lazy()\n\n    grouped_basic = lf_long.group_by([\"Validator Address\"]).agg(\n        [\n            pl.count(\"confidence_val\").alias(\"count_rows\"),\n            pl.min(\"confidence_val\").alias(\"min_conf\"),\n            pl.max(\"confidence_val\").alias(\"max_conf\"),\n            pl.mean(\"confidence_val\").alias(\"mean_conf\"),\n            pl.std(\"confidence_val\").alias(\"std_conf\"),\n            pl.n_unique(\"confidence_val\").alias(\"distinct_values_count\"),\n        ]\n    )\n    df_basic = grouped_basic.collect()\n\n    freq_lf = (\n        lf_long.group_by([\"Validator Address\", \"confidence_val\"])\n        .agg(pl.count(\"confidence_val\").alias(\"value_count\"))\n        .sort(\n            [\"Validator Address\", \"value_count\"],\n            descending=[False, True],\n        )\n    )\n    df_freq = freq_lf.collect()\n\n    def top_k_values_string(rows: list, k: int):\n        \"\"\"\n        Return a string with format: val(count), val2(count2), ...\n        e.g. '0(14), 35(5), 100(3)'\n        \"\"\"\n        parts = []\n        for r in rows[:k]:\n            cval = r[\"confidence_val\"]\n            count_ = r[\"value_count\"]\n            if cval is None:\n                val_str = \"null\"\n            else:\n                val_str = str(int(cval))\n            parts.append(f\"{val_str}({count_})\")\n        return \", \".join(parts)\n\n    def top_k_values_list(rows: list, k: int):\n        \"\"\"\n        Return just the numeric confidence values in the top k, ignoring their counts.\n        \"\"\"\n        out = []\n        for r in rows[:k]:\n            cval = r[\"confidence_val\"]\n            if cval is not None:\n                out.append(int(cval))\n        return out\n\n    validator_map = {}\n    freq_dicts = df_freq.to_dicts()\n\n    for key, group in groupby(freq_dicts, key=lambda d: d[\"Validator Address\"]):\n        group_list = list(group)\n        group_list_sorted = sorted(\n            group_list, key=lambda x: x[\"value_count\"], reverse=True\n        )\n        freq_str = top_k_values_string(group_list_sorted, top_k_freq)\n        freq_list = top_k_values_list(group_list_sorted, top_k_freq)\n        validator_map[key] = {\"freq_str\": freq_str, \"freq_list\": freq_list}\n\n    df_freq_map = pl.DataFrame(\n        {\n            \"Validator Address\": list(validator_map.keys()),\n            \"top_freq_values_str\": [v[\"freq_str\"] for v in validator_map.values()],\n            \"top_freq_values_list\": [v[\"freq_list\"] for v in validator_map.values()],\n        }\n    )\n\n    df_merged = df_basic.join(\n        df_freq_map, on=[\"Validator Address\"], how=\"left\"\n    )\n\n    df_merged = df_merged.with_columns(\n        [\n            (pl.col(\"max_conf\") == pl.col(\"min_conf\")).alias(\"zero_variation\"),\n            (pl.col(\"distinct_values_count\") == 1).alias(\"only_one_value\"),\n            pl.col(\"top_freq_values_list\").list.contains(0).alias(\"has_zero_conf\"),\n        ]\n    )\n\n    return df_merged.sort([\"Validator Address\"])\n\n\ndef check_confidence_vs_price_correlation(\n    df: pl.DataFrame, fx_pairs: List[str], autonity_pairs: List[str]\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Measures how well Confidence tracks price changes (absolute difference from previous submission).\n    Note: This still checks each 'Price' column vs. its corresponding 'Confidence' column.\n    \"\"\"\n    df_local = df.clone()\n\n    price_cols = fx_pairs + autonity_pairs\n\n    new_cols = []\n    for pc in price_cols:\n        decimal_col = pc.replace(\" Price\", \" Price Decimal\")\n        new_cols.append((pl.col(pc).cast(pl.Float64) / 1e18).alias(decimal_col))\n    df_local = df_local.with_columns(new_cols)\n\n    results_rows = []\n\n    def base_name(price_col: str) -&gt; str:\n        return price_col.replace(\" Price\", \"\")\n\n    for pc in price_cols:\n        conf_col = pc.replace(\"Price\", \"Confidence\").strip()\n        dec_col = pc.replace(\" Price\", \" Price Decimal\")\n        pair_lbl = base_name(pc)\n\n        if conf_col not in df_local.columns or dec_col not in df_local.columns:\n            continue\n\n        df_pair = (\n            df_local\n            .select([\"Validator Address\", \"Timestamp_dt\", dec_col, conf_col])\n            .filter(pl.col(dec_col).is_not_null() & pl.col(conf_col).is_not_null())\n            .sort([\"Validator Address\", \"Timestamp_dt\"])\n        )\n\n        df_pair = df_pair.with_columns(\n            (pl.col(dec_col) - pl.col(dec_col).shift(1))\n            .over(\"Validator Address\")\n            .abs()\n            .alias(\"abs_price_change\")\n        )\n\n        lf_cor = (\n            df_pair.lazy()\n            .group_by(\"Validator Address\")\n            .agg(\n                [\n                    pl.col(\"abs_price_change\").alias(\"price_change_list\"),\n                    pl.col(conf_col).alias(\"confidence_list\"),\n                ]\n            )\n        )\n        local_rows = lf_cor.collect().to_dicts()\n\n        for row in local_rows:\n            validator = row[\"Validator Address\"]\n            pc_list = row[\"price_change_list\"]\n            conf_list = row[\"confidence_list\"]\n\n            if len(pc_list) &lt; 2:\n                corr_val = None\n            else:\n                corr_val = pearson_correlation(pc_list, conf_list)\n            results_rows.append(\n                {\n                    \"Validator Address\": validator,\n                    \"pair_label\": pair_lbl,\n                    \"corr_conf_price_change\": corr_val,\n                    \"num_points\": len(pc_list),\n                }\n            )\n\n    df_corr = pl.DataFrame(results_rows)\n    return df_corr.sort([\"pair_label\", \"Validator Address\"])\n\n\ndef pearson_correlation(xs, ys):\n    \"\"\"\n    Computes Pearson correlation between two lists of floats.\n    \"\"\"\n    clean_data = [\n        (x, y)\n        for (x, y) in zip(xs, ys)\n        if (x is not None and y is not None and not math.isnan(x) and not math.isnan(y))\n    ]\n    if len(clean_data) &lt; 2:\n        return None\n\n    xs_clean, ys_clean = zip(*clean_data)\n    mean_x = statistics.mean(xs_clean)\n    mean_y = statistics.mean(ys_clean)\n    num = sum((x - mean_x) * (y - mean_y) for x, y in zip(xs_clean, ys_clean))\n    den_x = math.sqrt(sum((x - mean_x) ** 2 for x in xs_clean))\n    den_y = math.sqrt(sum((y - mean_y) ** 2 for y in ys_clean))\n    if den_x == 0 or den_y == 0:\n        return None\n    return num / (den_x * den_y)\n\n\ndef analyze_confidence_value_anomalies(\n    submission_glob: str, fx_pairs: List[str], autonity_pairs: List[str]\n):\n    \"\"\"\n    Main analysis function.\n    \"\"\"\n    df_all = load_and_preprocess_submissions(submission_glob)\n\n    conf_cols = list_confidence_columns(df_all)\n\n    df_conf_dist = confidence_distribution_by_validator(\n        df_all, conf_cols, top_k_freq=3\n    )\n\n    df_anomalies = df_conf_dist.filter(\n        pl.col(\"zero_variation\") | pl.col(\"only_one_value\")\n    )\n\n    df_corr = check_confidence_vs_price_correlation(df_all, fx_pairs, autonity_pairs)\n\n    return {\n        \"df_confidence_distribution\": df_conf_dist,\n        \"df_confidence_anomalies\": df_anomalies,\n        \"df_correlation_price_change\": df_corr,\n    }\n\n\nfx_price_cols = [\n    \"AUD-USD Price\",\n    \"CAD-USD Price\",\n    \"EUR-USD Price\",\n    \"GBP-USD Price\",\n    \"JPY-USD Price\",\n    \"SEK-USD Price\",\n]\nautonity_price_cols = [\n    \"ATN-USD Price\",\n    \"NTN-USD Price\",\n    \"NTN-ATN Price\",\n]\n\nresults = analyze_confidence_value_anomalies(\n    submission_glob=\"../submission-data/Oracle_Submission_*.csv\",\n    fx_pairs=fx_price_cols,\n    autonity_pairs=autonity_price_cols,\n)\n\n\n\n\n\nBelow directly reference results generated by the analysis.\nConfidence Value Distribution\n\nresults[\"df_confidence_distribution\"]\n\n\nshape: (60, 12)\n\n\n\nValidator Address\ncount_rows\nmin_conf\nmax_conf\nmean_conf\nstd_conf\ndistinct_values_count\ntop_freq_values_str\ntop_freq_values_list\nzero_variation\nonly_one_value\nhas_zero_conf\n\n\nstr\nu32\ni64\ni64\nf64\nf64\nu32\nstr\nlist[i64]\nbool\nbool\nbool\n\n\n\n\n\"0x00a96aaED75015Bb44cED878D927…\n25785\n90\n100\n99.902269\n0.983768\n3\n\"100(25533), 90(252), null(0)\"\n[100, 90]\nfalse\nfalse\nfalse\n\n\n\"0x01F788E4371a70D579C178Ea7F48…\n25146\n90\n100\n99.904557\n0.972293\n3\n\"100(24906), 90(240), null(0)\"\n[100, 90]\nfalse\nfalse\nfalse\n\n\n\"0x100E38f7BCEc53937BDd79ADE46F…\n0\nnull\nnull\nnull\nnull\n1\n\"null(0)\"\n[]\nnull\ntrue\nfalse\n\n\n\"0x1476A65D7B5739dE1805d5130441…\n22149\n100\n100\n100.0\n0.0\n2\n\"100(22149), null(0)\"\n[100]\ntrue\nfalse\nfalse\n\n\n\"0x197B2c44b887c4aC01243BDE7E4b…\n25920\n90\n100\n99.902778\n0.981227\n2\n\"100(25668), 90(252)\"\n[100, 90]\nfalse\nfalse\nfalse\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"0xd625d50B0d087861c286d726eC51…\n0\nnull\nnull\nnull\nnull\n1\n\"null(0)\"\n[]\nnull\ntrue\nfalse\n\n\n\"0xdF239e0D5b4E6e820B0cFEF6972A…\n0\nnull\nnull\nnull\nnull\n1\n\"null(0)\"\n[]\nnull\ntrue\nfalse\n\n\n\"0xf10f56Bf0A28E0737c7e6bB0aF92…\n25920\n90\n100\n99.902778\n0.981227\n2\n\"100(25668), 90(252)\"\n[100, 90]\nfalse\nfalse\nfalse\n\n\n\"0xf34CD6c09a59d7D3d1a6C3dC231a…\n25092\n100\n100\n100.0\n0.0\n2\n\"100(25092), null(0)\"\n[100]\ntrue\nfalse\nfalse\n\n\n\"0xfD97FB8835d25740A2Da27c69762…\n25920\n90\n100\n99.902778\n0.981227\n2\n\"100(25668), 90(252)\"\n[100, 90]\nfalse\nfalse\nfalse\n\n\n\n\n\n\n\nKey indicators:\n\nMean/std: Low or zero standard deviation indicates fixed or rarely-changing confidence.\nDistinct values count: Few distinct values may indicate hard-coded or rarely adjusted confidence.\n\n\nIdentified Anomalies (Zero or Single-Value Confidence)\n\nresults[\"df_confidence_anomalies\"]\n\n\nshape: (11, 12)\n\n\n\nValidator Address\ncount_rows\nmin_conf\nmax_conf\nmean_conf\nstd_conf\ndistinct_values_count\ntop_freq_values_str\ntop_freq_values_list\nzero_variation\nonly_one_value\nhas_zero_conf\n\n\nstr\nu32\ni64\ni64\nf64\nf64\nu32\nstr\nlist[i64]\nbool\nbool\nbool\n\n\n\n\n\"0x100E38f7BCEc53937BDd79ADE46F…\n0\nnull\nnull\nnull\nnull\n1\n\"null(0)\"\n[]\nnull\ntrue\nfalse\n\n\n\"0x1476A65D7B5739dE1805d5130441…\n22149\n100\n100\n100.0\n0.0\n2\n\"100(22149), null(0)\"\n[100]\ntrue\nfalse\nfalse\n\n\n\"0x26E2724dBD14Fbd52be430B97043…\n0\nnull\nnull\nnull\nnull\n1\n\"null(0)\"\n[]\nnull\ntrue\nfalse\n\n\n\"0x383A3c437d3F12f60E5fC9901194…\n25920\n100\n100\n100.0\n0.0\n1\n\"100(25920)\"\n[100]\ntrue\ntrue\nfalse\n\n\n\"0x3fe573552E14a0FC11Da25E43Fef…\n0\nnull\nnull\nnull\nnull\n1\n\"null(0)\"\n[]\nnull\ntrue\nfalse\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"0xDF2D0052ea56A860443039619f6D…\n25902\n100\n100\n100.0\n0.0\n2\n\"100(25902), null(0)\"\n[100]\ntrue\nfalse\nfalse\n\n\n\"0xcdEed21b471b0Dc54faF74480A0E…\n25920\n100\n100\n100.0\n0.0\n1\n\"100(25920)\"\n[100]\ntrue\ntrue\nfalse\n\n\n\"0xd625d50B0d087861c286d726eC51…\n0\nnull\nnull\nnull\nnull\n1\n\"null(0)\"\n[]\nnull\ntrue\nfalse\n\n\n\"0xdF239e0D5b4E6e820B0cFEF6972A…\n0\nnull\nnull\nnull\nnull\n1\n\"null(0)\"\n[]\nnull\ntrue\nfalse\n\n\n\"0xf34CD6c09a59d7D3d1a6C3dC231a…\n25092\n100\n100\n100.0\n0.0\n2\n\"100(25092), null(0)\"\n[100]\ntrue\nfalse\nfalse\n\n\n\n\n\n\n\nRows indicate validators consistently providing identical confidence, suggesting potential misconfiguration or logic errors.\n\nCorrelation Between Confidence and Price Changes\n\nresults[\"df_correlation_price_change\"].filter(pl.col(\"num_points\") &gt; 2)\n\n\nshape: (495, 4)\n\n\n\nValidator Address\npair_label\ncorr_conf_price_change\nnum_points\n\n\nstr\nstr\nf64\ni64\n\n\n\n\n\"0x00a96aaED75015Bb44cED878D927…\n\"ATN-USD\"\nnull\n2865\n\n\n\"0x01F788E4371a70D579C178Ea7F48…\n\"ATN-USD\"\nnull\n2794\n\n\n\"0x1476A65D7B5739dE1805d5130441…\n\"ATN-USD\"\nnull\n2461\n\n\n\"0x197B2c44b887c4aC01243BDE7E4b…\n\"ATN-USD\"\nnull\n2880\n\n\n\"0x19E356ebC20283fc74AF0BA4C179…\n\"ATN-USD\"\nnull\n2786\n\n\n…\n…\n…\n…\n\n\n\"0xcf716b3930d7cf6f2ADAD90A27c3…\n\"SEK-USD\"\n0.006676\n2880\n\n\n\"0xd61a48b0e11B0Dc6b7Bd713B1012…\n\"SEK-USD\"\nnull\n2866\n\n\n\"0xf10f56Bf0A28E0737c7e6bB0aF92…\n\"SEK-USD\"\n0.00658\n2880\n\n\n\"0xf34CD6c09a59d7D3d1a6C3dC231a…\n\"SEK-USD\"\nnull\n2788\n\n\n\"0xfD97FB8835d25740A2Da27c69762…\n\"SEK-USD\"\n-0.112979\n2880\n\n\n\n\n\n\nNote: You may observe many null values in the corr_conf_price_change column. This can happen if a validator always provides the same confidence (zero variance in confidence values) or the price change for the given pair is often 0 or extremely small (zero variance in price).\n\nCorrelation (corr_conf_price_change) near zero implies confidence metrics not aligned with real market volatility. Strong correlation (positive or negative) suggests meaningful responsiveness.\n\nInterpretation of Results\n\ndf_corr_fixed = results[\"df_correlation_price_change\"].with_columns(\n    pl.col(\"corr_conf_price_change\").cast(pl.Float64)\n)\n\nnum_anomalies = results[\"df_confidence_anomalies\"].height\nprint(f\"Validators with fixed confidence values: {num_anomalies}\")\n\nlow_corr_count = df_corr_fixed.filter(\n    (pl.col(\"corr_conf_price_change\").abs() &lt; 0.1)\n    & (pl.col(\"corr_conf_price_change\").is_not_null())\n).height\n\nprint(f\"Number of validator-currency pair combinations with low correlation (&lt;0.1): {low_corr_count}\")\n\nif num_anomalies &gt; 0:\n    print(\"Identified validators with potentially hard-coded or fixed confidence values.\")\nelse:\n    print(\"No significant anomalies in confidence values identified.\")\n\nif low_corr_count &gt; 0:\n    print(\"Confidence values for many validators do not adequately reflect market volatility.\")\nelse:\n    print(\"Confidence values generally align well with price changes.\")\n\nValidators with fixed confidence values: 11\nNumber of validator-currency pair combinations with low correlation (&lt;0.1): 143\nIdentified validators with potentially hard-coded or fixed confidence values.\nConfidence values for many validators do not adequately reflect market volatility.\n\n\nList of all Validators and their Standard Deviations\n\ndef compute_variation_metrics(df: pl.DataFrame, confidence_cols: List[str]) -&gt; pl.DataFrame:\n    \"\"\"\n    For each validator, compute:\n      - min_conf, max_conf, mean_conf, std_conf\n      - distinct_values_count, fraction_zero\n    \"\"\"\n    # Keep relevant columns\n    keep_cols = [\"Validator Address\", \"Timestamp_dt\"] + confidence_cols\n    df_small = df.select([c for c in keep_cols if c in df.columns])\n\n    # Reshape into long form\n    df_long = df_small.melt(\n        id_vars=[\"Validator Address\", \"Timestamp_dt\"],\n        value_vars=confidence_cols,\n        variable_name=\"confidence_col\",\n        value_name=\"confidence_val\",\n    )\n\n    # Group by validator only\n    metrics_lf = (\n        df_long.lazy()\n        .group_by([\"Validator Address\"])\n        .agg([\n            pl.min(\"confidence_val\").alias(\"min_conf\"),\n            pl.max(\"confidence_val\").alias(\"max_conf\"),\n            pl.mean(\"confidence_val\").alias(\"mean_conf\"),\n            pl.std(\"confidence_val\").alias(\"std_conf\"),\n            pl.n_unique(\"confidence_val\").alias(\"distinct_values_count\"),\n            (pl.col(\"confidence_val\") == 0).sum().alias(\"count_zero\"),\n            pl.count(\"confidence_val\").alias(\"count_total\"),\n        ])\n        .with_columns([\n            (pl.col(\"count_zero\") / pl.col(\"count_total\")).alias(\"fraction_zero\")\n        ])\n    )\n\n    return metrics_lf.collect().sort([\"Validator Address\"])\n\n\ndf_all_variation = load_and_preprocess_submissions(\"../submission-data/Oracle_Submission_*.csv\")\nall_conf_cols = list_confidence_columns(df_all_variation)\ndf_variation_metrics = compute_variation_metrics(df_all_variation, all_conf_cols)\ndf_variation_metrics = df_variation_metrics.sort(\"std_conf\", descending=False)\n\nfor row in df_variation_metrics.to_dicts():\n    mean_conf = row['mean_conf']\n    if mean_conf is None:\n        mean_conf = \"0\"\n    else:\n        mean_conf = round(mean_conf, 1)\n    std_conf = row['std_conf']\n    if std_conf is None:\n        std_conf = \"0\"\n    else:\n        std_conf = round(std_conf, 1)\n    print(\n        f\"Validator {row['Validator Address']}: \"\n        f\"min_conf={row['min_conf']}, \"\n        f\"max_conf={row['max_conf']}, \"\n        f\"mean_conf={mean_conf}, \"\n        f\"std_conf={std_conf}, \"\n    )\n\nValidator 0x100E38f7BCEc53937BDd79ADE46F34362470577B: min_conf=None, max_conf=None, mean_conf=0, std_conf=0, \nValidator 0x26E2724dBD14Fbd52be430B97043AA4c83F05852: min_conf=None, max_conf=None, mean_conf=0, std_conf=0, \nValidator 0x3fe573552E14a0FC11Da25E43Fef11e16a785068: min_conf=None, max_conf=None, mean_conf=0, std_conf=0, \nValidator 0xd625d50B0d087861c286d726eC51Cf4Bd9c54357: min_conf=None, max_conf=None, mean_conf=0, std_conf=0, \nValidator 0xdF239e0D5b4E6e820B0cFEF6972A90893c2073AB: min_conf=None, max_conf=None, mean_conf=0, std_conf=0, \nValidator 0x1476A65D7B5739dE1805d5130441A94022Ee49fe: min_conf=100, max_conf=100, mean_conf=100.0, std_conf=0.0, \nValidator 0x383A3c437d3F12f60E5fC990119468D3561EfBfc: min_conf=100, max_conf=100, mean_conf=100.0, std_conf=0.0, \nValidator 0x6747c02DE7eb2099265e55715Ba2E03e8563D051: min_conf=100, max_conf=100, mean_conf=100.0, std_conf=0.0, \nValidator 0xDF2D0052ea56A860443039619f6DAe4434bc0Ac4: min_conf=100, max_conf=100, mean_conf=100.0, std_conf=0.0, \nValidator 0xcdEed21b471b0Dc54faF74480A0E700fCc42a7b6: min_conf=100, max_conf=100, mean_conf=100.0, std_conf=0.0, \nValidator 0xf34CD6c09a59d7D3d1a6C3dC231a7834E5615D6A: min_conf=100, max_conf=100, mean_conf=100.0, std_conf=0.0, \nValidator 0x01F788E4371a70D579C178Ea7F48E04e8B2CD743: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x19E356ebC20283fc74AF0BA4C179502A1F62fA7B: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0xbfDcAF35f52F9ef423ac8F2621F9eef8be6dEd17: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x2928FE5b911BCAf837cAd93eB9626E86a189f1dd: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x4cD134001EEF0843B9c69Ba9569d11fDcF4bd495: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x197B2c44b887c4aC01243BDE7E4bBa8bd95BC3a8: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x23b4Be9536F93b8D550214912fD0e38417Ff7209: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x24915749B793375a8C93090AF19928aFF1CAEcb6: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x358488a4EdCA493FCD87610dcd50c62c8A3Dd658: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x5603caFE3313D0cf56Fd4bE4A2f606dD6E43F8Eb: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x59031767f20EA8F4a3d90d33aB0DAA2ca469Fd9a: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x64F83c2538A646A550Ad9bEEb63427a377359DEE: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x791A7F840ac11841cCB0FaA968B2e3a0Db930fCe: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x99E2B4B27BDe92b42D04B6CF302cF564D2C13b74: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x9C7dAABb5101623340C925CFD6fF74088ff5672e: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0xC1F9acAF1824F6C906b35A0D2584D6E25077C7f5: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0xD9fDab408dF7Ae751691BeC2efE3b713ba3f9C36: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0xcf716b3930d7cf6f2ADAD90A27c39fDc9D643BBd: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0xf10f56Bf0A28E0737c7e6bB0aF92f3DDad34aE6a: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0xfD97FB8835d25740A2Da27c69762D74F6A931858: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x8584A78A9b94f332A34BBf24D2AF83367Da31894: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x984A46Ec685Bb41A7BBb2bc39f80C78410ff4057: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x00a96aaED75015Bb44cED878D927dcb15ec1FF54: min_conf=90, max_conf=100, mean_conf=99.9, std_conf=1.0, \nValidator 0x36142A4f36974e2935192A1111C39330aA296D3C: min_conf=90, max_conf=100, mean_conf=93.3, std_conf=4.7, \nValidator 0x718361fc3637199F24a2437331677D6B89a40519: min_conf=90, max_conf=100, mean_conf=93.3, std_conf=4.7, \nValidator 0x94d28f08Ff81A80f4716C0a8EfC6CAC2Ec74d09E: min_conf=90, max_conf=100, mean_conf=93.3, std_conf=4.7, \nValidator 0xE9FFF86CAdC3136b3D94948B8Fd23631EDaa2dE3: min_conf=90, max_conf=100, mean_conf=93.3, std_conf=4.7, \nValidator 0xc5B9d978715F081E226cb28bADB7Ba4cde5f9775: min_conf=90, max_conf=100, mean_conf=93.3, std_conf=4.7, \nValidator 0x6a395dE946c0493157404E2b1947493c633f569E: min_conf=90, max_conf=100, mean_conf=93.3, std_conf=4.7, \nValidator 0xF9B38D02959379d43C764064dE201324d5e12931: min_conf=70, max_conf=100, mean_conf=93.1, std_conf=5.2, \nValidator 0xBE287C82A786218E008FF97320b08244BE4A282c: min_conf=70, max_conf=100, mean_conf=93.1, std_conf=5.2, \nValidator 0x7232e75a8bFd8c9ab002BB3A00eAa885BC72A6dd: min_conf=70, max_conf=100, mean_conf=93.1, std_conf=5.2, \nValidator 0x3AaF7817618728ffEF81898E11A3171C33faAE41: min_conf=70, max_conf=100, mean_conf=93.1, std_conf=5.2, \nValidator 0x5E17e837DcBa2728C94f95c38fA8a47CB9C8818F: min_conf=70, max_conf=100, mean_conf=93.1, std_conf=5.2, \nValidator 0x1Be7f70BCf8393a7e4A5BcC66F6f15d6e35cfBBC: min_conf=50, max_conf=100, mean_conf=66.7, std_conf=23.6, \nValidator 0x22A76e194A49c9e5508Cd4A3E1cD555D088ECB08: min_conf=50, max_conf=100, mean_conf=66.7, std_conf=23.6, \nValidator 0x3597d2D42f8Fbbc82E8b1046048773aD6DDB717E: min_conf=50, max_conf=100, mean_conf=66.7, std_conf=23.6, \nValidator 0x527192F3D2408C84087607b7feE1d0f907821E17: min_conf=50, max_conf=100, mean_conf=66.7, std_conf=23.6, \nValidator 0x551f3300FCFE0e392178b3542c009948008B2a9F: min_conf=50, max_conf=100, mean_conf=66.7, std_conf=23.6, \nValidator 0x94470A842Ea4f44e668EB9C2AB81367b6Ce01772: min_conf=50, max_conf=100, mean_conf=66.7, std_conf=23.6, \nValidator 0x9d28e40E9Ec4789f9A0D17e421F76D8D0868EA44: min_conf=50, max_conf=100, mean_conf=66.7, std_conf=23.6, \nValidator 0xB5d8be2AB4b6d7E6be7Ea28E91b370223a06289f: min_conf=50, max_conf=100, mean_conf=66.7, std_conf=23.6, \nValidator 0xBBf36374eb23968F25aecAEbb97BF3118f3c2fEC: min_conf=50, max_conf=100, mean_conf=66.7, std_conf=23.6, \nValidator 0xDCA5DFF3D42f2db3C18dBE823380A0A81db49A7E: min_conf=50, max_conf=100, mean_conf=66.7, std_conf=23.6, \nValidator 0xE4686A4C6E63A8ab51B458c52EB779AEcf0B74f7: min_conf=50, max_conf=100, mean_conf=66.7, std_conf=23.6, \nValidator 0x8f91e0ADF8065C3fFF92297267E02DF32C2978FF: min_conf=50, max_conf=100, mean_conf=66.7, std_conf=23.6, \nValidator 0xEf0Ba5e345C2C3937df5667A870Aae5105CAa3a5: min_conf=50, max_conf=100, mean_conf=66.7, std_conf=23.6, \nValidator 0xd61a48b0e11B0Dc6b7Bd713B1012563c52591BAA: min_conf=50, max_conf=100, mean_conf=66.7, std_conf=23.6, \nValidator 0x831B837C3DA1B6c2AB68a690206bDfF368877E19: min_conf=50, max_conf=100, mean_conf=66.7, std_conf=23.6, \n\n\nPlease note, min_conf, max_conf, mean_conf and std_conf are the minimum, maximum, mean and standard deviation of the confidence values this validator provided across all submissions.",
    "crumbs": [
      "Notebooks",
      "Issue 5"
    ]
  },
  {
    "objectID": "notebooks/issue_3.html",
    "href": "notebooks/issue_3.html",
    "title": "Issue 3",
    "section": "",
    "text": "This notebook documents the analysis for Issue #3: Out-of-Range / Suspicious Values in the Autonity Oracle data. It covers:\n\nWhat is this issue about?\n\nWhy conduct this issue analysis?\n\nHow to conduct this issue analysis?\n\nWhat are the results?\n\n\n\n\nCertain Oracle submissions are unexpectedly large, zero, negative, or significantly off-market compared to real FX data. Examples include:\n\nExtremely large prices like 6.3e+25 indicating scaling errors.\nNegative or zero prices, which should not occur.\nLarge spikes or sudden changes inconsistent with actual market data.\n\nAdditionally, cross-rates for Autonity tokens (ATN, NTN) may be inconsistent (NTN-USD ≠ NTN-ATN × ATN-USD).\n\n\n\n\n\nData Integrity: Ensuring accuracy and reliability of the Oracle data.\nSecurity & Reliability: Identifying potential bugs or malicious activities before Mainnet.\nCross-rate Consistency: Confirming internal consistency for token prices.\n\n\n\n\n\nUse Python with the Polars library (v1.24.0) to:\n\nLoad and preprocess Oracle submission CSV files.\nConverted price submissions from Wei to decimals (dividing by 1e18).\nCompared FX pairs to minute-level Yahoo Finance benchmarks to detect:\n\nDeviations exceeding ±20%.\nNegative, zero, or excessively large prices.\n\nChecked Autonity token cross-rates for consistency within a 10% tolerance.\n\nBelow is the analysis script:\n\nimport polars as pl\nimport glob\nimport math\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndef load_and_preprocess_oracle_submissions(submission_glob: str) -&gt; pl.LazyFrame:\n    \"\"\"\n    Loads Oracle Submission CSVs and returns a Polars DataFrame.\n    \"\"\"\n    files = sorted(glob.glob(submission_glob))\n    if not files:\n        raise ValueError(f\"No CSV files found matching pattern {submission_glob}\")\n\n    lf_list = []\n    for f in files:\n        lf_temp = pl.scan_csv(\n            f,\n            dtypes={\"Timestamp\": pl.Utf8},\n            null_values=[\"\"],\n            ignore_errors=True,\n        )\n        lf_list.append(lf_temp)\n\n    lf = pl.concat(lf_list)\n    lf = lf.with_columns(\n        pl.col(\"Timestamp\")\n        .str.strptime(pl.Datetime, strict=False)\n        .alias(\"Timestamp_dt\")\n    )\n\n    lf = lf.with_columns(\n        [\n            pl.col(\"Timestamp_dt\").cast(pl.Date).alias(\"date_only\"),\n            pl.col(\"Timestamp_dt\").dt.weekday().alias(\"weekday_num\"),\n        ]\n    )\n    return lf\n\n\ndef load_yahoo_finance_data(directory_pattern: str, pair_label: str) -&gt; pl.DataFrame:\n    \"\"\"\n    Loads Yahoo Finance CSVs and returns a Polars DataFrame.\n    \"\"\"\n    files = sorted(glob.glob(directory_pattern))\n    if not files:\n        raise ValueError(f\"No Yahoo Finance CSV files found: {directory_pattern}\")\n\n    lf_list = []\n    for f in files:\n        lf_temp = pl.scan_csv(\n            f,\n            has_header=False,\n            skip_rows=3,\n            new_columns=[\"Datetime\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\"],\n            try_parse_dates=True,\n        )\n        lf_list.append(lf_temp)\n\n    lf = pl.concat(lf_list)\n    df = (\n        lf.select(\n            [\n                pl.col(\"Datetime\").alias(\"timestamp_benchmark\"),\n                pl.col(\"Close\").alias(\"benchmark_close\"),\n            ]\n        )\n        .sort(\"timestamp_benchmark\")\n        .collect()\n        .with_columns(\n            [\n                pl.lit(pair_label).alias(\"symbol\"),\n            ]\n        )\n    )\n    return df\n\n\ndef load_all_fx_benchmarks() -&gt; dict[str, pl.DataFrame]:\n    \"\"\"\n    Loads FX data from Yahoo Finance.\n    \"\"\"\n    mapping = {\n        \"AUD-USD\": \"../yahoo-finance/data/AUDUSD/AUDUSD=X_1m_*.csv\",\n        \"CAD-USD\": \"../yahoo-finance/data/CADUSD/CADUSD=X_1m_*.csv\",\n        \"EUR-USD\": \"../yahoo-finance/data/EURUSD/EURUSD=X_1m_*.csv\",\n        \"GBP-USD\": \"../yahoo-finance/data/GBPUSD/GBPUSD=X_1m_*.csv\",\n        \"JPY-USD\": \"../yahoo-finance/data/JPYUSD/JPYUSD=X_1m_*.csv\",\n        \"SEK-USD\": \"../yahoo-finance/data/SEKUSD/SEKUSD=X_1m_*.csv\",\n    }\n\n    result = {}\n    for pair_label, pattern in mapping.items():\n        df_pair = load_yahoo_finance_data(pattern, pair_label)\n        result[pair_label] = df_pair\n    return result\n\n\ndef convert_wei_to_decimal(price_wei: float) -&gt; float:\n    \"\"\"\n    Convert from Wei-based representation to a decimal.\n    \"\"\"\n    if price_wei is None or math.isnan(price_wei):\n        return None\n    return price_wei / 1e18\n\n\ndef flag_suspicious_values(\n    df_submissions: pl.DataFrame,\n    fx_pairs: list[str],\n    autonity_pairs: list[str],\n    fx_benchmarks: dict[str, pl.DataFrame],\n    percent_threshold: float = 0.20,\n    join_tolerance: str = \"30s\",\n    dynamic_thresholds: dict[str, float] | None = None,\n):\n    \"\"\"\n    Detect suspicious or out-of-range values in Oracle data using as-of joins for time alignment\n    and dynamic thresholds for 'excessively large' values.\n    \"\"\"\n    new_cols = []\n    for c in fx_pairs + autonity_pairs:\n        if c.endswith(\" Price\"):\n            dec_col = c.replace(\" Price\", \" Price Decimal\")\n            new_cols.append((pl.col(c).cast(pl.Float64) / 1e18).alias(dec_col))\n\n    df_submissions = df_submissions.with_columns(new_cols)\n\n    suspicious_frames: list[pl.DataFrame] = []\n\n    final_columns = [\n        \"Timestamp_dt\",\n        \"Validator Address\",\n        \"oracle_price_decimal\",\n        \"benchmark_close\",\n        \"rel_diff_from_bench\",\n        \"ATN-USD Price Decimal\",\n        \"NTN-USD Price Decimal\",\n        \"NTN-ATN Price Decimal\",\n        \"ntn_usd_estimated\",\n        \"rel_diff_cross\",\n        \"suspect_reason\",\n    ]\n\n    for pair_label in fx_pairs:\n        if not pair_label.endswith(\" Price\"):\n            continue\n\n        base_name = pair_label.replace(\" Price\", \"\")  # e.g. \"AUD-USD\"\n        decimal_col = f\"{base_name} Price Decimal\"\n\n        if base_name not in fx_benchmarks:\n            continue\n\n        df_bench = fx_benchmarks[base_name]\n\n        df_local = (\n            df_submissions\n            .select([\"Timestamp_dt\", \"Validator Address\", decimal_col])\n            .sort(\"Timestamp_dt\")\n        )\n\n        df_bench_sorted = df_bench.sort(\"timestamp_benchmark\")\n\n        df_joined = df_local.join_asof(\n            df_bench_sorted,\n            left_on=\"Timestamp_dt\",\n            right_on=\"timestamp_benchmark\",\n            strategy=\"nearest\",    # or \"backward\"/\"forward\"\n            tolerance=join_tolerance\n        ).with_columns(\n            (\n                (pl.col(decimal_col) - pl.col(\"benchmark_close\")).abs()\n                / pl.col(\"benchmark_close\").abs()\n            )\n            .alias(\"rel_diff_from_bench\")\n        )\n\n        if dynamic_thresholds and base_name in dynamic_thresholds:\n            max_threshold = dynamic_thresholds[base_name]\n        else:\n            max_threshold = 5.0  # fallback\n\n        df_flagged_fx = (\n            df_joined\n            .select(\n                [\n                    \"Timestamp_dt\",\n                    \"Validator Address\",\n                    pl.col(decimal_col).alias(\"oracle_price_decimal\"),\n                    \"benchmark_close\",\n                    \"rel_diff_from_bench\",\n                ]\n            )\n            .with_columns(\n                [\n                    pl.when(\n                        (pl.col(\"oracle_price_decimal\").is_not_null()) &\n                        (pl.col(\"oracle_price_decimal\") &lt;= 0)\n                    )\n                    .then(pl.lit(\"Non-positive price; \"))\n                    .otherwise(pl.lit(\"\"))\n                    .alias(\"cond1\"),\n\n                    pl.when(\n                        (pl.col(\"oracle_price_decimal\").is_not_null()) &\n                        (pl.col(\"oracle_price_decimal\") &gt;= max_threshold)\n                    )\n                    .then(pl.lit(\"Excessively large price; \"))\n                    .otherwise(pl.lit(\"\"))\n                    .alias(\"cond2\"),\n\n                    pl.when(\n                        (pl.col(\"oracle_price_decimal\").is_not_null()) &\n                        (pl.col(\"benchmark_close\").is_not_null()) &\n                        (pl.col(\"rel_diff_from_bench\") &gt; percent_threshold)\n                    )\n                    .then(pl.lit(f\"Deviation &gt; {int(percent_threshold*100)}%; \"))\n                    .otherwise(pl.lit(\"\"))\n                    .alias(\"cond3\"),\n                ]\n            )\n            .with_columns(\n                [\n                    (pl.col(\"cond1\") + pl.col(\"cond2\") + pl.col(\"cond3\")).alias(\"suspect_reason\")\n                ]\n            )\n            .filter(pl.col(\"suspect_reason\") != \"\")\n            .drop([\"cond1\", \"cond2\", \"cond3\"])\n        )\n\n        df_flagged_fx = df_flagged_fx.with_columns(\n            [\n                pl.lit(None).cast(pl.Float64).alias(\"ATN-USD Price Decimal\"),\n                pl.lit(None).cast(pl.Float64).alias(\"NTN-USD Price Decimal\"),\n                pl.lit(None).cast(pl.Float64).alias(\"NTN-ATN Price Decimal\"),\n                pl.lit(None).cast(pl.Float64).alias(\"ntn_usd_estimated\"),\n                pl.lit(None).cast(pl.Float64).alias(\"rel_diff_cross\"),\n            ]\n        )\n\n        df_flagged_fx = df_flagged_fx.select(final_columns)\n        suspicious_frames.append(df_flagged_fx)\n\n    required_cols = {\n        \"ATN-USD Price Decimal\",\n        \"NTN-USD Price Decimal\",\n        \"NTN-ATN Price Decimal\",\n    }\n    if required_cols.issubset(set(df_submissions.columns)):\n        df_autonity = df_submissions.select(\n            [\n                \"Timestamp_dt\",\n                \"Validator Address\",\n                \"ATN-USD Price Decimal\",\n                \"NTN-USD Price Decimal\",\n                \"NTN-ATN Price Decimal\",\n            ]\n        )\n\n        df_autonity = df_autonity.with_columns(\n            (pl.col(\"NTN-ATN Price Decimal\") * pl.col(\"ATN-USD Price Decimal\"))\n            .alias(\"ntn_usd_estimated\")\n        )\n\n        df_autonity = df_autonity.with_columns(\n            (\n                (\n                    (pl.col(\"ntn_usd_estimated\") - pl.col(\"NTN-USD Price Decimal\")).abs()\n                    / (pl.col(\"NTN-USD Price Decimal\").abs() + 1e-18)\n                ).alias(\"rel_diff_cross\")\n            )\n        )\n\n        cross_tolerance = 0.10  # 10%\n        df_autonity_suspect = (\n            df_autonity\n            .with_columns(\n                pl.when(pl.col(\"rel_diff_cross\") &gt; cross_tolerance)\n                .then(pl.lit(\"Cross-rate mismatch &gt; 10%; \"))\n                .otherwise(pl.lit(\"\"))\n                .alias(\"suspect_reason\")\n            )\n            .filter(pl.col(\"suspect_reason\") != \"\")\n        )\n\n        df_autonity_suspect = df_autonity_suspect.with_columns(\n            [\n                pl.lit(None).cast(pl.Float64).alias(\"oracle_price_decimal\"),\n                pl.lit(None).cast(pl.Float64).alias(\"benchmark_close\"),\n                pl.lit(None).cast(pl.Float64).alias(\"rel_diff_from_bench\"),\n            ]\n        )\n\n        df_autonity_suspect = df_autonity_suspect.select(final_columns)\n        suspicious_frames.append(df_autonity_suspect)\n\n    if suspicious_frames:\n        df_suspicious = pl.concat(suspicious_frames, how=\"vertical\")\n    else:\n        df_suspicious = pl.DataFrame(\n            {\"Timestamp_dt\": [], \"Validator Address\": [], \"suspect_reason\": []}\n        )\n\n    return df_suspicious\n\n\ndef analyze_out_of_range_values(\n    submission_glob: str,\n    fx_pairs: list[str],\n    autonity_pairs: list[str],\n    yahoo_data_dict: dict[str, pl.DataFrame],\n    deviation_threshold: float = 0.20,\n    join_tolerance: str = \"30s\",\n    dynamic_thresholds: dict[str, float] | None = None,\n):\n    \"\"\"\n    Main analysis function.\n    \"\"\"\n    lf_sub = load_and_preprocess_oracle_submissions(submission_glob)\n    df_sub = lf_sub.collect()\n\n    df_suspicious = flag_suspicious_values(\n        df_submissions=df_sub,\n        fx_pairs=fx_pairs,\n        autonity_pairs=autonity_pairs,\n        fx_benchmarks=yahoo_data_dict,\n        percent_threshold=deviation_threshold,\n        join_tolerance=join_tolerance,\n        dynamic_thresholds=dynamic_thresholds,\n    )\n\n    if not df_suspicious.is_empty():\n        suspicious_preview = df_suspicious.to_dicts()\n        for row in suspicious_preview:\n            ts_ = row.get(\"Timestamp_dt\")\n            val_addr = row.get(\"Validator Address\")\n            reason = row.get(\"suspect_reason\")\n            price = row.get(\"oracle_price_decimal\")\n            benchmark = row.get(\"benchmark_close\")\n            rel_diff = row.get(\"rel_diff_from_bench\")\n            cross_diff = row.get(\"rel_diff_cross\")  # if from cross-rate\n\n            line_parts = [f\"{ts_} | {val_addr} | {reason}\"]\n            if price is not None:\n                line_parts.append(f\"oracle_price={price:.4f}\")\n            if benchmark is not None:\n                line_parts.append(f\"bench={benchmark:.4f}\")\n            if rel_diff is not None:\n                line_parts.append(f\"diff={rel_diff*100:.2f}%\")\n            if cross_diff is not None:\n                line_parts.append(f\"cross_diff={cross_diff*100:.2f}%\")\n\n    return df_suspicious\n\n\nfx_cols = [\n    \"AUD-USD Price\",\n    \"CAD-USD Price\",\n    \"EUR-USD Price\",\n    \"GBP-USD Price\",\n    \"JPY-USD Price\",\n    \"SEK-USD Price\",\n]\nautonity_cols = [\"ATN-USD Price\", \"NTN-USD Price\", \"NTN-ATN Price\"]\n\nyahoo_data = load_all_fx_benchmarks()\n\ndf_outliers = analyze_out_of_range_values(\n    submission_glob=\"../submission-data/Oracle_Submission_*.csv\",\n    fx_pairs=fx_cols,\n    autonity_pairs=autonity_cols,\n    yahoo_data_dict=yahoo_data,\n    deviation_threshold=0.20,\n    join_tolerance=\"30s\",\n    dynamic_thresholds={\n        \"AUD-USD\": 2.0,\n        \"CAD-USD\": 2.0,\n        \"EUR-USD\": 3.0,\n        \"GBP-USD\": 3.0,\n        \"JPY-USD\": 200.0,\n        \"SEK-USD\": 20.0,\n        \"ATN-USD\": 1.0,\n        \"NTN-USD\": 1.0,\n        \"NTN-ATN\": 1.0,        \n    },\n)\n\n\n\n\n\nThe following results summarize the suspicious submissions detected:\n\nnum_suspicious = df_outliers.height\nprint(f\"Total suspicious submissions detected: {num_suspicious}\")\n\nif num_suspicious == 0:\n    print(\"No suspicious values detected within the ±20% threshold.\")\nelse:\n    display(df_outliers)\n\nTotal suspicious submissions detected: 0\nNo suspicious values detected within the ±20% threshold.\n\n\nNote: You may see many null in the df_outliers table. This is expected behavior when a row is only flagged for a specific category (e.g., Forex mismatch or cross-rate mismatch), and the columns for the other category remain null. If the table is empty, that indicates no outliers were detected.\n\nNegative or zero prices: Indicate significant issues like data feed outages or software errors.\nExtreme values: Likely result from incorrect scaling or data staleness.\nLarge deviations (&gt;20%): Suggest problems with validator data sources or calculation logic.\nCross-rate mismatches (&gt;10%): Highlight misconfigurations or inconsistencies between token price feeds.\n\nValidators frequently flagged with suspicious data require further investigation, particularly if patterns or correlations emerge.\nList of all Validators and their Fraction Suspecious Submissions\n\nlf_sub = load_and_preprocess_oracle_submissions(\"../submission-data/Oracle_Submission_*.csv\")\ndf_all_submissions = lf_sub.collect()\n\ndf_validator_submissions = (\n    df_all_submissions\n    .group_by(\"Validator Address\")\n    .agg([\n        pl.count().alias(\"total_submissions\"),\n    ])\n)\n\ndf_validator_outliers = (\n    df_outliers\n    .group_by(\"Validator Address\")\n    .agg([\n        pl.count().alias(\"suspicious_submissions\"),\n    ])\n)\n\ndf_validator_stats = (\n    df_validator_submissions\n    .join(df_validator_outliers, on=\"Validator Address\", how=\"left\")\n    .with_columns([\n        pl.col(\"suspicious_submissions\").fill_null(0),  # if a validator never appears in df_outliers\n        (\n            pl.col(\"suspicious_submissions\") / pl.col(\"total_submissions\")\n        ).alias(\"suspicious_ratio\")\n    ])\n)\n\ndf_validator_stats = (\n    df_validator_stats\n    .select([\n        \"Validator Address\",\n        \"total_submissions\",\n        \"suspicious_submissions\",\n        (pl.col(\"suspicious_ratio\") * 100).round(2).alias(\"suspicious_ratio_pct\"),\n    ])\n    .sort(\"suspicious_submissions\", descending=True)\n)\n\nfor row in df_validator_stats.to_dicts():\n    fraction_suspicious_submissions = row['suspicious_ratio_pct']\n    if fraction_suspicious_submissions is None:\n        fraction_suspicious_submissions = \"0%\"\n    else:\n        fraction_suspicious_submissions = f\"{fraction_suspicious_submissions}%\"\n    print(\n        f\"Validator {row['Validator Address']}: \"\n        f\"total={row['total_submissions']}, \"\n        f\"suspicious_submissions={row['suspicious_submissions']}, \"\n        f\"fraction_suspicious_submissions={fraction_suspicious_submissions}\"\n    )\n\nValidator 0xE4686A4C6E63A8ab51B458c52EB779AEcf0B74f7: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x01F788E4371a70D579C178Ea7F48E04e8B2CD743: total=2837, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x3597d2D42f8Fbbc82E8b1046048773aD6DDB717E: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xf10f56Bf0A28E0737c7e6bB0aF92f3DDad34aE6a: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x94470A842Ea4f44e668EB9C2AB81367b6Ce01772: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xdF239e0D5b4E6e820B0cFEF6972A90893c2073AB: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xE9FFF86CAdC3136b3D94948B8Fd23631EDaa2dE3: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x791A7F840ac11841cCB0FaA968B2e3a0Db930fCe: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x527192F3D2408C84087607b7feE1d0f907821E17: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x3fe573552E14a0FC11Da25E43Fef11e16a785068: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x9d28e40E9Ec4789f9A0D17e421F76D8D0868EA44: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x984A46Ec685Bb41A7BBb2bc39f80C78410ff4057: total=2877, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x64F83c2538A646A550Ad9bEEb63427a377359DEE: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x22A76e194A49c9e5508Cd4A3E1cD555D088ECB08: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x23b4Be9536F93b8D550214912fD0e38417Ff7209: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x24915749B793375a8C93090AF19928aFF1CAEcb6: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xBE287C82A786218E008FF97320b08244BE4A282c: total=2879, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x94d28f08Ff81A80f4716C0a8EfC6CAC2Ec74d09E: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x00a96aaED75015Bb44cED878D927dcb15ec1FF54: total=2866, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x8584A78A9b94f332A34BBf24D2AF83367Da31894: total=2879, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xd61a48b0e11B0Dc6b7Bd713B1012563c52591BAA: total=2873, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x9C7dAABb5101623340C925CFD6fF74088ff5672e: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xfD97FB8835d25740A2Da27c69762D74F6A931858: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x8f91e0ADF8065C3fFF92297267E02DF32C2978FF: total=2879, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x59031767f20EA8F4a3d90d33aB0DAA2ca469Fd9a: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xD9fDab408dF7Ae751691BeC2efE3b713ba3f9C36: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x383A3c437d3F12f60E5fC990119468D3561EfBfc: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x26E2724dBD14Fbd52be430B97043AA4c83F05852: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xd625d50B0d087861c286d726eC51Cf4Bd9c54357: total=2856, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x1476A65D7B5739dE1805d5130441A94022Ee49fe: total=2462, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x7232e75a8bFd8c9ab002BB3A00eAa885BC72A6dd: total=2877, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xcdEed21b471b0Dc54faF74480A0E700fCc42a7b6: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xBBf36374eb23968F25aecAEbb97BF3118f3c2fEC: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xB5d8be2AB4b6d7E6be7Ea28E91b370223a06289f: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x551f3300FCFE0e392178b3542c009948008B2a9F: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x358488a4EdCA493FCD87610dcd50c62c8A3Dd658: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xEf0Ba5e345C2C3937df5667A870Aae5105CAa3a5: total=2879, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x4cD134001EEF0843B9c69Ba9569d11fDcF4bd495: total=2823, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x5603caFE3313D0cf56Fd4bE4A2f606dD6E43F8Eb: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xDF2D0052ea56A860443039619f6DAe4434bc0Ac4: total=2879, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xC1F9acAF1824F6C906b35A0D2584D6E25077C7f5: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x100E38f7BCEc53937BDd79ADE46F34362470577B: total=2876, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x99E2B4B27BDe92b42D04B6CF302cF564D2C13b74: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xbfDcAF35f52F9ef423ac8F2621F9eef8be6dEd17: total=2833, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x6a395dE946c0493157404E2b1947493c633f569E: total=2874, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x6747c02DE7eb2099265e55715Ba2E03e8563D051: total=2840, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xcf716b3930d7cf6f2ADAD90A27c39fDc9D643BBd: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x831B837C3DA1B6c2AB68a690206bDfF368877E19: total=2863, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x36142A4f36974e2935192A1111C39330aA296D3C: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x197B2c44b887c4aC01243BDE7E4bBa8bd95BC3a8: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xf34CD6c09a59d7D3d1a6C3dC231a7834E5615D6A: total=2834, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x2928FE5b911BCAf837cAd93eB9626E86a189f1dd: total=2829, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x1Be7f70BCf8393a7e4A5BcC66F6f15d6e35cfBBC: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xF9B38D02959379d43C764064dE201324d5e12931: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x3AaF7817618728ffEF81898E11A3171C33faAE41: total=2874, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xc5B9d978715F081E226cb28bADB7Ba4cde5f9775: total=2879, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x718361fc3637199F24a2437331677D6B89a40519: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xDCA5DFF3D42f2db3C18dBE823380A0A81db49A7E: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x19E356ebC20283fc74AF0BA4C179502A1F62fA7B: total=2833, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x5E17e837DcBa2728C94f95c38fA8a47CB9C8818F: total=2876, suspicious_submissions=0, fraction_suspicious_submissions=0%\n\n\nPlease note, total indicates the total number of submissions recorded for this validator. suspicious_submissions shows how many of those submissions were flagged as suspicious (e.g. out of range or zero/negative values). fraction_suspicious_submissions reports the percentage of the validator’s submissions that fell into the suspicious category.",
    "crumbs": [
      "Notebooks",
      "Issue 3"
    ]
  },
  {
    "objectID": "notebooks/issue_3.html#out-of-range-suspicious-values",
    "href": "notebooks/issue_3.html#out-of-range-suspicious-values",
    "title": "Issue 3",
    "section": "",
    "text": "This notebook documents the analysis for Issue #3: Out-of-Range / Suspicious Values in the Autonity Oracle data. It covers:\n\nWhat is this issue about?\n\nWhy conduct this issue analysis?\n\nHow to conduct this issue analysis?\n\nWhat are the results?\n\n\n\n\nCertain Oracle submissions are unexpectedly large, zero, negative, or significantly off-market compared to real FX data. Examples include:\n\nExtremely large prices like 6.3e+25 indicating scaling errors.\nNegative or zero prices, which should not occur.\nLarge spikes or sudden changes inconsistent with actual market data.\n\nAdditionally, cross-rates for Autonity tokens (ATN, NTN) may be inconsistent (NTN-USD ≠ NTN-ATN × ATN-USD).\n\n\n\n\n\nData Integrity: Ensuring accuracy and reliability of the Oracle data.\nSecurity & Reliability: Identifying potential bugs or malicious activities before Mainnet.\nCross-rate Consistency: Confirming internal consistency for token prices.\n\n\n\n\n\nUse Python with the Polars library (v1.24.0) to:\n\nLoad and preprocess Oracle submission CSV files.\nConverted price submissions from Wei to decimals (dividing by 1e18).\nCompared FX pairs to minute-level Yahoo Finance benchmarks to detect:\n\nDeviations exceeding ±20%.\nNegative, zero, or excessively large prices.\n\nChecked Autonity token cross-rates for consistency within a 10% tolerance.\n\nBelow is the analysis script:\n\nimport polars as pl\nimport glob\nimport math\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndef load_and_preprocess_oracle_submissions(submission_glob: str) -&gt; pl.LazyFrame:\n    \"\"\"\n    Loads Oracle Submission CSVs and returns a Polars DataFrame.\n    \"\"\"\n    files = sorted(glob.glob(submission_glob))\n    if not files:\n        raise ValueError(f\"No CSV files found matching pattern {submission_glob}\")\n\n    lf_list = []\n    for f in files:\n        lf_temp = pl.scan_csv(\n            f,\n            dtypes={\"Timestamp\": pl.Utf8},\n            null_values=[\"\"],\n            ignore_errors=True,\n        )\n        lf_list.append(lf_temp)\n\n    lf = pl.concat(lf_list)\n    lf = lf.with_columns(\n        pl.col(\"Timestamp\")\n        .str.strptime(pl.Datetime, strict=False)\n        .alias(\"Timestamp_dt\")\n    )\n\n    lf = lf.with_columns(\n        [\n            pl.col(\"Timestamp_dt\").cast(pl.Date).alias(\"date_only\"),\n            pl.col(\"Timestamp_dt\").dt.weekday().alias(\"weekday_num\"),\n        ]\n    )\n    return lf\n\n\ndef load_yahoo_finance_data(directory_pattern: str, pair_label: str) -&gt; pl.DataFrame:\n    \"\"\"\n    Loads Yahoo Finance CSVs and returns a Polars DataFrame.\n    \"\"\"\n    files = sorted(glob.glob(directory_pattern))\n    if not files:\n        raise ValueError(f\"No Yahoo Finance CSV files found: {directory_pattern}\")\n\n    lf_list = []\n    for f in files:\n        lf_temp = pl.scan_csv(\n            f,\n            has_header=False,\n            skip_rows=3,\n            new_columns=[\"Datetime\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\"],\n            try_parse_dates=True,\n        )\n        lf_list.append(lf_temp)\n\n    lf = pl.concat(lf_list)\n    df = (\n        lf.select(\n            [\n                pl.col(\"Datetime\").alias(\"timestamp_benchmark\"),\n                pl.col(\"Close\").alias(\"benchmark_close\"),\n            ]\n        )\n        .sort(\"timestamp_benchmark\")\n        .collect()\n        .with_columns(\n            [\n                pl.lit(pair_label).alias(\"symbol\"),\n            ]\n        )\n    )\n    return df\n\n\ndef load_all_fx_benchmarks() -&gt; dict[str, pl.DataFrame]:\n    \"\"\"\n    Loads FX data from Yahoo Finance.\n    \"\"\"\n    mapping = {\n        \"AUD-USD\": \"../yahoo-finance/data/AUDUSD/AUDUSD=X_1m_*.csv\",\n        \"CAD-USD\": \"../yahoo-finance/data/CADUSD/CADUSD=X_1m_*.csv\",\n        \"EUR-USD\": \"../yahoo-finance/data/EURUSD/EURUSD=X_1m_*.csv\",\n        \"GBP-USD\": \"../yahoo-finance/data/GBPUSD/GBPUSD=X_1m_*.csv\",\n        \"JPY-USD\": \"../yahoo-finance/data/JPYUSD/JPYUSD=X_1m_*.csv\",\n        \"SEK-USD\": \"../yahoo-finance/data/SEKUSD/SEKUSD=X_1m_*.csv\",\n    }\n\n    result = {}\n    for pair_label, pattern in mapping.items():\n        df_pair = load_yahoo_finance_data(pattern, pair_label)\n        result[pair_label] = df_pair\n    return result\n\n\ndef convert_wei_to_decimal(price_wei: float) -&gt; float:\n    \"\"\"\n    Convert from Wei-based representation to a decimal.\n    \"\"\"\n    if price_wei is None or math.isnan(price_wei):\n        return None\n    return price_wei / 1e18\n\n\ndef flag_suspicious_values(\n    df_submissions: pl.DataFrame,\n    fx_pairs: list[str],\n    autonity_pairs: list[str],\n    fx_benchmarks: dict[str, pl.DataFrame],\n    percent_threshold: float = 0.20,\n    join_tolerance: str = \"30s\",\n    dynamic_thresholds: dict[str, float] | None = None,\n):\n    \"\"\"\n    Detect suspicious or out-of-range values in Oracle data using as-of joins for time alignment\n    and dynamic thresholds for 'excessively large' values.\n    \"\"\"\n    new_cols = []\n    for c in fx_pairs + autonity_pairs:\n        if c.endswith(\" Price\"):\n            dec_col = c.replace(\" Price\", \" Price Decimal\")\n            new_cols.append((pl.col(c).cast(pl.Float64) / 1e18).alias(dec_col))\n\n    df_submissions = df_submissions.with_columns(new_cols)\n\n    suspicious_frames: list[pl.DataFrame] = []\n\n    final_columns = [\n        \"Timestamp_dt\",\n        \"Validator Address\",\n        \"oracle_price_decimal\",\n        \"benchmark_close\",\n        \"rel_diff_from_bench\",\n        \"ATN-USD Price Decimal\",\n        \"NTN-USD Price Decimal\",\n        \"NTN-ATN Price Decimal\",\n        \"ntn_usd_estimated\",\n        \"rel_diff_cross\",\n        \"suspect_reason\",\n    ]\n\n    for pair_label in fx_pairs:\n        if not pair_label.endswith(\" Price\"):\n            continue\n\n        base_name = pair_label.replace(\" Price\", \"\")  # e.g. \"AUD-USD\"\n        decimal_col = f\"{base_name} Price Decimal\"\n\n        if base_name not in fx_benchmarks:\n            continue\n\n        df_bench = fx_benchmarks[base_name]\n\n        df_local = (\n            df_submissions\n            .select([\"Timestamp_dt\", \"Validator Address\", decimal_col])\n            .sort(\"Timestamp_dt\")\n        )\n\n        df_bench_sorted = df_bench.sort(\"timestamp_benchmark\")\n\n        df_joined = df_local.join_asof(\n            df_bench_sorted,\n            left_on=\"Timestamp_dt\",\n            right_on=\"timestamp_benchmark\",\n            strategy=\"nearest\",    # or \"backward\"/\"forward\"\n            tolerance=join_tolerance\n        ).with_columns(\n            (\n                (pl.col(decimal_col) - pl.col(\"benchmark_close\")).abs()\n                / pl.col(\"benchmark_close\").abs()\n            )\n            .alias(\"rel_diff_from_bench\")\n        )\n\n        if dynamic_thresholds and base_name in dynamic_thresholds:\n            max_threshold = dynamic_thresholds[base_name]\n        else:\n            max_threshold = 5.0  # fallback\n\n        df_flagged_fx = (\n            df_joined\n            .select(\n                [\n                    \"Timestamp_dt\",\n                    \"Validator Address\",\n                    pl.col(decimal_col).alias(\"oracle_price_decimal\"),\n                    \"benchmark_close\",\n                    \"rel_diff_from_bench\",\n                ]\n            )\n            .with_columns(\n                [\n                    pl.when(\n                        (pl.col(\"oracle_price_decimal\").is_not_null()) &\n                        (pl.col(\"oracle_price_decimal\") &lt;= 0)\n                    )\n                    .then(pl.lit(\"Non-positive price; \"))\n                    .otherwise(pl.lit(\"\"))\n                    .alias(\"cond1\"),\n\n                    pl.when(\n                        (pl.col(\"oracle_price_decimal\").is_not_null()) &\n                        (pl.col(\"oracle_price_decimal\") &gt;= max_threshold)\n                    )\n                    .then(pl.lit(\"Excessively large price; \"))\n                    .otherwise(pl.lit(\"\"))\n                    .alias(\"cond2\"),\n\n                    pl.when(\n                        (pl.col(\"oracle_price_decimal\").is_not_null()) &\n                        (pl.col(\"benchmark_close\").is_not_null()) &\n                        (pl.col(\"rel_diff_from_bench\") &gt; percent_threshold)\n                    )\n                    .then(pl.lit(f\"Deviation &gt; {int(percent_threshold*100)}%; \"))\n                    .otherwise(pl.lit(\"\"))\n                    .alias(\"cond3\"),\n                ]\n            )\n            .with_columns(\n                [\n                    (pl.col(\"cond1\") + pl.col(\"cond2\") + pl.col(\"cond3\")).alias(\"suspect_reason\")\n                ]\n            )\n            .filter(pl.col(\"suspect_reason\") != \"\")\n            .drop([\"cond1\", \"cond2\", \"cond3\"])\n        )\n\n        df_flagged_fx = df_flagged_fx.with_columns(\n            [\n                pl.lit(None).cast(pl.Float64).alias(\"ATN-USD Price Decimal\"),\n                pl.lit(None).cast(pl.Float64).alias(\"NTN-USD Price Decimal\"),\n                pl.lit(None).cast(pl.Float64).alias(\"NTN-ATN Price Decimal\"),\n                pl.lit(None).cast(pl.Float64).alias(\"ntn_usd_estimated\"),\n                pl.lit(None).cast(pl.Float64).alias(\"rel_diff_cross\"),\n            ]\n        )\n\n        df_flagged_fx = df_flagged_fx.select(final_columns)\n        suspicious_frames.append(df_flagged_fx)\n\n    required_cols = {\n        \"ATN-USD Price Decimal\",\n        \"NTN-USD Price Decimal\",\n        \"NTN-ATN Price Decimal\",\n    }\n    if required_cols.issubset(set(df_submissions.columns)):\n        df_autonity = df_submissions.select(\n            [\n                \"Timestamp_dt\",\n                \"Validator Address\",\n                \"ATN-USD Price Decimal\",\n                \"NTN-USD Price Decimal\",\n                \"NTN-ATN Price Decimal\",\n            ]\n        )\n\n        df_autonity = df_autonity.with_columns(\n            (pl.col(\"NTN-ATN Price Decimal\") * pl.col(\"ATN-USD Price Decimal\"))\n            .alias(\"ntn_usd_estimated\")\n        )\n\n        df_autonity = df_autonity.with_columns(\n            (\n                (\n                    (pl.col(\"ntn_usd_estimated\") - pl.col(\"NTN-USD Price Decimal\")).abs()\n                    / (pl.col(\"NTN-USD Price Decimal\").abs() + 1e-18)\n                ).alias(\"rel_diff_cross\")\n            )\n        )\n\n        cross_tolerance = 0.10  # 10%\n        df_autonity_suspect = (\n            df_autonity\n            .with_columns(\n                pl.when(pl.col(\"rel_diff_cross\") &gt; cross_tolerance)\n                .then(pl.lit(\"Cross-rate mismatch &gt; 10%; \"))\n                .otherwise(pl.lit(\"\"))\n                .alias(\"suspect_reason\")\n            )\n            .filter(pl.col(\"suspect_reason\") != \"\")\n        )\n\n        df_autonity_suspect = df_autonity_suspect.with_columns(\n            [\n                pl.lit(None).cast(pl.Float64).alias(\"oracle_price_decimal\"),\n                pl.lit(None).cast(pl.Float64).alias(\"benchmark_close\"),\n                pl.lit(None).cast(pl.Float64).alias(\"rel_diff_from_bench\"),\n            ]\n        )\n\n        df_autonity_suspect = df_autonity_suspect.select(final_columns)\n        suspicious_frames.append(df_autonity_suspect)\n\n    if suspicious_frames:\n        df_suspicious = pl.concat(suspicious_frames, how=\"vertical\")\n    else:\n        df_suspicious = pl.DataFrame(\n            {\"Timestamp_dt\": [], \"Validator Address\": [], \"suspect_reason\": []}\n        )\n\n    return df_suspicious\n\n\ndef analyze_out_of_range_values(\n    submission_glob: str,\n    fx_pairs: list[str],\n    autonity_pairs: list[str],\n    yahoo_data_dict: dict[str, pl.DataFrame],\n    deviation_threshold: float = 0.20,\n    join_tolerance: str = \"30s\",\n    dynamic_thresholds: dict[str, float] | None = None,\n):\n    \"\"\"\n    Main analysis function.\n    \"\"\"\n    lf_sub = load_and_preprocess_oracle_submissions(submission_glob)\n    df_sub = lf_sub.collect()\n\n    df_suspicious = flag_suspicious_values(\n        df_submissions=df_sub,\n        fx_pairs=fx_pairs,\n        autonity_pairs=autonity_pairs,\n        fx_benchmarks=yahoo_data_dict,\n        percent_threshold=deviation_threshold,\n        join_tolerance=join_tolerance,\n        dynamic_thresholds=dynamic_thresholds,\n    )\n\n    if not df_suspicious.is_empty():\n        suspicious_preview = df_suspicious.to_dicts()\n        for row in suspicious_preview:\n            ts_ = row.get(\"Timestamp_dt\")\n            val_addr = row.get(\"Validator Address\")\n            reason = row.get(\"suspect_reason\")\n            price = row.get(\"oracle_price_decimal\")\n            benchmark = row.get(\"benchmark_close\")\n            rel_diff = row.get(\"rel_diff_from_bench\")\n            cross_diff = row.get(\"rel_diff_cross\")  # if from cross-rate\n\n            line_parts = [f\"{ts_} | {val_addr} | {reason}\"]\n            if price is not None:\n                line_parts.append(f\"oracle_price={price:.4f}\")\n            if benchmark is not None:\n                line_parts.append(f\"bench={benchmark:.4f}\")\n            if rel_diff is not None:\n                line_parts.append(f\"diff={rel_diff*100:.2f}%\")\n            if cross_diff is not None:\n                line_parts.append(f\"cross_diff={cross_diff*100:.2f}%\")\n\n    return df_suspicious\n\n\nfx_cols = [\n    \"AUD-USD Price\",\n    \"CAD-USD Price\",\n    \"EUR-USD Price\",\n    \"GBP-USD Price\",\n    \"JPY-USD Price\",\n    \"SEK-USD Price\",\n]\nautonity_cols = [\"ATN-USD Price\", \"NTN-USD Price\", \"NTN-ATN Price\"]\n\nyahoo_data = load_all_fx_benchmarks()\n\ndf_outliers = analyze_out_of_range_values(\n    submission_glob=\"../submission-data/Oracle_Submission_*.csv\",\n    fx_pairs=fx_cols,\n    autonity_pairs=autonity_cols,\n    yahoo_data_dict=yahoo_data,\n    deviation_threshold=0.20,\n    join_tolerance=\"30s\",\n    dynamic_thresholds={\n        \"AUD-USD\": 2.0,\n        \"CAD-USD\": 2.0,\n        \"EUR-USD\": 3.0,\n        \"GBP-USD\": 3.0,\n        \"JPY-USD\": 200.0,\n        \"SEK-USD\": 20.0,\n        \"ATN-USD\": 1.0,\n        \"NTN-USD\": 1.0,\n        \"NTN-ATN\": 1.0,        \n    },\n)\n\n\n\n\n\nThe following results summarize the suspicious submissions detected:\n\nnum_suspicious = df_outliers.height\nprint(f\"Total suspicious submissions detected: {num_suspicious}\")\n\nif num_suspicious == 0:\n    print(\"No suspicious values detected within the ±20% threshold.\")\nelse:\n    display(df_outliers)\n\nTotal suspicious submissions detected: 0\nNo suspicious values detected within the ±20% threshold.\n\n\nNote: You may see many null in the df_outliers table. This is expected behavior when a row is only flagged for a specific category (e.g., Forex mismatch or cross-rate mismatch), and the columns for the other category remain null. If the table is empty, that indicates no outliers were detected.\n\nNegative or zero prices: Indicate significant issues like data feed outages or software errors.\nExtreme values: Likely result from incorrect scaling or data staleness.\nLarge deviations (&gt;20%): Suggest problems with validator data sources or calculation logic.\nCross-rate mismatches (&gt;10%): Highlight misconfigurations or inconsistencies between token price feeds.\n\nValidators frequently flagged with suspicious data require further investigation, particularly if patterns or correlations emerge.\nList of all Validators and their Fraction Suspecious Submissions\n\nlf_sub = load_and_preprocess_oracle_submissions(\"../submission-data/Oracle_Submission_*.csv\")\ndf_all_submissions = lf_sub.collect()\n\ndf_validator_submissions = (\n    df_all_submissions\n    .group_by(\"Validator Address\")\n    .agg([\n        pl.count().alias(\"total_submissions\"),\n    ])\n)\n\ndf_validator_outliers = (\n    df_outliers\n    .group_by(\"Validator Address\")\n    .agg([\n        pl.count().alias(\"suspicious_submissions\"),\n    ])\n)\n\ndf_validator_stats = (\n    df_validator_submissions\n    .join(df_validator_outliers, on=\"Validator Address\", how=\"left\")\n    .with_columns([\n        pl.col(\"suspicious_submissions\").fill_null(0),  # if a validator never appears in df_outliers\n        (\n            pl.col(\"suspicious_submissions\") / pl.col(\"total_submissions\")\n        ).alias(\"suspicious_ratio\")\n    ])\n)\n\ndf_validator_stats = (\n    df_validator_stats\n    .select([\n        \"Validator Address\",\n        \"total_submissions\",\n        \"suspicious_submissions\",\n        (pl.col(\"suspicious_ratio\") * 100).round(2).alias(\"suspicious_ratio_pct\"),\n    ])\n    .sort(\"suspicious_submissions\", descending=True)\n)\n\nfor row in df_validator_stats.to_dicts():\n    fraction_suspicious_submissions = row['suspicious_ratio_pct']\n    if fraction_suspicious_submissions is None:\n        fraction_suspicious_submissions = \"0%\"\n    else:\n        fraction_suspicious_submissions = f\"{fraction_suspicious_submissions}%\"\n    print(\n        f\"Validator {row['Validator Address']}: \"\n        f\"total={row['total_submissions']}, \"\n        f\"suspicious_submissions={row['suspicious_submissions']}, \"\n        f\"fraction_suspicious_submissions={fraction_suspicious_submissions}\"\n    )\n\nValidator 0xE4686A4C6E63A8ab51B458c52EB779AEcf0B74f7: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x01F788E4371a70D579C178Ea7F48E04e8B2CD743: total=2837, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x3597d2D42f8Fbbc82E8b1046048773aD6DDB717E: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xf10f56Bf0A28E0737c7e6bB0aF92f3DDad34aE6a: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x94470A842Ea4f44e668EB9C2AB81367b6Ce01772: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xdF239e0D5b4E6e820B0cFEF6972A90893c2073AB: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xE9FFF86CAdC3136b3D94948B8Fd23631EDaa2dE3: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x791A7F840ac11841cCB0FaA968B2e3a0Db930fCe: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x527192F3D2408C84087607b7feE1d0f907821E17: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x3fe573552E14a0FC11Da25E43Fef11e16a785068: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x9d28e40E9Ec4789f9A0D17e421F76D8D0868EA44: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x984A46Ec685Bb41A7BBb2bc39f80C78410ff4057: total=2877, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x64F83c2538A646A550Ad9bEEb63427a377359DEE: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x22A76e194A49c9e5508Cd4A3E1cD555D088ECB08: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x23b4Be9536F93b8D550214912fD0e38417Ff7209: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x24915749B793375a8C93090AF19928aFF1CAEcb6: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xBE287C82A786218E008FF97320b08244BE4A282c: total=2879, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x94d28f08Ff81A80f4716C0a8EfC6CAC2Ec74d09E: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x00a96aaED75015Bb44cED878D927dcb15ec1FF54: total=2866, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x8584A78A9b94f332A34BBf24D2AF83367Da31894: total=2879, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xd61a48b0e11B0Dc6b7Bd713B1012563c52591BAA: total=2873, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x9C7dAABb5101623340C925CFD6fF74088ff5672e: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xfD97FB8835d25740A2Da27c69762D74F6A931858: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x8f91e0ADF8065C3fFF92297267E02DF32C2978FF: total=2879, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x59031767f20EA8F4a3d90d33aB0DAA2ca469Fd9a: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xD9fDab408dF7Ae751691BeC2efE3b713ba3f9C36: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x383A3c437d3F12f60E5fC990119468D3561EfBfc: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x26E2724dBD14Fbd52be430B97043AA4c83F05852: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xd625d50B0d087861c286d726eC51Cf4Bd9c54357: total=2856, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x1476A65D7B5739dE1805d5130441A94022Ee49fe: total=2462, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x7232e75a8bFd8c9ab002BB3A00eAa885BC72A6dd: total=2877, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xcdEed21b471b0Dc54faF74480A0E700fCc42a7b6: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xBBf36374eb23968F25aecAEbb97BF3118f3c2fEC: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xB5d8be2AB4b6d7E6be7Ea28E91b370223a06289f: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x551f3300FCFE0e392178b3542c009948008B2a9F: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x358488a4EdCA493FCD87610dcd50c62c8A3Dd658: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xEf0Ba5e345C2C3937df5667A870Aae5105CAa3a5: total=2879, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x4cD134001EEF0843B9c69Ba9569d11fDcF4bd495: total=2823, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x5603caFE3313D0cf56Fd4bE4A2f606dD6E43F8Eb: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xDF2D0052ea56A860443039619f6DAe4434bc0Ac4: total=2879, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xC1F9acAF1824F6C906b35A0D2584D6E25077C7f5: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x100E38f7BCEc53937BDd79ADE46F34362470577B: total=2876, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x99E2B4B27BDe92b42D04B6CF302cF564D2C13b74: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xbfDcAF35f52F9ef423ac8F2621F9eef8be6dEd17: total=2833, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x6a395dE946c0493157404E2b1947493c633f569E: total=2874, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x6747c02DE7eb2099265e55715Ba2E03e8563D051: total=2840, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xcf716b3930d7cf6f2ADAD90A27c39fDc9D643BBd: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x831B837C3DA1B6c2AB68a690206bDfF368877E19: total=2863, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x36142A4f36974e2935192A1111C39330aA296D3C: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x197B2c44b887c4aC01243BDE7E4bBa8bd95BC3a8: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xf34CD6c09a59d7D3d1a6C3dC231a7834E5615D6A: total=2834, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x2928FE5b911BCAf837cAd93eB9626E86a189f1dd: total=2829, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x1Be7f70BCf8393a7e4A5BcC66F6f15d6e35cfBBC: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xF9B38D02959379d43C764064dE201324d5e12931: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x3AaF7817618728ffEF81898E11A3171C33faAE41: total=2874, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xc5B9d978715F081E226cb28bADB7Ba4cde5f9775: total=2879, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x718361fc3637199F24a2437331677D6B89a40519: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0xDCA5DFF3D42f2db3C18dBE823380A0A81db49A7E: total=2880, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x19E356ebC20283fc74AF0BA4C179502A1F62fA7B: total=2833, suspicious_submissions=0, fraction_suspicious_submissions=0%\nValidator 0x5E17e837DcBa2728C94f95c38fA8a47CB9C8818F: total=2876, suspicious_submissions=0, fraction_suspicious_submissions=0%\n\n\nPlease note, total indicates the total number of submissions recorded for this validator. suspicious_submissions shows how many of those submissions were flagged as suspicious (e.g. out of range or zero/negative values). fraction_suspicious_submissions reports the percentage of the validator’s submissions that fell into the suspicious category.",
    "crumbs": [
      "Notebooks",
      "Issue 3"
    ]
  },
  {
    "objectID": "notebooks/issue_7.html",
    "href": "notebooks/issue_7.html",
    "title": "Issue 7",
    "section": "",
    "text": "This notebook documents the analysis for Issue #7: Timing / Synchronization Issues in the Autonity Oracle data. It covers:\n\nWhat is this issue about?\n\nWhy conduct this issue analysis?\n\nHow to conduct this issue analysis?\n\nWhat are the results?\n\n\n\n\nSome validators submit their data earlier or later than others, creating synchronization or timing problems in the Oracle system. Possible symptoms include:\n\nLarge differences in timestamps among validators for the same minute or round.\nData consistently arriving late or early.\nPotential clock skew or network delays.\n\nThis analysis examines how synchronized each validator’s submission timestamps are compared to the median submission timestamp within each minute.\n\n\n\n\n\nReliability: Timely, synchronized data submission is critical for accurate on-chain aggregation.\nDiagnostics: Identifying validators with systematic timing offsets provides clear targets for correction before Mainnet launch.\nTransparency: Documenting timing deviations clearly helps the team diagnose network or clock issues effectively.\n\n\n\n\n\nUse Python with the Polars library (v1.24.0) to:\n\nLoad and preprocess Oracle submission CSV files.\nParse timestamps from strings to actual datetimes.\nCompute each validator’s submission offset (in seconds) relative to the median timestamp within each 30-second bin (re-anchored every 6 hours).\nSummarize timing offsets per validator:\n\nMean, median, max offsets\nFraction of submissions exceeding thresholds (e.g., 30s, 60s)\n\n\nBelow is the script:\n\nimport polars as pl\nimport glob\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndef load_and_preprocess_submissions(submission_glob: str) -&gt; pl.DataFrame:\n    \"\"\"\n    Loads Oracle Submission CSVs into a Polars DataFrame,\n    parsing timestamps into datetime.\n    \"\"\"\n    files = sorted(glob.glob(submission_glob))\n    if not files:\n        raise ValueError(f\"No CSV files found matching pattern {submission_glob}\")\n\n    lf_list = []\n    for f in files:\n        lf_temp = pl.scan_csv(\n            f,\n            dtypes={\"Timestamp\": pl.Utf8},\n            null_values=[\"\"],\n            ignore_errors=True,\n        )\n        lf_list.append(lf_temp)\n\n    lf = pl.concat(lf_list)\n\n    lf = lf.with_columns(\n        pl.col(\"Timestamp\")\n        .str.strptime(pl.Datetime, strict=False)\n        .alias(\"Timestamp_dt\")\n    )\n\n    lf = lf.with_columns(\n        [\n            pl.col(\"Timestamp_dt\").cast(pl.Date).alias(\"date_only\"),\n            pl.col(\"Timestamp_dt\").dt.weekday().alias(\"weekday_num\"),\n        ]\n    )\n\n    df = lf.collect()\n\n    return df\n\n\ndef compute_timing_offsets_30s_reanchor_6h(\n    df: pl.DataFrame,\n    chunk_hours: int = 6,\n    period_seconds: int = 30\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Computes offsets by grouping submissions into ~30s bins,\n    re-anchoring every 'chunk_hours' hours.\n    \"\"\"\n    df_local = df.with_columns(\n        (pl.col(\"Timestamp_dt\").cast(pl.Int64) // 1_000_000_000).alias(\"epoch_seconds\")\n    )\n\n    anchor_epoch = df_local.select(pl.min(\"epoch_seconds\")).item()\n    chunk_length_sec = chunk_hours * 3600\n\n    df_local = df_local.with_columns(\n        (\n            (pl.col(\"epoch_seconds\") - anchor_epoch) // chunk_length_sec\n        ).alias(\"chunk_id\")\n    )\n\n    df_local = df_local.with_columns(\n        (\n            pl.col(\"epoch_seconds\")\n            - (anchor_epoch + pl.col(\"chunk_id\") * chunk_length_sec)\n        ).alias(\"local_elapsed\")\n    )\n\n    df_local = df_local.with_columns(\n        (pl.col(\"local_elapsed\") // period_seconds).alias(\"round_in_chunk\")\n    )\n\n    df_local = df_local.with_columns(\n        (\n            pl.col(\"chunk_id\").cast(pl.Utf8)\n            + \"-\"\n            + pl.col(\"round_in_chunk\").cast(pl.Utf8)\n        ).alias(\"round_label\")\n    )\n\n    median_lf = (\n        df_local.lazy()\n        .group_by(\"round_label\")\n        .agg(pl.median(\"epoch_seconds\").alias(\"median_epoch_seconds\"))\n    )\n\n    df_with_median = (\n        df_local.lazy()\n        .join(median_lf, on=\"round_label\", how=\"left\")\n        .with_columns(\n            (pl.col(\"epoch_seconds\") - pl.col(\"median_epoch_seconds\"))\n            .alias(\"offset_seconds\")\n        )\n        .with_columns(\n            pl.col(\"offset_seconds\").abs().alias(\"abs_offset_seconds\")\n        )\n    )\n\n    return df_with_median.collect().sort([\"Timestamp_dt\", \"Validator Address\"])\n\n\ndef summarize_timing_offsets(df_offsets: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"\n    Summarizes computed offsets in timings per validator.\n    \"\"\"\n    thresholds = [10, 30, 60, 300]\n\n    def exceed_expr(t: int):\n        return (\n            (pl.col(\"abs_offset_seconds\") &gt; t)\n            .cast(pl.Int64)\n            .sum()\n            .alias(f\"exceed_{t}s_count\")\n        )\n\n    agg_exprs = [\n        pl.count(\"Validator Address\").alias(\"total_submissions\"),\n        pl.mean(\"offset_seconds\").alias(\"mean_offset_seconds\"),\n        pl.median(\"offset_seconds\").alias(\"median_offset_seconds\"),\n        pl.max(\"abs_offset_seconds\").alias(\"max_offset_seconds\"),\n    ] + [exceed_expr(t) for t in thresholds]\n\n    lf_summary = (\n        df_offsets.lazy()\n        .group_by(\"Validator Address\")\n        .agg(agg_exprs)\n        .with_columns(\n            [\n                (pl.col(f\"exceed_{t}s_count\") / pl.col(\"total_submissions\"))\n                .alias(f\"fraction_exceed_{t}s\")\n                for t in thresholds\n            ]\n        )\n    )\n    return lf_summary.collect().sort(\"mean_offset_seconds\")\n\n\ndef analyze_timing_synchronization_issues_30s_6h(\n    submission_glob: str\n) -&gt; dict:\n    \"\"\"\n    Main analysis function with 30s-based grouping and 6-hour re-anchoring.\n    Returns a dict of DataFrames:\n      - df_all_data:   The raw submission data\n      - df_with_offsets:  The data with computed offsets\n      - df_validator_offsets: Summaries per validator\n    \"\"\"\n    df_all = load_and_preprocess_submissions(submission_glob)\n    df_with_offsets = compute_timing_offsets_30s_reanchor_6h(df_all)\n    df_validator_offsets = summarize_timing_offsets(df_with_offsets)\n\n    return {\n        \"df_all_data\": df_all,\n        \"df_with_offsets\": df_with_offsets,\n        \"df_validator_offsets\": df_validator_offsets,\n    }\n\n\nresults = analyze_timing_synchronization_issues_30s_6h(\n    submission_glob=\"../submission-data/Oracle_Submission_*.csv\"\n)\n\n\n\n\n\nBelow are summaries and interpretation based on the computed results.\nPer-Submission Timing Offsets\n\n# Preview submission offsets\nresults[\"df_with_offsets\"]\n\n\nshape: (171_957, 31)\n\n\n\nTimestamp\nValidator Address\nAUD-USD Price\nAUD-USD Confidence\nCAD-USD Price\nCAD-USD Confidence\nEUR-USD Price\nEUR-USD Confidence\nGBP-USD Price\nGBP-USD Confidence\nJPY-USD Price\nJPY-USD Confidence\nSEK-USD Price\nSEK-USD Confidence\nATN-USD Price\nATN-USD Confidence\nNTN-USD Price\nNTN-USD Confidence\nNTN-ATN Price\nNTN-ATN Confidence\nTimestamp_dt\ndate_only\nweekday_num\nepoch_seconds\nchunk_id\nlocal_elapsed\nround_in_chunk\nround_label\nmedian_epoch_seconds\noffset_seconds\nabs_offset_seconds\n\n\nstr\nstr\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ndatetime[μs, UTC]\ndate\ni8\ni64\ni64\ni64\ni64\nstr\nf64\nf64\nf64\n\n\n\n\n\"2025-01-01T00:00:12+00:00\"\n\"0x100E38f7BCEc53937BDd79ADE46F…\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\n2025-01-01 00:00:12 UTC\n2025-01-01\n3\n1735689\n0\n0\n0\n\"0-0\"\n1.735704e6\n-15.0\n15.0\n\n\n\"2025-01-01T00:00:12+00:00\"\n\"0x1476A65D7B5739dE1805d5130441…\n620325937441701400\n100\n695598691900918500\n100\n1038118434882147900\n100\n1253219756960152400\n100\n6361566293421600\n100\n90498188759938100\n100\n719187673997958190\n100\n739468327258002540\n100\n1028199389385116100\n100\n2025-01-01 00:00:12 UTC\n2025-01-01\n3\n1735689\n0\n0\n0\n\"0-0\"\n1.735704e6\n-15.0\n15.0\n\n\n\"2025-01-01T00:00:12+00:00\"\n\"0x197B2c44b887c4aC01243BDE7E4b…\n621851874883402800\n90\n695797383801836900\n90\n1040636869764295700\n90\n1254849995231570000\n90\n6362132586843100\n90\n90668407500090700\n90\n719187673997958190\n100\n739468327258002540\n100\n1028199389385116100\n100\n2025-01-01 00:00:12 UTC\n2025-01-01\n3\n1735689\n0\n0\n0\n\"0-0\"\n1.735704e6\n-15.0\n15.0\n\n\n\"2025-01-01T00:00:12+00:00\"\n\"0x1Be7f70BCf8393a7e4A5BcC66F6f…\n621851874883402800\n50\n696815552923141200\n50\n1040636869764295700\n50\n1254849995231570000\n50\n6369426751592400\n50\n90668407500090700\n50\n719187695422512255\n100\n739470280548770193\n100\n1028202074723125300\n100\n2025-01-01 00:00:12 UTC\n2025-01-01\n3\n1735689\n0\n0\n0\n\"0-0\"\n1.735704e6\n-15.0\n15.0\n\n\n\"2025-01-01T00:00:12+00:00\"\n\"0x22A76e194A49c9e5508Cd4A3E1cD…\n618489241070407000\n50\n694800529715923900\n50\n1034998473377251800\n50\n1251589518688734700\n50\n6354941963946300\n50\n90327970019785400\n50\n719187673997958190\n100\n739468021811862294\n100\n1028198964675195000\n100\n2025-01-01 00:00:12 UTC\n2025-01-01\n3\n1735689\n0\n0\n0\n\"0-0\"\n1.735704e6\n-15.0\n15.0\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"2025-01-01T23:59:44+00:00\"\n\"0xdF239e0D5b4E6e820B0cFEF6972A…\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\n2025-01-01 23:59:44 UTC\n2025-01-01\n3\n1735775\n0\n86\n2\n\"0-2\"\n1.735762e6\n13.0\n13.0\n\n\n\"2025-01-01T23:59:44+00:00\"\n\"0xf10f56Bf0A28E0737c7e6bB0aF92…\n618878902204408000\n100\n695394575677597300\n100\n1035639460950966400\n100\n1251439155469255300\n100\n6359816494648100\n100\n90434278037913900\n100\n722416634674059554\n100\n738461996489158416\n100\n1022210676007396000\n100\n2025-01-01 23:59:44 UTC\n2025-01-01\n3\n1735775\n0\n86\n2\n\"0-2\"\n1.735762e6\n13.0\n13.0\n\n\n\"2025-01-01T23:59:44+00:00\"\n\"0xf34CD6c09a59d7D3d1a6C3dC231a…\n619348846941496400\n100\n695394575677597300\n100\n1036552710966674300\n100\n1251905399877559600\n100\n6363149239103500\n100\n90507036482522300\n100\n722446500612394871\n100\n738492525770173755\n100\n1022210676007396000\n100\n2025-01-01 23:59:44 UTC\n2025-01-01\n3\n1735775\n0\n86\n2\n\"0-2\"\n1.735762e6\n13.0\n13.0\n\n\n\"2025-01-01T23:59:44+00:00\"\n\"0xfD97FB8835d25740A2Da27c69762…\n618878902204408000\n100\n695394575677597300\n100\n1035639460950966400\n100\n1251439155469255300\n100\n6359816494648100\n100\n90434278037913900\n100\n722446500612394871\n100\n738492525770173755\n100\n1022210676007396000\n100\n2025-01-01 23:59:44 UTC\n2025-01-01\n3\n1735775\n0\n86\n2\n\"0-2\"\n1.735762e6\n13.0\n13.0\n\n\n\"2025-01-01T23:59:45+00:00\"\n\"0x19E356ebC20283fc74AF0BA4C179…\n618878902204408000\n100\n695394575677597300\n100\n1035639460950966400\n100\n1251439155469255300\n100\n6359816494648100\n100\n90434278037913900\n100\n722446525464800496\n100\n738492525770173755\n100\n1022210640843001800\n100\n2025-01-01 23:59:45 UTC\n2025-01-01\n3\n1735775\n0\n86\n2\n\"0-2\"\n1.735762e6\n13.0\n13.0\n\n\n\n\n\n\nThe table above illustrates validators’ submission offsets compared to the median timestamp per minute:\n\nPositive values indicate submissions later than median.\nNegative values indicate earlier submissions.\n\nValidator-Level Offset Summary\n\n# Validator-level summary of timing offsets\nresults[\"df_validator_offsets\"]\n\n\nshape: (60, 13)\n\n\n\nValidator Address\ntotal_submissions\nmean_offset_seconds\nmedian_offset_seconds\nmax_offset_seconds\nexceed_10s_count\nexceed_30s_count\nexceed_60s_count\nexceed_300s_count\nfraction_exceed_10s\nfraction_exceed_30s\nfraction_exceed_60s\nfraction_exceed_300s\n\n\nstr\nu32\nf64\nf64\nf64\ni64\ni64\ni64\ni64\nf64\nf64\nf64\nf64\n\n\n\n\n\"0x1476A65D7B5739dE1805d5130441…\n2462\n-0.666531\n-1.0\n15.0\n780\n0\n0\n0\n0.316816\n0.0\n0.0\n0.0\n\n\n\"0x7232e75a8bFd8c9ab002BB3A00eA…\n2877\n-0.254432\n0.0\n15.0\n778\n0\n0\n0\n0.270421\n0.0\n0.0\n0.0\n\n\n\"0x831B837C3DA1B6c2AB68a690206b…\n2863\n-0.25358\n0.0\n15.0\n777\n0\n0\n0\n0.271394\n0.0\n0.0\n0.0\n\n\n\"0x6747c02DE7eb2099265e55715Ba2…\n2840\n-0.253169\n0.0\n15.0\n770\n0\n0\n0\n0.271127\n0.0\n0.0\n0.0\n\n\n\"0x01F788E4371a70D579C178Ea7F48…\n2837\n-0.251322\n0.0\n15.0\n768\n0\n0\n0\n0.270708\n0.0\n0.0\n0.0\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"0x984A46Ec685Bb41A7BBb2bc39f80…\n2877\n-0.2374\n0.0\n15.0\n780\n0\n0\n0\n0.271116\n0.0\n0.0\n0.0\n\n\n\"0xd61a48b0e11B0Dc6b7Bd713B1012…\n2873\n-0.233554\n0.0\n15.0\n779\n0\n0\n0\n0.271145\n0.0\n0.0\n0.0\n\n\n\"0xbfDcAF35f52F9ef423ac8F2621F9…\n2833\n-0.232616\n0.0\n15.0\n768\n0\n0\n0\n0.271091\n0.0\n0.0\n0.0\n\n\n\"0x00a96aaED75015Bb44cED878D927…\n2866\n-0.223657\n0.0\n15.0\n774\n0\n0\n0\n0.270063\n0.0\n0.0\n0.0\n\n\n\"0x4cD134001EEF0843B9c69Ba9569d…\n2823\n-0.216082\n0.0\n15.0\n766\n0\n0\n0\n0.271343\n0.0\n0.0\n0.0\n\n\n\n\n\n\nInterpretation of Validator Timing Offsets\nInterpretation of validator-level offsets using the metrics above:\n\nMean Offset: Validators with high positive mean offsets (&gt;20s) consistently submit late, suggesting potential clock or scheduling issues.\nMedian Offset: Confirms the consistency of early or late submissions.\nMax Offset: Large values (&gt;60s) suggest occasional severe delays or network disruptions.\nFraction Exceeding Thresholds: High fractions (&gt;10%) indicate frequent timing deviations.\n\nValidators with Significant Timing Issues\n\nlate_validators = results[\"df_validator_offsets\"].filter(pl.col(\"mean_offset_seconds\") &gt; 30)\nearly_validators = results[\"df_validator_offsets\"].filter(pl.col(\"mean_offset_seconds\") &lt; -30)\n\nprint(\"Consistently Late Validators (&gt;30s delay):\", late_validators[\"Validator Address\"].to_list())\nprint(\"Consistently Early Validators (&gt;30s early):\", early_validators[\"Validator Address\"].to_list())\n\nConsistently Late Validators (&gt;30s delay): []\nConsistently Early Validators (&gt;30s early): []\n\n\n\nValidators listed as “Consistently Late” or “Early” warrant immediate investigation of clock synchronization or scheduling configurations.\n\nWeekend vs. Weekday Offset Patterns\n\ndf_offsets = results[\"df_with_offsets\"]\ndf_weekday_offset = df_offsets.group_by(\"weekday_num\").agg(pl.mean(\"abs_offset_seconds\").alias(\"avg_abs_offset\")).sort(\"weekday_num\")\ndf_weekday_offset\n\n\nshape: (1, 2)\n\n\n\nweekday_num\navg_abs_offset\n\n\ni8\nf64\n\n\n\n\n3\n7.219497\n\n\n\n\n\n\nThe above table reveals whether average absolute timing offsets differ substantially by day-of-week. Higher offsets on weekends could indicate decreased validator attention or configuration issues specific to weekends.\nList of all Validators and their Mean of Abs Offset Seconds\n\ndf_offsets = results[\"df_with_offsets\"]\ndf_mean_abs_offset = (\n    df_offsets\n    .group_by(\"Validator Address\")\n    .agg(\n        pl.mean(\"abs_offset_seconds\").alias(\"mean_abs_offset_seconds\")\n    )\n    .sort(\"mean_abs_offset_seconds\", descending=True)\n)\n\nfor row in df_mean_abs_offset.to_dicts():\n    print(\n        f\"Validator {row['Validator Address']}: \"\n        f\"mean_abs_offset_seconds={row['mean_abs_offset_seconds']:.2f}\"\n    )\n\nValidator 0x1476A65D7B5739dE1805d5130441A94022Ee49fe: mean_abs_offset_seconds=7.83\nValidator 0x4cD134001EEF0843B9c69Ba9569d11fDcF4bd495: mean_abs_offset_seconds=7.22\nValidator 0x2928FE5b911BCAf837cAd93eB9626E86a189f1dd: mean_abs_offset_seconds=7.22\nValidator 0x6747c02DE7eb2099265e55715Ba2E03e8563D051: mean_abs_offset_seconds=7.22\nValidator 0xd61a48b0e11B0Dc6b7Bd713B1012563c52591BAA: mean_abs_offset_seconds=7.21\nValidator 0x831B837C3DA1B6c2AB68a690206bDfF368877E19: mean_abs_offset_seconds=7.21\nValidator 0x01F788E4371a70D579C178Ea7F48E04e8B2CD743: mean_abs_offset_seconds=7.21\nValidator 0x5E17e837DcBa2728C94f95c38fA8a47CB9C8818F: mean_abs_offset_seconds=7.21\nValidator 0x984A46Ec685Bb41A7BBb2bc39f80C78410ff4057: mean_abs_offset_seconds=7.21\nValidator 0x6a395dE946c0493157404E2b1947493c633f569E: mean_abs_offset_seconds=7.21\nValidator 0x8f91e0ADF8065C3fFF92297267E02DF32C2978FF: mean_abs_offset_seconds=7.21\nValidator 0x8584A78A9b94f332A34BBf24D2AF83367Da31894: mean_abs_offset_seconds=7.21\nValidator 0xEf0Ba5e345C2C3937df5667A870Aae5105CAa3a5: mean_abs_offset_seconds=7.21\nValidator 0xbfDcAF35f52F9ef423ac8F2621F9eef8be6dEd17: mean_abs_offset_seconds=7.21\nValidator 0x3AaF7817618728ffEF81898E11A3171C33faAE41: mean_abs_offset_seconds=7.21\nValidator 0xDF2D0052ea56A860443039619f6DAe4434bc0Ac4: mean_abs_offset_seconds=7.21\nValidator 0xf34CD6c09a59d7D3d1a6C3dC231a7834E5615D6A: mean_abs_offset_seconds=7.21\nValidator 0x59031767f20EA8F4a3d90d33aB0DAA2ca469Fd9a: mean_abs_offset_seconds=7.21\nValidator 0xf10f56Bf0A28E0737c7e6bB0aF92f3DDad34aE6a: mean_abs_offset_seconds=7.21\nValidator 0x94d28f08Ff81A80f4716C0a8EfC6CAC2Ec74d09E: mean_abs_offset_seconds=7.21\nValidator 0xB5d8be2AB4b6d7E6be7Ea28E91b370223a06289f: mean_abs_offset_seconds=7.21\nValidator 0x383A3c437d3F12f60E5fC990119468D3561EfBfc: mean_abs_offset_seconds=7.21\nValidator 0x94470A842Ea4f44e668EB9C2AB81367b6Ce01772: mean_abs_offset_seconds=7.21\nValidator 0x1Be7f70BCf8393a7e4A5BcC66F6f15d6e35cfBBC: mean_abs_offset_seconds=7.21\nValidator 0x791A7F840ac11841cCB0FaA968B2e3a0Db930fCe: mean_abs_offset_seconds=7.21\nValidator 0x3fe573552E14a0FC11Da25E43Fef11e16a785068: mean_abs_offset_seconds=7.21\nValidator 0x23b4Be9536F93b8D550214912fD0e38417Ff7209: mean_abs_offset_seconds=7.21\nValidator 0xDCA5DFF3D42f2db3C18dBE823380A0A81db49A7E: mean_abs_offset_seconds=7.21\nValidator 0xE9FFF86CAdC3136b3D94948B8Fd23631EDaa2dE3: mean_abs_offset_seconds=7.21\nValidator 0xfD97FB8835d25740A2Da27c69762D74F6A931858: mean_abs_offset_seconds=7.21\nValidator 0x26E2724dBD14Fbd52be430B97043AA4c83F05852: mean_abs_offset_seconds=7.21\nValidator 0xF9B38D02959379d43C764064dE201324d5e12931: mean_abs_offset_seconds=7.21\nValidator 0x22A76e194A49c9e5508Cd4A3E1cD555D088ECB08: mean_abs_offset_seconds=7.21\nValidator 0xE4686A4C6E63A8ab51B458c52EB779AEcf0B74f7: mean_abs_offset_seconds=7.21\nValidator 0x551f3300FCFE0e392178b3542c009948008B2a9F: mean_abs_offset_seconds=7.21\nValidator 0x718361fc3637199F24a2437331677D6B89a40519: mean_abs_offset_seconds=7.21\nValidator 0x3597d2D42f8Fbbc82E8b1046048773aD6DDB717E: mean_abs_offset_seconds=7.21\nValidator 0xcf716b3930d7cf6f2ADAD90A27c39fDc9D643BBd: mean_abs_offset_seconds=7.21\nValidator 0xcdEed21b471b0Dc54faF74480A0E700fCc42a7b6: mean_abs_offset_seconds=7.21\nValidator 0x358488a4EdCA493FCD87610dcd50c62c8A3Dd658: mean_abs_offset_seconds=7.21\nValidator 0x9C7dAABb5101623340C925CFD6fF74088ff5672e: mean_abs_offset_seconds=7.21\nValidator 0xBBf36374eb23968F25aecAEbb97BF3118f3c2fEC: mean_abs_offset_seconds=7.21\nValidator 0x24915749B793375a8C93090AF19928aFF1CAEcb6: mean_abs_offset_seconds=7.21\nValidator 0xC1F9acAF1824F6C906b35A0D2584D6E25077C7f5: mean_abs_offset_seconds=7.21\nValidator 0x64F83c2538A646A550Ad9bEEb63427a377359DEE: mean_abs_offset_seconds=7.21\nValidator 0x9d28e40E9Ec4789f9A0D17e421F76D8D0868EA44: mean_abs_offset_seconds=7.21\nValidator 0x36142A4f36974e2935192A1111C39330aA296D3C: mean_abs_offset_seconds=7.21\nValidator 0xD9fDab408dF7Ae751691BeC2efE3b713ba3f9C36: mean_abs_offset_seconds=7.21\nValidator 0x5603caFE3313D0cf56Fd4bE4A2f606dD6E43F8Eb: mean_abs_offset_seconds=7.21\nValidator 0xdF239e0D5b4E6e820B0cFEF6972A90893c2073AB: mean_abs_offset_seconds=7.21\nValidator 0x197B2c44b887c4aC01243BDE7E4bBa8bd95BC3a8: mean_abs_offset_seconds=7.21\nValidator 0x527192F3D2408C84087607b7feE1d0f907821E17: mean_abs_offset_seconds=7.21\nValidator 0x99E2B4B27BDe92b42D04B6CF302cF564D2C13b74: mean_abs_offset_seconds=7.21\nValidator 0x19E356ebC20283fc74AF0BA4C179502A1F62fA7B: mean_abs_offset_seconds=7.21\nValidator 0x100E38f7BCEc53937BDd79ADE46F34362470577B: mean_abs_offset_seconds=7.21\nValidator 0xBE287C82A786218E008FF97320b08244BE4A282c: mean_abs_offset_seconds=7.21\nValidator 0xc5B9d978715F081E226cb28bADB7Ba4cde5f9775: mean_abs_offset_seconds=7.21\nValidator 0xd625d50B0d087861c286d726eC51Cf4Bd9c54357: mean_abs_offset_seconds=7.21\nValidator 0x7232e75a8bFd8c9ab002BB3A00eAa885BC72A6dd: mean_abs_offset_seconds=7.21\nValidator 0x00a96aaED75015Bb44cED878D927dcb15ec1FF54: mean_abs_offset_seconds=7.20\n\n\nPlease note, a low mean_abs_offset_seconds indicates the validator typically submits very close to the group median, while a high value indicates they often drift too far from the typical submission time.",
    "crumbs": [
      "Notebooks",
      "Issue 7"
    ]
  },
  {
    "objectID": "notebooks/issue_7.html#timing-synchronization-issues",
    "href": "notebooks/issue_7.html#timing-synchronization-issues",
    "title": "Issue 7",
    "section": "",
    "text": "This notebook documents the analysis for Issue #7: Timing / Synchronization Issues in the Autonity Oracle data. It covers:\n\nWhat is this issue about?\n\nWhy conduct this issue analysis?\n\nHow to conduct this issue analysis?\n\nWhat are the results?\n\n\n\n\nSome validators submit their data earlier or later than others, creating synchronization or timing problems in the Oracle system. Possible symptoms include:\n\nLarge differences in timestamps among validators for the same minute or round.\nData consistently arriving late or early.\nPotential clock skew or network delays.\n\nThis analysis examines how synchronized each validator’s submission timestamps are compared to the median submission timestamp within each minute.\n\n\n\n\n\nReliability: Timely, synchronized data submission is critical for accurate on-chain aggregation.\nDiagnostics: Identifying validators with systematic timing offsets provides clear targets for correction before Mainnet launch.\nTransparency: Documenting timing deviations clearly helps the team diagnose network or clock issues effectively.\n\n\n\n\n\nUse Python with the Polars library (v1.24.0) to:\n\nLoad and preprocess Oracle submission CSV files.\nParse timestamps from strings to actual datetimes.\nCompute each validator’s submission offset (in seconds) relative to the median timestamp within each 30-second bin (re-anchored every 6 hours).\nSummarize timing offsets per validator:\n\nMean, median, max offsets\nFraction of submissions exceeding thresholds (e.g., 30s, 60s)\n\n\nBelow is the script:\n\nimport polars as pl\nimport glob\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndef load_and_preprocess_submissions(submission_glob: str) -&gt; pl.DataFrame:\n    \"\"\"\n    Loads Oracle Submission CSVs into a Polars DataFrame,\n    parsing timestamps into datetime.\n    \"\"\"\n    files = sorted(glob.glob(submission_glob))\n    if not files:\n        raise ValueError(f\"No CSV files found matching pattern {submission_glob}\")\n\n    lf_list = []\n    for f in files:\n        lf_temp = pl.scan_csv(\n            f,\n            dtypes={\"Timestamp\": pl.Utf8},\n            null_values=[\"\"],\n            ignore_errors=True,\n        )\n        lf_list.append(lf_temp)\n\n    lf = pl.concat(lf_list)\n\n    lf = lf.with_columns(\n        pl.col(\"Timestamp\")\n        .str.strptime(pl.Datetime, strict=False)\n        .alias(\"Timestamp_dt\")\n    )\n\n    lf = lf.with_columns(\n        [\n            pl.col(\"Timestamp_dt\").cast(pl.Date).alias(\"date_only\"),\n            pl.col(\"Timestamp_dt\").dt.weekday().alias(\"weekday_num\"),\n        ]\n    )\n\n    df = lf.collect()\n\n    return df\n\n\ndef compute_timing_offsets_30s_reanchor_6h(\n    df: pl.DataFrame,\n    chunk_hours: int = 6,\n    period_seconds: int = 30\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Computes offsets by grouping submissions into ~30s bins,\n    re-anchoring every 'chunk_hours' hours.\n    \"\"\"\n    df_local = df.with_columns(\n        (pl.col(\"Timestamp_dt\").cast(pl.Int64) // 1_000_000_000).alias(\"epoch_seconds\")\n    )\n\n    anchor_epoch = df_local.select(pl.min(\"epoch_seconds\")).item()\n    chunk_length_sec = chunk_hours * 3600\n\n    df_local = df_local.with_columns(\n        (\n            (pl.col(\"epoch_seconds\") - anchor_epoch) // chunk_length_sec\n        ).alias(\"chunk_id\")\n    )\n\n    df_local = df_local.with_columns(\n        (\n            pl.col(\"epoch_seconds\")\n            - (anchor_epoch + pl.col(\"chunk_id\") * chunk_length_sec)\n        ).alias(\"local_elapsed\")\n    )\n\n    df_local = df_local.with_columns(\n        (pl.col(\"local_elapsed\") // period_seconds).alias(\"round_in_chunk\")\n    )\n\n    df_local = df_local.with_columns(\n        (\n            pl.col(\"chunk_id\").cast(pl.Utf8)\n            + \"-\"\n            + pl.col(\"round_in_chunk\").cast(pl.Utf8)\n        ).alias(\"round_label\")\n    )\n\n    median_lf = (\n        df_local.lazy()\n        .group_by(\"round_label\")\n        .agg(pl.median(\"epoch_seconds\").alias(\"median_epoch_seconds\"))\n    )\n\n    df_with_median = (\n        df_local.lazy()\n        .join(median_lf, on=\"round_label\", how=\"left\")\n        .with_columns(\n            (pl.col(\"epoch_seconds\") - pl.col(\"median_epoch_seconds\"))\n            .alias(\"offset_seconds\")\n        )\n        .with_columns(\n            pl.col(\"offset_seconds\").abs().alias(\"abs_offset_seconds\")\n        )\n    )\n\n    return df_with_median.collect().sort([\"Timestamp_dt\", \"Validator Address\"])\n\n\ndef summarize_timing_offsets(df_offsets: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"\n    Summarizes computed offsets in timings per validator.\n    \"\"\"\n    thresholds = [10, 30, 60, 300]\n\n    def exceed_expr(t: int):\n        return (\n            (pl.col(\"abs_offset_seconds\") &gt; t)\n            .cast(pl.Int64)\n            .sum()\n            .alias(f\"exceed_{t}s_count\")\n        )\n\n    agg_exprs = [\n        pl.count(\"Validator Address\").alias(\"total_submissions\"),\n        pl.mean(\"offset_seconds\").alias(\"mean_offset_seconds\"),\n        pl.median(\"offset_seconds\").alias(\"median_offset_seconds\"),\n        pl.max(\"abs_offset_seconds\").alias(\"max_offset_seconds\"),\n    ] + [exceed_expr(t) for t in thresholds]\n\n    lf_summary = (\n        df_offsets.lazy()\n        .group_by(\"Validator Address\")\n        .agg(agg_exprs)\n        .with_columns(\n            [\n                (pl.col(f\"exceed_{t}s_count\") / pl.col(\"total_submissions\"))\n                .alias(f\"fraction_exceed_{t}s\")\n                for t in thresholds\n            ]\n        )\n    )\n    return lf_summary.collect().sort(\"mean_offset_seconds\")\n\n\ndef analyze_timing_synchronization_issues_30s_6h(\n    submission_glob: str\n) -&gt; dict:\n    \"\"\"\n    Main analysis function with 30s-based grouping and 6-hour re-anchoring.\n    Returns a dict of DataFrames:\n      - df_all_data:   The raw submission data\n      - df_with_offsets:  The data with computed offsets\n      - df_validator_offsets: Summaries per validator\n    \"\"\"\n    df_all = load_and_preprocess_submissions(submission_glob)\n    df_with_offsets = compute_timing_offsets_30s_reanchor_6h(df_all)\n    df_validator_offsets = summarize_timing_offsets(df_with_offsets)\n\n    return {\n        \"df_all_data\": df_all,\n        \"df_with_offsets\": df_with_offsets,\n        \"df_validator_offsets\": df_validator_offsets,\n    }\n\n\nresults = analyze_timing_synchronization_issues_30s_6h(\n    submission_glob=\"../submission-data/Oracle_Submission_*.csv\"\n)\n\n\n\n\n\nBelow are summaries and interpretation based on the computed results.\nPer-Submission Timing Offsets\n\n# Preview submission offsets\nresults[\"df_with_offsets\"]\n\n\nshape: (171_957, 31)\n\n\n\nTimestamp\nValidator Address\nAUD-USD Price\nAUD-USD Confidence\nCAD-USD Price\nCAD-USD Confidence\nEUR-USD Price\nEUR-USD Confidence\nGBP-USD Price\nGBP-USD Confidence\nJPY-USD Price\nJPY-USD Confidence\nSEK-USD Price\nSEK-USD Confidence\nATN-USD Price\nATN-USD Confidence\nNTN-USD Price\nNTN-USD Confidence\nNTN-ATN Price\nNTN-ATN Confidence\nTimestamp_dt\ndate_only\nweekday_num\nepoch_seconds\nchunk_id\nlocal_elapsed\nround_in_chunk\nround_label\nmedian_epoch_seconds\noffset_seconds\nabs_offset_seconds\n\n\nstr\nstr\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ndatetime[μs, UTC]\ndate\ni8\ni64\ni64\ni64\ni64\nstr\nf64\nf64\nf64\n\n\n\n\n\"2025-01-01T00:00:12+00:00\"\n\"0x100E38f7BCEc53937BDd79ADE46F…\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\n2025-01-01 00:00:12 UTC\n2025-01-01\n3\n1735689\n0\n0\n0\n\"0-0\"\n1.735704e6\n-15.0\n15.0\n\n\n\"2025-01-01T00:00:12+00:00\"\n\"0x1476A65D7B5739dE1805d5130441…\n620325937441701400\n100\n695598691900918500\n100\n1038118434882147900\n100\n1253219756960152400\n100\n6361566293421600\n100\n90498188759938100\n100\n719187673997958190\n100\n739468327258002540\n100\n1028199389385116100\n100\n2025-01-01 00:00:12 UTC\n2025-01-01\n3\n1735689\n0\n0\n0\n\"0-0\"\n1.735704e6\n-15.0\n15.0\n\n\n\"2025-01-01T00:00:12+00:00\"\n\"0x197B2c44b887c4aC01243BDE7E4b…\n621851874883402800\n90\n695797383801836900\n90\n1040636869764295700\n90\n1254849995231570000\n90\n6362132586843100\n90\n90668407500090700\n90\n719187673997958190\n100\n739468327258002540\n100\n1028199389385116100\n100\n2025-01-01 00:00:12 UTC\n2025-01-01\n3\n1735689\n0\n0\n0\n\"0-0\"\n1.735704e6\n-15.0\n15.0\n\n\n\"2025-01-01T00:00:12+00:00\"\n\"0x1Be7f70BCf8393a7e4A5BcC66F6f…\n621851874883402800\n50\n696815552923141200\n50\n1040636869764295700\n50\n1254849995231570000\n50\n6369426751592400\n50\n90668407500090700\n50\n719187695422512255\n100\n739470280548770193\n100\n1028202074723125300\n100\n2025-01-01 00:00:12 UTC\n2025-01-01\n3\n1735689\n0\n0\n0\n\"0-0\"\n1.735704e6\n-15.0\n15.0\n\n\n\"2025-01-01T00:00:12+00:00\"\n\"0x22A76e194A49c9e5508Cd4A3E1cD…\n618489241070407000\n50\n694800529715923900\n50\n1034998473377251800\n50\n1251589518688734700\n50\n6354941963946300\n50\n90327970019785400\n50\n719187673997958190\n100\n739468021811862294\n100\n1028198964675195000\n100\n2025-01-01 00:00:12 UTC\n2025-01-01\n3\n1735689\n0\n0\n0\n\"0-0\"\n1.735704e6\n-15.0\n15.0\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"2025-01-01T23:59:44+00:00\"\n\"0xdF239e0D5b4E6e820B0cFEF6972A…\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\n2025-01-01 23:59:44 UTC\n2025-01-01\n3\n1735775\n0\n86\n2\n\"0-2\"\n1.735762e6\n13.0\n13.0\n\n\n\"2025-01-01T23:59:44+00:00\"\n\"0xf10f56Bf0A28E0737c7e6bB0aF92…\n618878902204408000\n100\n695394575677597300\n100\n1035639460950966400\n100\n1251439155469255300\n100\n6359816494648100\n100\n90434278037913900\n100\n722416634674059554\n100\n738461996489158416\n100\n1022210676007396000\n100\n2025-01-01 23:59:44 UTC\n2025-01-01\n3\n1735775\n0\n86\n2\n\"0-2\"\n1.735762e6\n13.0\n13.0\n\n\n\"2025-01-01T23:59:44+00:00\"\n\"0xf34CD6c09a59d7D3d1a6C3dC231a…\n619348846941496400\n100\n695394575677597300\n100\n1036552710966674300\n100\n1251905399877559600\n100\n6363149239103500\n100\n90507036482522300\n100\n722446500612394871\n100\n738492525770173755\n100\n1022210676007396000\n100\n2025-01-01 23:59:44 UTC\n2025-01-01\n3\n1735775\n0\n86\n2\n\"0-2\"\n1.735762e6\n13.0\n13.0\n\n\n\"2025-01-01T23:59:44+00:00\"\n\"0xfD97FB8835d25740A2Da27c69762…\n618878902204408000\n100\n695394575677597300\n100\n1035639460950966400\n100\n1251439155469255300\n100\n6359816494648100\n100\n90434278037913900\n100\n722446500612394871\n100\n738492525770173755\n100\n1022210676007396000\n100\n2025-01-01 23:59:44 UTC\n2025-01-01\n3\n1735775\n0\n86\n2\n\"0-2\"\n1.735762e6\n13.0\n13.0\n\n\n\"2025-01-01T23:59:45+00:00\"\n\"0x19E356ebC20283fc74AF0BA4C179…\n618878902204408000\n100\n695394575677597300\n100\n1035639460950966400\n100\n1251439155469255300\n100\n6359816494648100\n100\n90434278037913900\n100\n722446525464800496\n100\n738492525770173755\n100\n1022210640843001800\n100\n2025-01-01 23:59:45 UTC\n2025-01-01\n3\n1735775\n0\n86\n2\n\"0-2\"\n1.735762e6\n13.0\n13.0\n\n\n\n\n\n\nThe table above illustrates validators’ submission offsets compared to the median timestamp per minute:\n\nPositive values indicate submissions later than median.\nNegative values indicate earlier submissions.\n\nValidator-Level Offset Summary\n\n# Validator-level summary of timing offsets\nresults[\"df_validator_offsets\"]\n\n\nshape: (60, 13)\n\n\n\nValidator Address\ntotal_submissions\nmean_offset_seconds\nmedian_offset_seconds\nmax_offset_seconds\nexceed_10s_count\nexceed_30s_count\nexceed_60s_count\nexceed_300s_count\nfraction_exceed_10s\nfraction_exceed_30s\nfraction_exceed_60s\nfraction_exceed_300s\n\n\nstr\nu32\nf64\nf64\nf64\ni64\ni64\ni64\ni64\nf64\nf64\nf64\nf64\n\n\n\n\n\"0x1476A65D7B5739dE1805d5130441…\n2462\n-0.666531\n-1.0\n15.0\n780\n0\n0\n0\n0.316816\n0.0\n0.0\n0.0\n\n\n\"0x7232e75a8bFd8c9ab002BB3A00eA…\n2877\n-0.254432\n0.0\n15.0\n778\n0\n0\n0\n0.270421\n0.0\n0.0\n0.0\n\n\n\"0x831B837C3DA1B6c2AB68a690206b…\n2863\n-0.25358\n0.0\n15.0\n777\n0\n0\n0\n0.271394\n0.0\n0.0\n0.0\n\n\n\"0x6747c02DE7eb2099265e55715Ba2…\n2840\n-0.253169\n0.0\n15.0\n770\n0\n0\n0\n0.271127\n0.0\n0.0\n0.0\n\n\n\"0x01F788E4371a70D579C178Ea7F48…\n2837\n-0.251322\n0.0\n15.0\n768\n0\n0\n0\n0.270708\n0.0\n0.0\n0.0\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"0x984A46Ec685Bb41A7BBb2bc39f80…\n2877\n-0.2374\n0.0\n15.0\n780\n0\n0\n0\n0.271116\n0.0\n0.0\n0.0\n\n\n\"0xd61a48b0e11B0Dc6b7Bd713B1012…\n2873\n-0.233554\n0.0\n15.0\n779\n0\n0\n0\n0.271145\n0.0\n0.0\n0.0\n\n\n\"0xbfDcAF35f52F9ef423ac8F2621F9…\n2833\n-0.232616\n0.0\n15.0\n768\n0\n0\n0\n0.271091\n0.0\n0.0\n0.0\n\n\n\"0x00a96aaED75015Bb44cED878D927…\n2866\n-0.223657\n0.0\n15.0\n774\n0\n0\n0\n0.270063\n0.0\n0.0\n0.0\n\n\n\"0x4cD134001EEF0843B9c69Ba9569d…\n2823\n-0.216082\n0.0\n15.0\n766\n0\n0\n0\n0.271343\n0.0\n0.0\n0.0\n\n\n\n\n\n\nInterpretation of Validator Timing Offsets\nInterpretation of validator-level offsets using the metrics above:\n\nMean Offset: Validators with high positive mean offsets (&gt;20s) consistently submit late, suggesting potential clock or scheduling issues.\nMedian Offset: Confirms the consistency of early or late submissions.\nMax Offset: Large values (&gt;60s) suggest occasional severe delays or network disruptions.\nFraction Exceeding Thresholds: High fractions (&gt;10%) indicate frequent timing deviations.\n\nValidators with Significant Timing Issues\n\nlate_validators = results[\"df_validator_offsets\"].filter(pl.col(\"mean_offset_seconds\") &gt; 30)\nearly_validators = results[\"df_validator_offsets\"].filter(pl.col(\"mean_offset_seconds\") &lt; -30)\n\nprint(\"Consistently Late Validators (&gt;30s delay):\", late_validators[\"Validator Address\"].to_list())\nprint(\"Consistently Early Validators (&gt;30s early):\", early_validators[\"Validator Address\"].to_list())\n\nConsistently Late Validators (&gt;30s delay): []\nConsistently Early Validators (&gt;30s early): []\n\n\n\nValidators listed as “Consistently Late” or “Early” warrant immediate investigation of clock synchronization or scheduling configurations.\n\nWeekend vs. Weekday Offset Patterns\n\ndf_offsets = results[\"df_with_offsets\"]\ndf_weekday_offset = df_offsets.group_by(\"weekday_num\").agg(pl.mean(\"abs_offset_seconds\").alias(\"avg_abs_offset\")).sort(\"weekday_num\")\ndf_weekday_offset\n\n\nshape: (1, 2)\n\n\n\nweekday_num\navg_abs_offset\n\n\ni8\nf64\n\n\n\n\n3\n7.219497\n\n\n\n\n\n\nThe above table reveals whether average absolute timing offsets differ substantially by day-of-week. Higher offsets on weekends could indicate decreased validator attention or configuration issues specific to weekends.\nList of all Validators and their Mean of Abs Offset Seconds\n\ndf_offsets = results[\"df_with_offsets\"]\ndf_mean_abs_offset = (\n    df_offsets\n    .group_by(\"Validator Address\")\n    .agg(\n        pl.mean(\"abs_offset_seconds\").alias(\"mean_abs_offset_seconds\")\n    )\n    .sort(\"mean_abs_offset_seconds\", descending=True)\n)\n\nfor row in df_mean_abs_offset.to_dicts():\n    print(\n        f\"Validator {row['Validator Address']}: \"\n        f\"mean_abs_offset_seconds={row['mean_abs_offset_seconds']:.2f}\"\n    )\n\nValidator 0x1476A65D7B5739dE1805d5130441A94022Ee49fe: mean_abs_offset_seconds=7.83\nValidator 0x4cD134001EEF0843B9c69Ba9569d11fDcF4bd495: mean_abs_offset_seconds=7.22\nValidator 0x2928FE5b911BCAf837cAd93eB9626E86a189f1dd: mean_abs_offset_seconds=7.22\nValidator 0x6747c02DE7eb2099265e55715Ba2E03e8563D051: mean_abs_offset_seconds=7.22\nValidator 0xd61a48b0e11B0Dc6b7Bd713B1012563c52591BAA: mean_abs_offset_seconds=7.21\nValidator 0x831B837C3DA1B6c2AB68a690206bDfF368877E19: mean_abs_offset_seconds=7.21\nValidator 0x01F788E4371a70D579C178Ea7F48E04e8B2CD743: mean_abs_offset_seconds=7.21\nValidator 0x5E17e837DcBa2728C94f95c38fA8a47CB9C8818F: mean_abs_offset_seconds=7.21\nValidator 0x984A46Ec685Bb41A7BBb2bc39f80C78410ff4057: mean_abs_offset_seconds=7.21\nValidator 0x6a395dE946c0493157404E2b1947493c633f569E: mean_abs_offset_seconds=7.21\nValidator 0x8f91e0ADF8065C3fFF92297267E02DF32C2978FF: mean_abs_offset_seconds=7.21\nValidator 0x8584A78A9b94f332A34BBf24D2AF83367Da31894: mean_abs_offset_seconds=7.21\nValidator 0xEf0Ba5e345C2C3937df5667A870Aae5105CAa3a5: mean_abs_offset_seconds=7.21\nValidator 0xbfDcAF35f52F9ef423ac8F2621F9eef8be6dEd17: mean_abs_offset_seconds=7.21\nValidator 0x3AaF7817618728ffEF81898E11A3171C33faAE41: mean_abs_offset_seconds=7.21\nValidator 0xDF2D0052ea56A860443039619f6DAe4434bc0Ac4: mean_abs_offset_seconds=7.21\nValidator 0xf34CD6c09a59d7D3d1a6C3dC231a7834E5615D6A: mean_abs_offset_seconds=7.21\nValidator 0x59031767f20EA8F4a3d90d33aB0DAA2ca469Fd9a: mean_abs_offset_seconds=7.21\nValidator 0xf10f56Bf0A28E0737c7e6bB0aF92f3DDad34aE6a: mean_abs_offset_seconds=7.21\nValidator 0x94d28f08Ff81A80f4716C0a8EfC6CAC2Ec74d09E: mean_abs_offset_seconds=7.21\nValidator 0xB5d8be2AB4b6d7E6be7Ea28E91b370223a06289f: mean_abs_offset_seconds=7.21\nValidator 0x383A3c437d3F12f60E5fC990119468D3561EfBfc: mean_abs_offset_seconds=7.21\nValidator 0x94470A842Ea4f44e668EB9C2AB81367b6Ce01772: mean_abs_offset_seconds=7.21\nValidator 0x1Be7f70BCf8393a7e4A5BcC66F6f15d6e35cfBBC: mean_abs_offset_seconds=7.21\nValidator 0x791A7F840ac11841cCB0FaA968B2e3a0Db930fCe: mean_abs_offset_seconds=7.21\nValidator 0x3fe573552E14a0FC11Da25E43Fef11e16a785068: mean_abs_offset_seconds=7.21\nValidator 0x23b4Be9536F93b8D550214912fD0e38417Ff7209: mean_abs_offset_seconds=7.21\nValidator 0xDCA5DFF3D42f2db3C18dBE823380A0A81db49A7E: mean_abs_offset_seconds=7.21\nValidator 0xE9FFF86CAdC3136b3D94948B8Fd23631EDaa2dE3: mean_abs_offset_seconds=7.21\nValidator 0xfD97FB8835d25740A2Da27c69762D74F6A931858: mean_abs_offset_seconds=7.21\nValidator 0x26E2724dBD14Fbd52be430B97043AA4c83F05852: mean_abs_offset_seconds=7.21\nValidator 0xF9B38D02959379d43C764064dE201324d5e12931: mean_abs_offset_seconds=7.21\nValidator 0x22A76e194A49c9e5508Cd4A3E1cD555D088ECB08: mean_abs_offset_seconds=7.21\nValidator 0xE4686A4C6E63A8ab51B458c52EB779AEcf0B74f7: mean_abs_offset_seconds=7.21\nValidator 0x551f3300FCFE0e392178b3542c009948008B2a9F: mean_abs_offset_seconds=7.21\nValidator 0x718361fc3637199F24a2437331677D6B89a40519: mean_abs_offset_seconds=7.21\nValidator 0x3597d2D42f8Fbbc82E8b1046048773aD6DDB717E: mean_abs_offset_seconds=7.21\nValidator 0xcf716b3930d7cf6f2ADAD90A27c39fDc9D643BBd: mean_abs_offset_seconds=7.21\nValidator 0xcdEed21b471b0Dc54faF74480A0E700fCc42a7b6: mean_abs_offset_seconds=7.21\nValidator 0x358488a4EdCA493FCD87610dcd50c62c8A3Dd658: mean_abs_offset_seconds=7.21\nValidator 0x9C7dAABb5101623340C925CFD6fF74088ff5672e: mean_abs_offset_seconds=7.21\nValidator 0xBBf36374eb23968F25aecAEbb97BF3118f3c2fEC: mean_abs_offset_seconds=7.21\nValidator 0x24915749B793375a8C93090AF19928aFF1CAEcb6: mean_abs_offset_seconds=7.21\nValidator 0xC1F9acAF1824F6C906b35A0D2584D6E25077C7f5: mean_abs_offset_seconds=7.21\nValidator 0x64F83c2538A646A550Ad9bEEb63427a377359DEE: mean_abs_offset_seconds=7.21\nValidator 0x9d28e40E9Ec4789f9A0D17e421F76D8D0868EA44: mean_abs_offset_seconds=7.21\nValidator 0x36142A4f36974e2935192A1111C39330aA296D3C: mean_abs_offset_seconds=7.21\nValidator 0xD9fDab408dF7Ae751691BeC2efE3b713ba3f9C36: mean_abs_offset_seconds=7.21\nValidator 0x5603caFE3313D0cf56Fd4bE4A2f606dD6E43F8Eb: mean_abs_offset_seconds=7.21\nValidator 0xdF239e0D5b4E6e820B0cFEF6972A90893c2073AB: mean_abs_offset_seconds=7.21\nValidator 0x197B2c44b887c4aC01243BDE7E4bBa8bd95BC3a8: mean_abs_offset_seconds=7.21\nValidator 0x527192F3D2408C84087607b7feE1d0f907821E17: mean_abs_offset_seconds=7.21\nValidator 0x99E2B4B27BDe92b42D04B6CF302cF564D2C13b74: mean_abs_offset_seconds=7.21\nValidator 0x19E356ebC20283fc74AF0BA4C179502A1F62fA7B: mean_abs_offset_seconds=7.21\nValidator 0x100E38f7BCEc53937BDd79ADE46F34362470577B: mean_abs_offset_seconds=7.21\nValidator 0xBE287C82A786218E008FF97320b08244BE4A282c: mean_abs_offset_seconds=7.21\nValidator 0xc5B9d978715F081E226cb28bADB7Ba4cde5f9775: mean_abs_offset_seconds=7.21\nValidator 0xd625d50B0d087861c286d726eC51Cf4Bd9c54357: mean_abs_offset_seconds=7.21\nValidator 0x7232e75a8bFd8c9ab002BB3A00eAa885BC72A6dd: mean_abs_offset_seconds=7.21\nValidator 0x00a96aaED75015Bb44cED878D927dcb15ec1FF54: mean_abs_offset_seconds=7.20\n\n\nPlease note, a low mean_abs_offset_seconds indicates the validator typically submits very close to the group median, while a high value indicates they often drift too far from the typical submission time.",
    "crumbs": [
      "Notebooks",
      "Issue 7"
    ]
  },
  {
    "objectID": "notebooks/issue_4.html",
    "href": "notebooks/issue_4.html",
    "title": "Issue 4",
    "section": "",
    "text": "This notebook documents the analysis for Issue #4: Stale / Lagging Data in the Autonity Oracle data. It covers:\n\nWhat is this issue about?\n\nWhy conduct this issue analysis?\n\nHow to conduct this issue analysis?\n\nWhat are the results?\n\n\n\n\nIn the Oracle system, validators submit price data that must reflect real-world market movements. However, issues may occur:\n\nStale data: Validator submits identical prices repeatedly for prolonged periods.\nLagging data: Validator’s reported price remains nearly unchanged despite significant market changes.\n\nThese indicate problems such as disconnected feeds or outdated caches.\n\n\n\n\n\nAccuracy: Ensuring data freshness and reliability.\nTroubleshooting: Detect potential API disconnections, stuck feeds, or caching issues.\nConfidence: Critical for Mainnet readiness.\n\n\n\n\n\nUse Python with the Polars library (v1.24.0) to:\n\nLoading Oracle submission data and Yahoo Finance benchmarks.\nDetecting stale data (repeated identical submissions ≥30 consecutive intervals).\nDetecting lagging data (market moves significantly, validator’s submission barely changes within 60-minute windows).\n\nBelow is the Python script to perform the analysis:\n\nimport polars as pl\nimport glob\nfrom typing import List, Dict\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndef load_and_preprocess_submissions(submission_glob: str) -&gt; pl.DataFrame:\n    \"\"\"\n    Loads Oracle Submission CSVs and returns a Polars DataFrame.\n    \"\"\"\n    files = sorted(glob.glob(submission_glob))\n    if not files:\n        raise ValueError(f\"No CSV files found matching pattern {submission_glob}\")\n    lf_list = []\n    for f in files:\n        lf_temp = pl.scan_csv(\n            f,\n            dtypes={\"Timestamp\": pl.Utf8},\n            null_values=[\"\"],\n            ignore_errors=True,\n        )\n        lf_list.append(lf_temp)\n\n    lf = pl.concat(lf_list)\n    lf = lf.with_columns(\n        pl.col(\"Timestamp\").str.strptime(pl.Datetime, strict=False).alias(\"Timestamp_dt\")\n    )\n    lf = lf.with_columns(\n        [\n            pl.col(\"Timestamp_dt\").cast(pl.Date).alias(\"date_only\"),\n            pl.col(\"Timestamp_dt\").dt.weekday().alias(\"weekday_num\"),\n        ]\n    )\n    return lf.collect()\n\n\ndef load_yahoo_finance_data(directory_pattern: str, pair_label: str) -&gt; pl.DataFrame:\n    \"\"\"\n    Loads Yahoo Finance CSVs and returns a Polars DataFrame.\n    \"\"\"\n    files = sorted(glob.glob(directory_pattern))\n    if not files:\n        raise ValueError(f\"No Yahoo Finance CSV files found: {directory_pattern}\")\n\n    lf_list = []\n    for f in files:\n        lf_temp = pl.scan_csv(\n            f,\n            has_header=False,\n            skip_rows=3,\n            new_columns=[\"Datetime\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\"],\n            try_parse_dates=True,\n        )\n        lf_list.append(lf_temp)\n\n    lf = pl.concat(lf_list).sort(\"Datetime\").select(\n        [\n            pl.col(\"Datetime\").alias(\"timestamp_benchmark\"),\n            pl.col(\"Close\").alias(\"benchmark_close\"),\n        ]\n    )\n    df = lf.collect().with_columns(pl.lit(pair_label).alias(\"symbol\"))\n    return df\n\n\ndef load_all_fx_benchmarks() -&gt; Dict[str, pl.DataFrame]:\n    \"\"\"\n    Loads FX data from Yahoo Finance.\n    \"\"\"\n    mapping = {\n        \"AUD-USD\": \"../yahoo-finance/data/AUDUSD/AUDUSD=X_1m_*.csv\",\n        \"CAD-USD\": \"../yahoo-finance/data/CADUSD/CADUSD=X_1m_*.csv\",\n        \"EUR-USD\": \"../yahoo-finance/data/EURUSD/EURUSD=X_1m_*.csv\",\n        \"GBP-USD\": \"../yahoo-finance/data/GBPUSD/GBPUSD=X_1m_*.csv\",\n        \"JPY-USD\": \"../yahoo-finance/data/JPYUSD/JPYUSD=X_1m_*.csv\",\n        \"SEK-USD\": \"../yahoo-finance/data/SEKUSD/SEKUSD=X_1m_*.csv\",\n    }\n    result = {}\n    for pair_label, pattern in mapping.items():\n        df_pair = load_yahoo_finance_data(pattern, pair_label)\n        result[pair_label] = df_pair\n    return result\n\n\ndef detect_stale_data(\n    df: pl.DataFrame,\n    price_cols: List[str],\n    max_consecutive_threshold: int = 30,\n    stale_tolerance: float = 1e-9  # Tolerance for float comparison\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Identifies potential stale data when the same price is repeated for\n    at least max_consecutive_threshold intervals, allowing small float tolerance.\n    Skips any rows with None in the relevant price columns to avoid TypeError.\n    \"\"\"\n    suspicious_frames = []\n    df_local = df.clone()\n    \n    new_cols = []\n    for pc in price_cols:\n        dec_col = pc.replace(\" Price\", \" Price Decimal\")\n        new_cols.append((pl.col(pc).cast(pl.Float64) / 1e18).alias(dec_col))\n    df_local = df_local.with_columns(new_cols)\n    \n    for pc in price_cols:\n        dec_col = pc.replace(\" Price\", \" Price Decimal\")\n        if dec_col in df_local.columns:\n            df_local = df_local.filter(pl.col(dec_col).is_not_null())\n\n    for pc in price_cols:\n        dec_col = pc.replace(\" Price\", \" Price Decimal\")\n        if dec_col not in df_local.columns:\n            continue\n\n        df_sub = (\n            df_local.select([\"Validator Address\", \"Timestamp_dt\", dec_col])\n            .filter(pl.col(\"Validator Address\").is_not_null())\n            .sort([\"Validator Address\", \"Timestamp_dt\"])\n        )\n\n        df_list = df_sub.to_dicts()\n        suspicious_records = []\n        \n        if not df_list:\n            continue\n\n        current_run_price = None\n        current_run_start_idx = 0\n        current_run_len = 0\n        current_validator = None\n\n        def finalize_run(run_val, start_i, end_i, run_len):\n            start_ts = df_list[start_i][\"Timestamp_dt\"]\n            end_ts = df_list[end_i][\"Timestamp_dt\"]\n            vaddr = df_list[start_i][\"Validator Address\"]\n            return {\n                \"Validator Address\": vaddr,\n                \"price_col\": pc,\n                \"repeated_value\": run_val,\n                \"start_timestamp\": start_ts,\n                \"end_timestamp\": end_ts,\n                \"run_length\": run_len,\n            }\n\n        for i, row in enumerate(df_list):\n            vaddr = row[\"Validator Address\"]\n            price_val = row[dec_col]\n            \n            if (current_validator is not None) and (vaddr != current_validator):\n                if current_run_len &gt;= max_consecutive_threshold:\n                    rec = finalize_run(current_run_price, current_run_start_idx, i - 1, current_run_len)\n                    suspicious_records.append(rec)\n                current_run_price = None\n                current_run_start_idx = i\n                current_run_len = 0\n                current_validator = vaddr\n\n            if (\n                current_run_price is not None\n                and vaddr == current_validator\n                and abs(price_val - current_run_price) &lt; stale_tolerance\n            ):\n                current_run_len += 1\n            else:\n                if current_run_len &gt;= max_consecutive_threshold:\n                    rec = finalize_run(current_run_price, current_run_start_idx, i - 1, current_run_len)\n                    suspicious_records.append(rec)\n                \n                current_run_price = price_val\n                current_run_start_idx = i\n                current_run_len = 1\n                current_validator = vaddr\n\n        if current_run_len &gt;= max_consecutive_threshold:\n            rec = finalize_run(current_run_price, current_run_start_idx, len(df_list) - 1, current_run_len)\n            suspicious_records.append(rec)\n\n        if suspicious_records:\n            df_sus = pl.DataFrame(suspicious_records)\n            suspicious_frames.append(df_sus)\n\n    if suspicious_frames:\n        return pl.concat(suspicious_frames, how=\"vertical\")\n    else:\n        return pl.DataFrame(\n            {\n                \"Validator Address\": [],\n                \"price_col\": [],\n                \"repeated_value\": [],\n                \"start_timestamp\": [],\n                \"end_timestamp\": [],\n                \"run_length\": [],\n            }\n        )\n\n\ndef detect_lagging_data(\n    df_oracle: pl.DataFrame,\n    fx_benchmarks: Dict[str, pl.DataFrame],\n    fx_pairs: List[str],\n    lag_threshold: float = 0.05,\n    time_window_minutes: int = 60\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Compare each validator's reported FX price vs. Yahoo's benchmark.\n    Now uses a forward as-of join to find the price 'at or after' (T + time_window_minutes).\n    \"\"\"\n    df_local = df_oracle.clone()\n    for pc in fx_pairs:\n        dec_col = pc.replace(\" Price\", \" Price Decimal\")\n        df_local = df_local.with_columns(\n            (pl.col(pc).cast(pl.Float64) / 1e18).alias(dec_col)\n        )\n\n    suspicious_frames = []\n\n    for pc in fx_pairs:\n        base_label = pc.replace(\" Price\", \"\")\n        dec_col = base_label + \" Price Decimal\"\n        if dec_col not in df_local.columns:\n            continue\n        if base_label not in fx_benchmarks:\n            continue\n\n        df_sub = df_local.select([\"Timestamp_dt\", \"Validator Address\", dec_col]).filter(\n            pl.col(\"Validator Address\").is_not_null()\n        )\n        df_sub = df_sub.with_columns(\n            pl.col(\"Timestamp_dt\").dt.truncate(\"1m\").alias(\"ts_minute\")\n        )\n\n        lf_sub = (\n            df_sub.lazy()\n            .group_by([\"ts_minute\", \"Validator Address\"])\n            .agg(pl.col(dec_col).last().alias(\"price_decimal\"))\n        )\n        df_val_prices = lf_sub.collect().sort([\"Validator Address\", \"ts_minute\"])\n\n        df_val_prices_future = df_val_prices.with_columns(\n            (pl.col(\"ts_minute\") + pl.duration(minutes=time_window_minutes)).alias(\"ts_future\")\n        )\n\n        left_lf = df_val_prices_future.lazy().sort([\"Validator Address\", \"ts_minute\"])\n        right_lf = (\n            df_val_prices_future.lazy()\n            .select([\n                pl.col(\"Validator Address\"),\n                pl.col(\"ts_minute\").alias(\"ts_minute_future\"),\n                pl.col(\"price_decimal\").alias(\"price_decimal_future\"),\n            ])\n            .sort([\"Validator Address\", \"ts_minute_future\"])\n        )\n\n        joined_lf = left_lf.join_asof(\n            right_lf,\n            left_on=\"ts_future\",\n            right_on=\"ts_minute_future\",\n            on=\"Validator Address\",\n            strategy=\"forward\",\n            suffix=\"_r\"\n        )\n\n        df_joined = joined_lf.collect().with_columns(\n            pl.col(\"price_decimal\").alias(\"price_now\")\n        )\n\n        df_joined = df_joined.with_columns(\n            pl.when(\n                (pl.col(\"price_decimal_future\").is_not_null())\n                & (pl.col(\"price_decimal_future\") &gt; 0)\n                & (pl.col(\"price_now\") &gt; 0)\n            )\n            .then((pl.col(\"price_decimal_future\") - pl.col(\"price_now\")) / pl.col(\"price_now\"))\n            .otherwise(None)\n            .alias(\"validator_pct_change\")\n        )\n\n        df_bench = fx_benchmarks[base_label]\n        df_bench = df_bench.with_columns(\n            pl.col(\"timestamp_benchmark\").dt.truncate(\"1m\").alias(\"ts_minute_bench\")\n        ).sort(\"ts_minute_bench\")\n\n        lf_bench_now = (\n            df_bench.lazy()\n            .group_by(\"ts_minute_bench\")\n            .agg(pl.col(\"benchmark_close\").last().alias(\"bench_price\"))\n            .sort(\"ts_minute_bench\")\n        )\n        df_bench_now = lf_bench_now.collect().with_columns(\n            (pl.col(\"ts_minute_bench\") + pl.duration(minutes=time_window_minutes)).alias(\"ts_future_bench\")\n        )\n\n        df_bench_future = df_bench_now.select([\n            pl.col(\"ts_minute_bench\").alias(\"ts_minute_bench_future\"),\n            pl.col(\"bench_price\").alias(\"bench_price_future\"),\n        ]).sort(\"ts_minute_bench_future\")\n\n        ldf_bench_now = df_bench_now.lazy().sort(\"ts_minute_bench\")\n        ldf_bench_future = df_bench_future.lazy()\n\n        ldf_bench_joined = ldf_bench_now.join_asof(\n            ldf_bench_future,\n            left_on=\"ts_future_bench\",\n            right_on=\"ts_minute_bench_future\",\n            strategy=\"forward\",\n            suffix=\"_r\"\n        )\n\n        df_bench_joined = ldf_bench_joined.collect().with_columns([\n            pl.when(\n                (pl.col(\"bench_price_future\").is_not_null())\n                & (pl.col(\"bench_price_future\") &gt; 0)\n                & (pl.col(\"bench_price\") &gt; 0)\n            )\n            .then(\n                (pl.col(\"bench_price_future\") - pl.col(\"bench_price\")) / pl.col(\"bench_price\")\n            )\n            .otherwise(None)\n            .alias(\"bench_pct_change\")\n        ])\n\n        df_final_join = (\n            df_joined.lazy()\n            .join(\n                df_bench_joined.select([\"ts_minute_bench\", \"bench_pct_change\"]).lazy(),\n                left_on=\"ts_minute\",\n                right_on=\"ts_minute_bench\",\n                how=\"left\"\n            )\n            .collect()\n        )\n\n        df_lagging_ = df_final_join.with_columns([\n            pl.when(\n                (pl.col(\"bench_pct_change\").abs() &gt; lag_threshold)\n                & (pl.col(\"validator_pct_change\").abs() &lt; lag_threshold)\n            )\n            .then(pl.lit(\"Lagging data vs. real market\"))\n            .otherwise(pl.lit(\"\"))\n            .alias(\"lag_reason\")\n        ]).filter(pl.col(\"lag_reason\") != \"\")\n\n        if not df_lagging_.is_empty():\n            df_lagging_ = df_lagging_.select([\n                pl.col(\"Validator Address\"),\n                pl.lit(base_label).alias(\"pair_label\"),\n                pl.col(\"ts_minute\").alias(\"window_start\"),\n                pl.col(\"price_now\"),\n                pl.col(\"price_decimal_future\").alias(\"price_future\"),\n                pl.col(\"validator_pct_change\"),\n                pl.col(\"bench_pct_change\"),\n                pl.col(\"lag_reason\"),\n            ])\n            suspicious_frames.append(df_lagging_)\n\n    if suspicious_frames:\n        return pl.concat(suspicious_frames, how=\"vertical\")\n    else:\n        return pl.DataFrame(\n            {\n                \"Validator Address\": [],\n                \"pair_label\": [],\n                \"window_start\": [],\n                \"price_now\": [],\n                \"price_future\": [],\n                \"validator_pct_change\": [],\n                \"bench_pct_change\": [],\n                \"lag_reason\": [],\n            }\n        )\n\n\ndef analyze_stale_lagging_data(\n    submission_glob: str,\n    fx_pairs: List[str],\n    autonity_pairs: List[str],\n    yahoo_data_dict: Dict[str, pl.DataFrame],\n    max_consecutive_threshold: int = 30,\n    lag_threshold: float = 0.05,\n    lag_window_minutes: int = 60,\n):\n    \"\"\"\n    Main analysis function.\n    \"\"\"\n    df_all = load_and_preprocess_submissions(submission_glob)\n\n    price_cols_all = fx_pairs + autonity_pairs\n    df_stale = detect_stale_data(df_all, price_cols_all, max_consecutive_threshold)\n\n    df_lagging = detect_lagging_data(\n        df_oracle=df_all,\n        fx_benchmarks=yahoo_data_dict,\n        fx_pairs=fx_pairs,\n        lag_threshold=lag_threshold,\n        time_window_minutes=lag_window_minutes,\n    )\n\n    return {\n        \"df_all_data\": df_all,\n        \"df_stale\": df_stale,\n        \"df_lagging\": df_lagging,\n    }\n\n\nfx_price_cols = [\n    \"AUD-USD Price\",\n    \"CAD-USD Price\",\n    \"EUR-USD Price\",\n    \"GBP-USD Price\",\n    \"JPY-USD Price\",\n    \"SEK-USD Price\",\n]\nautonity_price_cols = [\n    \"ATN-USD Price\",\n    \"NTN-USD Price\",\n    \"NTN-ATN Price\",\n]\n\nyahoo_data = load_all_fx_benchmarks()\n\nresults = analyze_stale_lagging_data(\n    submission_glob=\"../submission-data/Oracle_Submission_*.csv\",\n    fx_pairs=fx_price_cols,\n    autonity_pairs=autonity_price_cols,\n    yahoo_data_dict=yahoo_data,\n    max_consecutive_threshold=30,\n    lag_threshold=0.05,\n    lag_window_minutes=60,\n)\n\n\n\n\n\nThe following cells summarize the results obtained dynamically from the analysis above.\n\n\nIdentify validators that repeatedly submit the same price data beyond the threshold.\n\ndf_stale = results[\"df_stale\"]\n\nnum_stale = df_stale.height\nprint(f\"Total stale data runs detected: {num_stale}\")\n\nif num_stale &gt; 0:\n    display(df_stale.sort(\"run_length\", descending=True))\nelse:\n    print(\"No stale data runs exceeding threshold were detected.\")\n\nTotal stale data runs detected: 1966\n\n\n\nshape: (1_966, 6)\n\n\n\nValidator Address\nprice_col\nrepeated_value\nstart_timestamp\nend_timestamp\nrun_length\n\n\nstr\nstr\nf64\ndatetime[μs, UTC]\ndatetime[μs, UTC]\ni64\n\n\n\n\n\"0x94d28f08Ff81A80f4716C0a8EfC6…\n\"JPY-USD Price\"\n0.006361\n2025-01-01 00:03:42 UTC\n2025-01-01 23:59:44 UTC\n2873\n\n\n\"0x1Be7f70BCf8393a7e4A5BcC66F6f…\n\"AUD-USD Price\"\n0.61885\n2025-01-01 00:08:42 UTC\n2025-01-01 23:59:44 UTC\n2863\n\n\n\"0x1Be7f70BCf8393a7e4A5BcC66F6f…\n\"CAD-USD Price\"\n0.695386\n2025-01-01 00:08:42 UTC\n2025-01-01 23:59:44 UTC\n2863\n\n\n\"0x1Be7f70BCf8393a7e4A5BcC66F6f…\n\"EUR-USD Price\"\n1.035626\n2025-01-01 00:08:42 UTC\n2025-01-01 23:59:44 UTC\n2863\n\n\n\"0x1Be7f70BCf8393a7e4A5BcC66F6f…\n\"GBP-USD Price\"\n1.250801\n2025-01-01 00:08:42 UTC\n2025-01-01 23:59:44 UTC\n2863\n\n\n…\n…\n…\n…\n…\n…\n\n\n\"0xfD97FB8835d25740A2Da27c69762…\n\"CAD-USD Price\"\n0.695314\n2025-01-01 00:06:12 UTC\n2025-01-01 00:20:42 UTC\n30\n\n\n\"0xfD97FB8835d25740A2Da27c69762…\n\"EUR-USD Price\"\n1.035626\n2025-01-01 00:06:12 UTC\n2025-01-01 00:20:42 UTC\n30\n\n\n\"0xfD97FB8835d25740A2Da27c69762…\n\"GBP-USD Price\"\n1.25159\n2025-01-01 00:06:12 UTC\n2025-01-01 00:20:42 UTC\n30\n\n\n\"0xfD97FB8835d25740A2Da27c69762…\n\"JPY-USD Price\"\n0.006361\n2025-01-01 00:06:12 UTC\n2025-01-01 00:20:42 UTC\n30\n\n\n\"0xfD97FB8835d25740A2Da27c69762…\n\"SEK-USD Price\"\n0.090369\n2025-01-01 00:06:12 UTC\n2025-01-01 00:20:42 UTC\n30\n\n\n\n\n\n\nInterpretation:\n\nHigh counts or long durations suggest systematic feed issues or stalled updates.\nValidators frequently appearing here may need urgent investigation.\n\n\n\n\nDetect intervals where the validator’s price fails to reflect significant market movements (≥5% within 60 minutes):\n\ndf_lagging = results[\"df_lagging\"]\n\nnum_lagging = df_lagging.height\nprint(f\"Total lagging data intervals detected: {num_lagging}\")\n\nif num_lagging &gt; 0:\n    df_top_lagging = (\n        df_lagging\n        .with_columns([\n            pl.col(\"bench_pct_change\").cast(pl.Float64),\n            pl.col(\"validator_pct_change\").cast(pl.Float64),\n        ])\n        .with_columns([\n            (pl.col(\"bench_pct_change\") - pl.col(\"validator_pct_change\")).abs().alias(\"abs_diff\")\n        ])\n        .sort(\"abs_diff\", descending=True)\n    )\n    display(df_top_lagging)\nelse:\n    print(\"No lagging data intervals exceeding threshold were detected.\")\n\nTotal lagging data intervals detected: 0\nNo lagging data intervals exceeding threshold were detected.\n\n\nInterpretation:\n\nHigh differences indicate significant mismatches, suggesting disconnections or feed issues.\nFrequent occurrences for specific validators or currency pairs indicate persistent issues.\n\n\n\n\nThe tables and statistics above directly highlight:\n\nValidators with stale or lagging data: Indicating possible systemic issues or node misconfigurations.\nAffected currency pairs: Useful for pinpointing feed-related problems.\n\n\nif num_stale &gt; 0:\n    top_stale_validators = df_stale.group_by(\"Validator Address\").agg(\n        pl.sum(\"run_length\").alias(\"total_stale_intervals\"),\n        pl.count().alias(\"num_stale_runs\")\n    ).sort(\"total_stale_intervals\", descending=True)\n    print(\"Top validators by total stale intervals:\")\n    display(top_stale_validators)\nelse:\n    print(\"No stale data to summarize.\")\n\nif num_lagging &gt; 0:\n    top_lagging_validators = df_lagging.group_by(\"Validator Address\").count().sort(\"count\", descending=True)\n    print(\"Top validators by number of lagging intervals:\")\n    display(top_lagging_validators)\nelse:\n    print(\"No lagging data to summarize.\")\n\nTop validators by total stale intervals:\n\n\n\nshape: (55, 3)\n\n\n\nValidator Address\ntotal_stale_intervals\nnum_stale_runs\n\n\nstr\ni64\nu32\n\n\n\n\n\"0x36142A4f36974e2935192A1111C3…\n17280\n21\n\n\n\"0xB5d8be2AB4b6d7E6be7Ea28E91b3…\n17280\n12\n\n\n\"0xDCA5DFF3D42f2db3C18dBE823380…\n17280\n12\n\n\n\"0x3597d2D42f8Fbbc82E8b10460487…\n17280\n12\n\n\n\"0xBBf36374eb23968F25aecAEbb97B…\n17280\n12\n\n\n…\n…\n…\n\n\n\"0xDF2D0052ea56A860443039619f6D…\n15935\n34\n\n\n\"0xE9FFF86CAdC3136b3D94948B8Fd2…\n15873\n59\n\n\n\"0x551f3300FCFE0e392178b3542c00…\n15400\n8\n\n\n\"0x22A76e194A49c9e5508Cd4A3E1cD…\n15356\n8\n\n\n\"0x1476A65D7B5739dE1805d5130441…\n13581\n64\n\n\n\n\n\n\nNo lagging data to summarize.\n\n\nList of all Validators and their Stale Scores\n\ndf_all = results[\"df_all_data\"]\ndf_stale = results[\"df_stale\"]\n\ndf_totals = (\n    df_all\n    .group_by(\"Validator Address\")\n    .agg(pl.count().alias(\"total_submissions\"))\n    .filter(pl.col(\"Validator Address\").is_not_null())\n)\n\ndf_stale_sum = (\n    df_stale\n    .group_by(\"Validator Address\")\n    .agg(pl.col(\"run_length\").sum().alias(\"sum_stale_intervals\"))\n)\n\ndf_scores = (\n    df_totals\n    .join(df_stale_sum, on=\"Validator Address\", how=\"left\")\n    .fill_null(0)\n    .with_columns(\n        (pl.col(\"sum_stale_intervals\") / pl.col(\"total_submissions\")).alias(\"stale_score\")\n    )\n    .sort(\"stale_score\", descending=True)\n)\n\nfor row in df_scores.to_dicts():\n    print(\n        f\"Validator {row['Validator Address']}: \"\n        f\"total={row['total_submissions']}, \"\n        f\"sum_stale_intervals={row['sum_stale_intervals']}, \"\n        f\"stale_score={row['stale_score']:.1f}\"\n    )\n\nValidator 0xBBf36374eb23968F25aecAEbb97BF3118f3c2fEC: total=2880, sum_stale_intervals=17280, stale_score=6.0\nValidator 0x94470A842Ea4f44e668EB9C2AB81367b6Ce01772: total=2880, sum_stale_intervals=17280, stale_score=6.0\nValidator 0x527192F3D2408C84087607b7feE1d0f907821E17: total=2880, sum_stale_intervals=17280, stale_score=6.0\nValidator 0xF9B38D02959379d43C764064dE201324d5e12931: total=2880, sum_stale_intervals=17280, stale_score=6.0\nValidator 0x791A7F840ac11841cCB0FaA968B2e3a0Db930fCe: total=2880, sum_stale_intervals=17280, stale_score=6.0\nValidator 0xDCA5DFF3D42f2db3C18dBE823380A0A81db49A7E: total=2880, sum_stale_intervals=17280, stale_score=6.0\nValidator 0x36142A4f36974e2935192A1111C39330aA296D3C: total=2880, sum_stale_intervals=17280, stale_score=6.0\nValidator 0x383A3c437d3F12f60E5fC990119468D3561EfBfc: total=2880, sum_stale_intervals=17280, stale_score=6.0\nValidator 0x9d28e40E9Ec4789f9A0D17e421F76D8D0868EA44: total=2880, sum_stale_intervals=17280, stale_score=6.0\nValidator 0xB5d8be2AB4b6d7E6be7Ea28E91b370223a06289f: total=2880, sum_stale_intervals=17280, stale_score=6.0\nValidator 0x358488a4EdCA493FCD87610dcd50c62c8A3Dd658: total=2880, sum_stale_intervals=17280, stale_score=6.0\nValidator 0x3597d2D42f8Fbbc82E8b1046048773aD6DDB717E: total=2880, sum_stale_intervals=17280, stale_score=6.0\nValidator 0xEf0Ba5e345C2C3937df5667A870Aae5105CAa3a5: total=2879, sum_stale_intervals=17268, stale_score=6.0\nValidator 0x8f91e0ADF8065C3fFF92297267E02DF32C2978FF: total=2879, sum_stale_intervals=17268, stale_score=6.0\nValidator 0x718361fc3637199F24a2437331677D6B89a40519: total=2880, sum_stale_intervals=17263, stale_score=6.0\nValidator 0x5603caFE3313D0cf56Fd4bE4A2f606dD6E43F8Eb: total=2880, sum_stale_intervals=17250, stale_score=6.0\nValidator 0x9C7dAABb5101623340C925CFD6fF74088ff5672e: total=2880, sum_stale_intervals=17250, stale_score=6.0\nValidator 0x984A46Ec685Bb41A7BBb2bc39f80C78410ff4057: total=2877, sum_stale_intervals=17232, stale_score=6.0\nValidator 0x197B2c44b887c4aC01243BDE7E4bBa8bd95BC3a8: total=2880, sum_stale_intervals=17245, stale_score=6.0\nValidator 0x6a395dE946c0493157404E2b1947493c633f569E: total=2874, sum_stale_intervals=17208, stale_score=6.0\nValidator 0x94d28f08Ff81A80f4716C0a8EfC6CAC2Ec74d09E: total=2880, sum_stale_intervals=17238, stale_score=6.0\nValidator 0xd61a48b0e11B0Dc6b7Bd713B1012563c52591BAA: total=2873, sum_stale_intervals=17196, stale_score=6.0\nValidator 0x00a96aaED75015Bb44cED878D927dcb15ec1FF54: total=2866, sum_stale_intervals=17136, stale_score=6.0\nValidator 0xBE287C82A786218E008FF97320b08244BE4A282c: total=2879, sum_stale_intervals=17202, stale_score=6.0\nValidator 0x5E17e837DcBa2728C94f95c38fA8a47CB9C8818F: total=2876, sum_stale_intervals=17178, stale_score=6.0\nValidator 0x99E2B4B27BDe92b42D04B6CF302cF564D2C13b74: total=2880, sum_stale_intervals=17196, stale_score=6.0\nValidator 0x8584A78A9b94f332A34BBf24D2AF83367Da31894: total=2879, sum_stale_intervals=17190, stale_score=6.0\nValidator 0x7232e75a8bFd8c9ab002BB3A00eAa885BC72A6dd: total=2877, sum_stale_intervals=17178, stale_score=6.0\nValidator 0x3AaF7817618728ffEF81898E11A3171C33faAE41: total=2874, sum_stale_intervals=17148, stale_score=6.0\nValidator 0xcf716b3930d7cf6f2ADAD90A27c39fDc9D643BBd: total=2880, sum_stale_intervals=17181, stale_score=6.0\nValidator 0x1Be7f70BCf8393a7e4A5BcC66F6f15d6e35cfBBC: total=2880, sum_stale_intervals=17178, stale_score=6.0\nValidator 0x831B837C3DA1B6c2AB68a690206bDfF368877E19: total=2863, sum_stale_intervals=17076, stale_score=6.0\nValidator 0x23b4Be9536F93b8D550214912fD0e38417Ff7209: total=2880, sum_stale_intervals=17154, stale_score=6.0\nValidator 0x24915749B793375a8C93090AF19928aFF1CAEcb6: total=2880, sum_stale_intervals=17154, stale_score=6.0\nValidator 0xE4686A4C6E63A8ab51B458c52EB779AEcf0B74f7: total=2880, sum_stale_intervals=17124, stale_score=5.9\nValidator 0x59031767f20EA8F4a3d90d33aB0DAA2ca469Fd9a: total=2880, sum_stale_intervals=17124, stale_score=5.9\nValidator 0xcdEed21b471b0Dc54faF74480A0E700fCc42a7b6: total=2880, sum_stale_intervals=17090, stale_score=5.9\nValidator 0x2928FE5b911BCAf837cAd93eB9626E86a189f1dd: total=2829, sum_stale_intervals=16710, stale_score=5.9\nValidator 0x6747c02DE7eb2099265e55715Ba2E03e8563D051: total=2840, sum_stale_intervals=16770, stale_score=5.9\nValidator 0xf10f56Bf0A28E0737c7e6bB0aF92f3DDad34aE6a: total=2880, sum_stale_intervals=16997, stale_score=5.9\nValidator 0xC1F9acAF1824F6C906b35A0D2584D6E25077C7f5: total=2880, sum_stale_intervals=16957, stale_score=5.9\nValidator 0xfD97FB8835d25740A2Da27c69762D74F6A931858: total=2880, sum_stale_intervals=16951, stale_score=5.9\nValidator 0xbfDcAF35f52F9ef423ac8F2621F9eef8be6dEd17: total=2833, sum_stale_intervals=16632, stale_score=5.9\nValidator 0xf34CD6c09a59d7D3d1a6C3dC231a7834E5615D6A: total=2834, sum_stale_intervals=16623, stale_score=5.9\nValidator 0x01F788E4371a70D579C178Ea7F48E04e8B2CD743: total=2837, sum_stale_intervals=16614, stale_score=5.9\nValidator 0x4cD134001EEF0843B9c69Ba9569d11fDcF4bd495: total=2823, sum_stale_intervals=16518, stale_score=5.9\nValidator 0x64F83c2538A646A550Ad9bEEb63427a377359DEE: total=2880, sum_stale_intervals=16826, stale_score=5.8\nValidator 0xD9fDab408dF7Ae751691BeC2efE3b713ba3f9C36: total=2880, sum_stale_intervals=16781, stale_score=5.8\nValidator 0x19E356ebC20283fc74AF0BA4C179502A1F62fA7B: total=2833, sum_stale_intervals=16448, stale_score=5.8\nValidator 0xc5B9d978715F081E226cb28bADB7Ba4cde5f9775: total=2879, sum_stale_intervals=15975, stale_score=5.5\nValidator 0xDF2D0052ea56A860443039619f6DAe4434bc0Ac4: total=2879, sum_stale_intervals=15935, stale_score=5.5\nValidator 0x1476A65D7B5739dE1805d5130441A94022Ee49fe: total=2462, sum_stale_intervals=13581, stale_score=5.5\nValidator 0xE9FFF86CAdC3136b3D94948B8Fd23631EDaa2dE3: total=2880, sum_stale_intervals=15873, stale_score=5.5\nValidator 0x551f3300FCFE0e392178b3542c009948008B2a9F: total=2880, sum_stale_intervals=15400, stale_score=5.3\nValidator 0x22A76e194A49c9e5508Cd4A3E1cD555D088ECB08: total=2880, sum_stale_intervals=15356, stale_score=5.3\nValidator 0x26E2724dBD14Fbd52be430B97043AA4c83F05852: total=2880, sum_stale_intervals=0, stale_score=0.0\nValidator 0x3fe573552E14a0FC11Da25E43Fef11e16a785068: total=2880, sum_stale_intervals=0, stale_score=0.0\nValidator 0xd625d50B0d087861c286d726eC51Cf4Bd9c54357: total=2856, sum_stale_intervals=0, stale_score=0.0\nValidator 0xdF239e0D5b4E6e820B0cFEF6972A90893c2073AB: total=2880, sum_stale_intervals=0, stale_score=0.0\nValidator 0x100E38f7BCEc53937BDd79ADE46F34362470577B: total=2876, sum_stale_intervals=0, stale_score=0.0\n\n\nPlease note, total represents the total number of submissions for this validator. sum_stale_intervals sums all “stale” runs across each price column. For instance, if a validator has several columns remain identical for 30+ consecutive intervals, each column’s run is added. stale_score = sum_stale_intervals / total, which can exceed 1 because a single row (submission) may contribute to multiple stale runs (one per column).",
    "crumbs": [
      "Notebooks",
      "Issue 4"
    ]
  },
  {
    "objectID": "notebooks/issue_4.html#stale-lagging-data",
    "href": "notebooks/issue_4.html#stale-lagging-data",
    "title": "Issue 4",
    "section": "",
    "text": "This notebook documents the analysis for Issue #4: Stale / Lagging Data in the Autonity Oracle data. It covers:\n\nWhat is this issue about?\n\nWhy conduct this issue analysis?\n\nHow to conduct this issue analysis?\n\nWhat are the results?\n\n\n\n\nIn the Oracle system, validators submit price data that must reflect real-world market movements. However, issues may occur:\n\nStale data: Validator submits identical prices repeatedly for prolonged periods.\nLagging data: Validator’s reported price remains nearly unchanged despite significant market changes.\n\nThese indicate problems such as disconnected feeds or outdated caches.\n\n\n\n\n\nAccuracy: Ensuring data freshness and reliability.\nTroubleshooting: Detect potential API disconnections, stuck feeds, or caching issues.\nConfidence: Critical for Mainnet readiness.\n\n\n\n\n\nUse Python with the Polars library (v1.24.0) to:\n\nLoading Oracle submission data and Yahoo Finance benchmarks.\nDetecting stale data (repeated identical submissions ≥30 consecutive intervals).\nDetecting lagging data (market moves significantly, validator’s submission barely changes within 60-minute windows).\n\nBelow is the Python script to perform the analysis:\n\nimport polars as pl\nimport glob\nfrom typing import List, Dict\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndef load_and_preprocess_submissions(submission_glob: str) -&gt; pl.DataFrame:\n    \"\"\"\n    Loads Oracle Submission CSVs and returns a Polars DataFrame.\n    \"\"\"\n    files = sorted(glob.glob(submission_glob))\n    if not files:\n        raise ValueError(f\"No CSV files found matching pattern {submission_glob}\")\n    lf_list = []\n    for f in files:\n        lf_temp = pl.scan_csv(\n            f,\n            dtypes={\"Timestamp\": pl.Utf8},\n            null_values=[\"\"],\n            ignore_errors=True,\n        )\n        lf_list.append(lf_temp)\n\n    lf = pl.concat(lf_list)\n    lf = lf.with_columns(\n        pl.col(\"Timestamp\").str.strptime(pl.Datetime, strict=False).alias(\"Timestamp_dt\")\n    )\n    lf = lf.with_columns(\n        [\n            pl.col(\"Timestamp_dt\").cast(pl.Date).alias(\"date_only\"),\n            pl.col(\"Timestamp_dt\").dt.weekday().alias(\"weekday_num\"),\n        ]\n    )\n    return lf.collect()\n\n\ndef load_yahoo_finance_data(directory_pattern: str, pair_label: str) -&gt; pl.DataFrame:\n    \"\"\"\n    Loads Yahoo Finance CSVs and returns a Polars DataFrame.\n    \"\"\"\n    files = sorted(glob.glob(directory_pattern))\n    if not files:\n        raise ValueError(f\"No Yahoo Finance CSV files found: {directory_pattern}\")\n\n    lf_list = []\n    for f in files:\n        lf_temp = pl.scan_csv(\n            f,\n            has_header=False,\n            skip_rows=3,\n            new_columns=[\"Datetime\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\"],\n            try_parse_dates=True,\n        )\n        lf_list.append(lf_temp)\n\n    lf = pl.concat(lf_list).sort(\"Datetime\").select(\n        [\n            pl.col(\"Datetime\").alias(\"timestamp_benchmark\"),\n            pl.col(\"Close\").alias(\"benchmark_close\"),\n        ]\n    )\n    df = lf.collect().with_columns(pl.lit(pair_label).alias(\"symbol\"))\n    return df\n\n\ndef load_all_fx_benchmarks() -&gt; Dict[str, pl.DataFrame]:\n    \"\"\"\n    Loads FX data from Yahoo Finance.\n    \"\"\"\n    mapping = {\n        \"AUD-USD\": \"../yahoo-finance/data/AUDUSD/AUDUSD=X_1m_*.csv\",\n        \"CAD-USD\": \"../yahoo-finance/data/CADUSD/CADUSD=X_1m_*.csv\",\n        \"EUR-USD\": \"../yahoo-finance/data/EURUSD/EURUSD=X_1m_*.csv\",\n        \"GBP-USD\": \"../yahoo-finance/data/GBPUSD/GBPUSD=X_1m_*.csv\",\n        \"JPY-USD\": \"../yahoo-finance/data/JPYUSD/JPYUSD=X_1m_*.csv\",\n        \"SEK-USD\": \"../yahoo-finance/data/SEKUSD/SEKUSD=X_1m_*.csv\",\n    }\n    result = {}\n    for pair_label, pattern in mapping.items():\n        df_pair = load_yahoo_finance_data(pattern, pair_label)\n        result[pair_label] = df_pair\n    return result\n\n\ndef detect_stale_data(\n    df: pl.DataFrame,\n    price_cols: List[str],\n    max_consecutive_threshold: int = 30,\n    stale_tolerance: float = 1e-9  # Tolerance for float comparison\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Identifies potential stale data when the same price is repeated for\n    at least max_consecutive_threshold intervals, allowing small float tolerance.\n    Skips any rows with None in the relevant price columns to avoid TypeError.\n    \"\"\"\n    suspicious_frames = []\n    df_local = df.clone()\n    \n    new_cols = []\n    for pc in price_cols:\n        dec_col = pc.replace(\" Price\", \" Price Decimal\")\n        new_cols.append((pl.col(pc).cast(pl.Float64) / 1e18).alias(dec_col))\n    df_local = df_local.with_columns(new_cols)\n    \n    for pc in price_cols:\n        dec_col = pc.replace(\" Price\", \" Price Decimal\")\n        if dec_col in df_local.columns:\n            df_local = df_local.filter(pl.col(dec_col).is_not_null())\n\n    for pc in price_cols:\n        dec_col = pc.replace(\" Price\", \" Price Decimal\")\n        if dec_col not in df_local.columns:\n            continue\n\n        df_sub = (\n            df_local.select([\"Validator Address\", \"Timestamp_dt\", dec_col])\n            .filter(pl.col(\"Validator Address\").is_not_null())\n            .sort([\"Validator Address\", \"Timestamp_dt\"])\n        )\n\n        df_list = df_sub.to_dicts()\n        suspicious_records = []\n        \n        if not df_list:\n            continue\n\n        current_run_price = None\n        current_run_start_idx = 0\n        current_run_len = 0\n        current_validator = None\n\n        def finalize_run(run_val, start_i, end_i, run_len):\n            start_ts = df_list[start_i][\"Timestamp_dt\"]\n            end_ts = df_list[end_i][\"Timestamp_dt\"]\n            vaddr = df_list[start_i][\"Validator Address\"]\n            return {\n                \"Validator Address\": vaddr,\n                \"price_col\": pc,\n                \"repeated_value\": run_val,\n                \"start_timestamp\": start_ts,\n                \"end_timestamp\": end_ts,\n                \"run_length\": run_len,\n            }\n\n        for i, row in enumerate(df_list):\n            vaddr = row[\"Validator Address\"]\n            price_val = row[dec_col]\n            \n            if (current_validator is not None) and (vaddr != current_validator):\n                if current_run_len &gt;= max_consecutive_threshold:\n                    rec = finalize_run(current_run_price, current_run_start_idx, i - 1, current_run_len)\n                    suspicious_records.append(rec)\n                current_run_price = None\n                current_run_start_idx = i\n                current_run_len = 0\n                current_validator = vaddr\n\n            if (\n                current_run_price is not None\n                and vaddr == current_validator\n                and abs(price_val - current_run_price) &lt; stale_tolerance\n            ):\n                current_run_len += 1\n            else:\n                if current_run_len &gt;= max_consecutive_threshold:\n                    rec = finalize_run(current_run_price, current_run_start_idx, i - 1, current_run_len)\n                    suspicious_records.append(rec)\n                \n                current_run_price = price_val\n                current_run_start_idx = i\n                current_run_len = 1\n                current_validator = vaddr\n\n        if current_run_len &gt;= max_consecutive_threshold:\n            rec = finalize_run(current_run_price, current_run_start_idx, len(df_list) - 1, current_run_len)\n            suspicious_records.append(rec)\n\n        if suspicious_records:\n            df_sus = pl.DataFrame(suspicious_records)\n            suspicious_frames.append(df_sus)\n\n    if suspicious_frames:\n        return pl.concat(suspicious_frames, how=\"vertical\")\n    else:\n        return pl.DataFrame(\n            {\n                \"Validator Address\": [],\n                \"price_col\": [],\n                \"repeated_value\": [],\n                \"start_timestamp\": [],\n                \"end_timestamp\": [],\n                \"run_length\": [],\n            }\n        )\n\n\ndef detect_lagging_data(\n    df_oracle: pl.DataFrame,\n    fx_benchmarks: Dict[str, pl.DataFrame],\n    fx_pairs: List[str],\n    lag_threshold: float = 0.05,\n    time_window_minutes: int = 60\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Compare each validator's reported FX price vs. Yahoo's benchmark.\n    Now uses a forward as-of join to find the price 'at or after' (T + time_window_minutes).\n    \"\"\"\n    df_local = df_oracle.clone()\n    for pc in fx_pairs:\n        dec_col = pc.replace(\" Price\", \" Price Decimal\")\n        df_local = df_local.with_columns(\n            (pl.col(pc).cast(pl.Float64) / 1e18).alias(dec_col)\n        )\n\n    suspicious_frames = []\n\n    for pc in fx_pairs:\n        base_label = pc.replace(\" Price\", \"\")\n        dec_col = base_label + \" Price Decimal\"\n        if dec_col not in df_local.columns:\n            continue\n        if base_label not in fx_benchmarks:\n            continue\n\n        df_sub = df_local.select([\"Timestamp_dt\", \"Validator Address\", dec_col]).filter(\n            pl.col(\"Validator Address\").is_not_null()\n        )\n        df_sub = df_sub.with_columns(\n            pl.col(\"Timestamp_dt\").dt.truncate(\"1m\").alias(\"ts_minute\")\n        )\n\n        lf_sub = (\n            df_sub.lazy()\n            .group_by([\"ts_minute\", \"Validator Address\"])\n            .agg(pl.col(dec_col).last().alias(\"price_decimal\"))\n        )\n        df_val_prices = lf_sub.collect().sort([\"Validator Address\", \"ts_minute\"])\n\n        df_val_prices_future = df_val_prices.with_columns(\n            (pl.col(\"ts_minute\") + pl.duration(minutes=time_window_minutes)).alias(\"ts_future\")\n        )\n\n        left_lf = df_val_prices_future.lazy().sort([\"Validator Address\", \"ts_minute\"])\n        right_lf = (\n            df_val_prices_future.lazy()\n            .select([\n                pl.col(\"Validator Address\"),\n                pl.col(\"ts_minute\").alias(\"ts_minute_future\"),\n                pl.col(\"price_decimal\").alias(\"price_decimal_future\"),\n            ])\n            .sort([\"Validator Address\", \"ts_minute_future\"])\n        )\n\n        joined_lf = left_lf.join_asof(\n            right_lf,\n            left_on=\"ts_future\",\n            right_on=\"ts_minute_future\",\n            on=\"Validator Address\",\n            strategy=\"forward\",\n            suffix=\"_r\"\n        )\n\n        df_joined = joined_lf.collect().with_columns(\n            pl.col(\"price_decimal\").alias(\"price_now\")\n        )\n\n        df_joined = df_joined.with_columns(\n            pl.when(\n                (pl.col(\"price_decimal_future\").is_not_null())\n                & (pl.col(\"price_decimal_future\") &gt; 0)\n                & (pl.col(\"price_now\") &gt; 0)\n            )\n            .then((pl.col(\"price_decimal_future\") - pl.col(\"price_now\")) / pl.col(\"price_now\"))\n            .otherwise(None)\n            .alias(\"validator_pct_change\")\n        )\n\n        df_bench = fx_benchmarks[base_label]\n        df_bench = df_bench.with_columns(\n            pl.col(\"timestamp_benchmark\").dt.truncate(\"1m\").alias(\"ts_minute_bench\")\n        ).sort(\"ts_minute_bench\")\n\n        lf_bench_now = (\n            df_bench.lazy()\n            .group_by(\"ts_minute_bench\")\n            .agg(pl.col(\"benchmark_close\").last().alias(\"bench_price\"))\n            .sort(\"ts_minute_bench\")\n        )\n        df_bench_now = lf_bench_now.collect().with_columns(\n            (pl.col(\"ts_minute_bench\") + pl.duration(minutes=time_window_minutes)).alias(\"ts_future_bench\")\n        )\n\n        df_bench_future = df_bench_now.select([\n            pl.col(\"ts_minute_bench\").alias(\"ts_minute_bench_future\"),\n            pl.col(\"bench_price\").alias(\"bench_price_future\"),\n        ]).sort(\"ts_minute_bench_future\")\n\n        ldf_bench_now = df_bench_now.lazy().sort(\"ts_minute_bench\")\n        ldf_bench_future = df_bench_future.lazy()\n\n        ldf_bench_joined = ldf_bench_now.join_asof(\n            ldf_bench_future,\n            left_on=\"ts_future_bench\",\n            right_on=\"ts_minute_bench_future\",\n            strategy=\"forward\",\n            suffix=\"_r\"\n        )\n\n        df_bench_joined = ldf_bench_joined.collect().with_columns([\n            pl.when(\n                (pl.col(\"bench_price_future\").is_not_null())\n                & (pl.col(\"bench_price_future\") &gt; 0)\n                & (pl.col(\"bench_price\") &gt; 0)\n            )\n            .then(\n                (pl.col(\"bench_price_future\") - pl.col(\"bench_price\")) / pl.col(\"bench_price\")\n            )\n            .otherwise(None)\n            .alias(\"bench_pct_change\")\n        ])\n\n        df_final_join = (\n            df_joined.lazy()\n            .join(\n                df_bench_joined.select([\"ts_minute_bench\", \"bench_pct_change\"]).lazy(),\n                left_on=\"ts_minute\",\n                right_on=\"ts_minute_bench\",\n                how=\"left\"\n            )\n            .collect()\n        )\n\n        df_lagging_ = df_final_join.with_columns([\n            pl.when(\n                (pl.col(\"bench_pct_change\").abs() &gt; lag_threshold)\n                & (pl.col(\"validator_pct_change\").abs() &lt; lag_threshold)\n            )\n            .then(pl.lit(\"Lagging data vs. real market\"))\n            .otherwise(pl.lit(\"\"))\n            .alias(\"lag_reason\")\n        ]).filter(pl.col(\"lag_reason\") != \"\")\n\n        if not df_lagging_.is_empty():\n            df_lagging_ = df_lagging_.select([\n                pl.col(\"Validator Address\"),\n                pl.lit(base_label).alias(\"pair_label\"),\n                pl.col(\"ts_minute\").alias(\"window_start\"),\n                pl.col(\"price_now\"),\n                pl.col(\"price_decimal_future\").alias(\"price_future\"),\n                pl.col(\"validator_pct_change\"),\n                pl.col(\"bench_pct_change\"),\n                pl.col(\"lag_reason\"),\n            ])\n            suspicious_frames.append(df_lagging_)\n\n    if suspicious_frames:\n        return pl.concat(suspicious_frames, how=\"vertical\")\n    else:\n        return pl.DataFrame(\n            {\n                \"Validator Address\": [],\n                \"pair_label\": [],\n                \"window_start\": [],\n                \"price_now\": [],\n                \"price_future\": [],\n                \"validator_pct_change\": [],\n                \"bench_pct_change\": [],\n                \"lag_reason\": [],\n            }\n        )\n\n\ndef analyze_stale_lagging_data(\n    submission_glob: str,\n    fx_pairs: List[str],\n    autonity_pairs: List[str],\n    yahoo_data_dict: Dict[str, pl.DataFrame],\n    max_consecutive_threshold: int = 30,\n    lag_threshold: float = 0.05,\n    lag_window_minutes: int = 60,\n):\n    \"\"\"\n    Main analysis function.\n    \"\"\"\n    df_all = load_and_preprocess_submissions(submission_glob)\n\n    price_cols_all = fx_pairs + autonity_pairs\n    df_stale = detect_stale_data(df_all, price_cols_all, max_consecutive_threshold)\n\n    df_lagging = detect_lagging_data(\n        df_oracle=df_all,\n        fx_benchmarks=yahoo_data_dict,\n        fx_pairs=fx_pairs,\n        lag_threshold=lag_threshold,\n        time_window_minutes=lag_window_minutes,\n    )\n\n    return {\n        \"df_all_data\": df_all,\n        \"df_stale\": df_stale,\n        \"df_lagging\": df_lagging,\n    }\n\n\nfx_price_cols = [\n    \"AUD-USD Price\",\n    \"CAD-USD Price\",\n    \"EUR-USD Price\",\n    \"GBP-USD Price\",\n    \"JPY-USD Price\",\n    \"SEK-USD Price\",\n]\nautonity_price_cols = [\n    \"ATN-USD Price\",\n    \"NTN-USD Price\",\n    \"NTN-ATN Price\",\n]\n\nyahoo_data = load_all_fx_benchmarks()\n\nresults = analyze_stale_lagging_data(\n    submission_glob=\"../submission-data/Oracle_Submission_*.csv\",\n    fx_pairs=fx_price_cols,\n    autonity_pairs=autonity_price_cols,\n    yahoo_data_dict=yahoo_data,\n    max_consecutive_threshold=30,\n    lag_threshold=0.05,\n    lag_window_minutes=60,\n)\n\n\n\n\n\nThe following cells summarize the results obtained dynamically from the analysis above.\n\n\nIdentify validators that repeatedly submit the same price data beyond the threshold.\n\ndf_stale = results[\"df_stale\"]\n\nnum_stale = df_stale.height\nprint(f\"Total stale data runs detected: {num_stale}\")\n\nif num_stale &gt; 0:\n    display(df_stale.sort(\"run_length\", descending=True))\nelse:\n    print(\"No stale data runs exceeding threshold were detected.\")\n\nTotal stale data runs detected: 1966\n\n\n\nshape: (1_966, 6)\n\n\n\nValidator Address\nprice_col\nrepeated_value\nstart_timestamp\nend_timestamp\nrun_length\n\n\nstr\nstr\nf64\ndatetime[μs, UTC]\ndatetime[μs, UTC]\ni64\n\n\n\n\n\"0x94d28f08Ff81A80f4716C0a8EfC6…\n\"JPY-USD Price\"\n0.006361\n2025-01-01 00:03:42 UTC\n2025-01-01 23:59:44 UTC\n2873\n\n\n\"0x1Be7f70BCf8393a7e4A5BcC66F6f…\n\"AUD-USD Price\"\n0.61885\n2025-01-01 00:08:42 UTC\n2025-01-01 23:59:44 UTC\n2863\n\n\n\"0x1Be7f70BCf8393a7e4A5BcC66F6f…\n\"CAD-USD Price\"\n0.695386\n2025-01-01 00:08:42 UTC\n2025-01-01 23:59:44 UTC\n2863\n\n\n\"0x1Be7f70BCf8393a7e4A5BcC66F6f…\n\"EUR-USD Price\"\n1.035626\n2025-01-01 00:08:42 UTC\n2025-01-01 23:59:44 UTC\n2863\n\n\n\"0x1Be7f70BCf8393a7e4A5BcC66F6f…\n\"GBP-USD Price\"\n1.250801\n2025-01-01 00:08:42 UTC\n2025-01-01 23:59:44 UTC\n2863\n\n\n…\n…\n…\n…\n…\n…\n\n\n\"0xfD97FB8835d25740A2Da27c69762…\n\"CAD-USD Price\"\n0.695314\n2025-01-01 00:06:12 UTC\n2025-01-01 00:20:42 UTC\n30\n\n\n\"0xfD97FB8835d25740A2Da27c69762…\n\"EUR-USD Price\"\n1.035626\n2025-01-01 00:06:12 UTC\n2025-01-01 00:20:42 UTC\n30\n\n\n\"0xfD97FB8835d25740A2Da27c69762…\n\"GBP-USD Price\"\n1.25159\n2025-01-01 00:06:12 UTC\n2025-01-01 00:20:42 UTC\n30\n\n\n\"0xfD97FB8835d25740A2Da27c69762…\n\"JPY-USD Price\"\n0.006361\n2025-01-01 00:06:12 UTC\n2025-01-01 00:20:42 UTC\n30\n\n\n\"0xfD97FB8835d25740A2Da27c69762…\n\"SEK-USD Price\"\n0.090369\n2025-01-01 00:06:12 UTC\n2025-01-01 00:20:42 UTC\n30\n\n\n\n\n\n\nInterpretation:\n\nHigh counts or long durations suggest systematic feed issues or stalled updates.\nValidators frequently appearing here may need urgent investigation.\n\n\n\n\nDetect intervals where the validator’s price fails to reflect significant market movements (≥5% within 60 minutes):\n\ndf_lagging = results[\"df_lagging\"]\n\nnum_lagging = df_lagging.height\nprint(f\"Total lagging data intervals detected: {num_lagging}\")\n\nif num_lagging &gt; 0:\n    df_top_lagging = (\n        df_lagging\n        .with_columns([\n            pl.col(\"bench_pct_change\").cast(pl.Float64),\n            pl.col(\"validator_pct_change\").cast(pl.Float64),\n        ])\n        .with_columns([\n            (pl.col(\"bench_pct_change\") - pl.col(\"validator_pct_change\")).abs().alias(\"abs_diff\")\n        ])\n        .sort(\"abs_diff\", descending=True)\n    )\n    display(df_top_lagging)\nelse:\n    print(\"No lagging data intervals exceeding threshold were detected.\")\n\nTotal lagging data intervals detected: 0\nNo lagging data intervals exceeding threshold were detected.\n\n\nInterpretation:\n\nHigh differences indicate significant mismatches, suggesting disconnections or feed issues.\nFrequent occurrences for specific validators or currency pairs indicate persistent issues.\n\n\n\n\nThe tables and statistics above directly highlight:\n\nValidators with stale or lagging data: Indicating possible systemic issues or node misconfigurations.\nAffected currency pairs: Useful for pinpointing feed-related problems.\n\n\nif num_stale &gt; 0:\n    top_stale_validators = df_stale.group_by(\"Validator Address\").agg(\n        pl.sum(\"run_length\").alias(\"total_stale_intervals\"),\n        pl.count().alias(\"num_stale_runs\")\n    ).sort(\"total_stale_intervals\", descending=True)\n    print(\"Top validators by total stale intervals:\")\n    display(top_stale_validators)\nelse:\n    print(\"No stale data to summarize.\")\n\nif num_lagging &gt; 0:\n    top_lagging_validators = df_lagging.group_by(\"Validator Address\").count().sort(\"count\", descending=True)\n    print(\"Top validators by number of lagging intervals:\")\n    display(top_lagging_validators)\nelse:\n    print(\"No lagging data to summarize.\")\n\nTop validators by total stale intervals:\n\n\n\nshape: (55, 3)\n\n\n\nValidator Address\ntotal_stale_intervals\nnum_stale_runs\n\n\nstr\ni64\nu32\n\n\n\n\n\"0x36142A4f36974e2935192A1111C3…\n17280\n21\n\n\n\"0xB5d8be2AB4b6d7E6be7Ea28E91b3…\n17280\n12\n\n\n\"0xDCA5DFF3D42f2db3C18dBE823380…\n17280\n12\n\n\n\"0x3597d2D42f8Fbbc82E8b10460487…\n17280\n12\n\n\n\"0xBBf36374eb23968F25aecAEbb97B…\n17280\n12\n\n\n…\n…\n…\n\n\n\"0xDF2D0052ea56A860443039619f6D…\n15935\n34\n\n\n\"0xE9FFF86CAdC3136b3D94948B8Fd2…\n15873\n59\n\n\n\"0x551f3300FCFE0e392178b3542c00…\n15400\n8\n\n\n\"0x22A76e194A49c9e5508Cd4A3E1cD…\n15356\n8\n\n\n\"0x1476A65D7B5739dE1805d5130441…\n13581\n64\n\n\n\n\n\n\nNo lagging data to summarize.\n\n\nList of all Validators and their Stale Scores\n\ndf_all = results[\"df_all_data\"]\ndf_stale = results[\"df_stale\"]\n\ndf_totals = (\n    df_all\n    .group_by(\"Validator Address\")\n    .agg(pl.count().alias(\"total_submissions\"))\n    .filter(pl.col(\"Validator Address\").is_not_null())\n)\n\ndf_stale_sum = (\n    df_stale\n    .group_by(\"Validator Address\")\n    .agg(pl.col(\"run_length\").sum().alias(\"sum_stale_intervals\"))\n)\n\ndf_scores = (\n    df_totals\n    .join(df_stale_sum, on=\"Validator Address\", how=\"left\")\n    .fill_null(0)\n    .with_columns(\n        (pl.col(\"sum_stale_intervals\") / pl.col(\"total_submissions\")).alias(\"stale_score\")\n    )\n    .sort(\"stale_score\", descending=True)\n)\n\nfor row in df_scores.to_dicts():\n    print(\n        f\"Validator {row['Validator Address']}: \"\n        f\"total={row['total_submissions']}, \"\n        f\"sum_stale_intervals={row['sum_stale_intervals']}, \"\n        f\"stale_score={row['stale_score']:.1f}\"\n    )\n\nValidator 0xBBf36374eb23968F25aecAEbb97BF3118f3c2fEC: total=2880, sum_stale_intervals=17280, stale_score=6.0\nValidator 0x94470A842Ea4f44e668EB9C2AB81367b6Ce01772: total=2880, sum_stale_intervals=17280, stale_score=6.0\nValidator 0x527192F3D2408C84087607b7feE1d0f907821E17: total=2880, sum_stale_intervals=17280, stale_score=6.0\nValidator 0xF9B38D02959379d43C764064dE201324d5e12931: total=2880, sum_stale_intervals=17280, stale_score=6.0\nValidator 0x791A7F840ac11841cCB0FaA968B2e3a0Db930fCe: total=2880, sum_stale_intervals=17280, stale_score=6.0\nValidator 0xDCA5DFF3D42f2db3C18dBE823380A0A81db49A7E: total=2880, sum_stale_intervals=17280, stale_score=6.0\nValidator 0x36142A4f36974e2935192A1111C39330aA296D3C: total=2880, sum_stale_intervals=17280, stale_score=6.0\nValidator 0x383A3c437d3F12f60E5fC990119468D3561EfBfc: total=2880, sum_stale_intervals=17280, stale_score=6.0\nValidator 0x9d28e40E9Ec4789f9A0D17e421F76D8D0868EA44: total=2880, sum_stale_intervals=17280, stale_score=6.0\nValidator 0xB5d8be2AB4b6d7E6be7Ea28E91b370223a06289f: total=2880, sum_stale_intervals=17280, stale_score=6.0\nValidator 0x358488a4EdCA493FCD87610dcd50c62c8A3Dd658: total=2880, sum_stale_intervals=17280, stale_score=6.0\nValidator 0x3597d2D42f8Fbbc82E8b1046048773aD6DDB717E: total=2880, sum_stale_intervals=17280, stale_score=6.0\nValidator 0xEf0Ba5e345C2C3937df5667A870Aae5105CAa3a5: total=2879, sum_stale_intervals=17268, stale_score=6.0\nValidator 0x8f91e0ADF8065C3fFF92297267E02DF32C2978FF: total=2879, sum_stale_intervals=17268, stale_score=6.0\nValidator 0x718361fc3637199F24a2437331677D6B89a40519: total=2880, sum_stale_intervals=17263, stale_score=6.0\nValidator 0x5603caFE3313D0cf56Fd4bE4A2f606dD6E43F8Eb: total=2880, sum_stale_intervals=17250, stale_score=6.0\nValidator 0x9C7dAABb5101623340C925CFD6fF74088ff5672e: total=2880, sum_stale_intervals=17250, stale_score=6.0\nValidator 0x984A46Ec685Bb41A7BBb2bc39f80C78410ff4057: total=2877, sum_stale_intervals=17232, stale_score=6.0\nValidator 0x197B2c44b887c4aC01243BDE7E4bBa8bd95BC3a8: total=2880, sum_stale_intervals=17245, stale_score=6.0\nValidator 0x6a395dE946c0493157404E2b1947493c633f569E: total=2874, sum_stale_intervals=17208, stale_score=6.0\nValidator 0x94d28f08Ff81A80f4716C0a8EfC6CAC2Ec74d09E: total=2880, sum_stale_intervals=17238, stale_score=6.0\nValidator 0xd61a48b0e11B0Dc6b7Bd713B1012563c52591BAA: total=2873, sum_stale_intervals=17196, stale_score=6.0\nValidator 0x00a96aaED75015Bb44cED878D927dcb15ec1FF54: total=2866, sum_stale_intervals=17136, stale_score=6.0\nValidator 0xBE287C82A786218E008FF97320b08244BE4A282c: total=2879, sum_stale_intervals=17202, stale_score=6.0\nValidator 0x5E17e837DcBa2728C94f95c38fA8a47CB9C8818F: total=2876, sum_stale_intervals=17178, stale_score=6.0\nValidator 0x99E2B4B27BDe92b42D04B6CF302cF564D2C13b74: total=2880, sum_stale_intervals=17196, stale_score=6.0\nValidator 0x8584A78A9b94f332A34BBf24D2AF83367Da31894: total=2879, sum_stale_intervals=17190, stale_score=6.0\nValidator 0x7232e75a8bFd8c9ab002BB3A00eAa885BC72A6dd: total=2877, sum_stale_intervals=17178, stale_score=6.0\nValidator 0x3AaF7817618728ffEF81898E11A3171C33faAE41: total=2874, sum_stale_intervals=17148, stale_score=6.0\nValidator 0xcf716b3930d7cf6f2ADAD90A27c39fDc9D643BBd: total=2880, sum_stale_intervals=17181, stale_score=6.0\nValidator 0x1Be7f70BCf8393a7e4A5BcC66F6f15d6e35cfBBC: total=2880, sum_stale_intervals=17178, stale_score=6.0\nValidator 0x831B837C3DA1B6c2AB68a690206bDfF368877E19: total=2863, sum_stale_intervals=17076, stale_score=6.0\nValidator 0x23b4Be9536F93b8D550214912fD0e38417Ff7209: total=2880, sum_stale_intervals=17154, stale_score=6.0\nValidator 0x24915749B793375a8C93090AF19928aFF1CAEcb6: total=2880, sum_stale_intervals=17154, stale_score=6.0\nValidator 0xE4686A4C6E63A8ab51B458c52EB779AEcf0B74f7: total=2880, sum_stale_intervals=17124, stale_score=5.9\nValidator 0x59031767f20EA8F4a3d90d33aB0DAA2ca469Fd9a: total=2880, sum_stale_intervals=17124, stale_score=5.9\nValidator 0xcdEed21b471b0Dc54faF74480A0E700fCc42a7b6: total=2880, sum_stale_intervals=17090, stale_score=5.9\nValidator 0x2928FE5b911BCAf837cAd93eB9626E86a189f1dd: total=2829, sum_stale_intervals=16710, stale_score=5.9\nValidator 0x6747c02DE7eb2099265e55715Ba2E03e8563D051: total=2840, sum_stale_intervals=16770, stale_score=5.9\nValidator 0xf10f56Bf0A28E0737c7e6bB0aF92f3DDad34aE6a: total=2880, sum_stale_intervals=16997, stale_score=5.9\nValidator 0xC1F9acAF1824F6C906b35A0D2584D6E25077C7f5: total=2880, sum_stale_intervals=16957, stale_score=5.9\nValidator 0xfD97FB8835d25740A2Da27c69762D74F6A931858: total=2880, sum_stale_intervals=16951, stale_score=5.9\nValidator 0xbfDcAF35f52F9ef423ac8F2621F9eef8be6dEd17: total=2833, sum_stale_intervals=16632, stale_score=5.9\nValidator 0xf34CD6c09a59d7D3d1a6C3dC231a7834E5615D6A: total=2834, sum_stale_intervals=16623, stale_score=5.9\nValidator 0x01F788E4371a70D579C178Ea7F48E04e8B2CD743: total=2837, sum_stale_intervals=16614, stale_score=5.9\nValidator 0x4cD134001EEF0843B9c69Ba9569d11fDcF4bd495: total=2823, sum_stale_intervals=16518, stale_score=5.9\nValidator 0x64F83c2538A646A550Ad9bEEb63427a377359DEE: total=2880, sum_stale_intervals=16826, stale_score=5.8\nValidator 0xD9fDab408dF7Ae751691BeC2efE3b713ba3f9C36: total=2880, sum_stale_intervals=16781, stale_score=5.8\nValidator 0x19E356ebC20283fc74AF0BA4C179502A1F62fA7B: total=2833, sum_stale_intervals=16448, stale_score=5.8\nValidator 0xc5B9d978715F081E226cb28bADB7Ba4cde5f9775: total=2879, sum_stale_intervals=15975, stale_score=5.5\nValidator 0xDF2D0052ea56A860443039619f6DAe4434bc0Ac4: total=2879, sum_stale_intervals=15935, stale_score=5.5\nValidator 0x1476A65D7B5739dE1805d5130441A94022Ee49fe: total=2462, sum_stale_intervals=13581, stale_score=5.5\nValidator 0xE9FFF86CAdC3136b3D94948B8Fd23631EDaa2dE3: total=2880, sum_stale_intervals=15873, stale_score=5.5\nValidator 0x551f3300FCFE0e392178b3542c009948008B2a9F: total=2880, sum_stale_intervals=15400, stale_score=5.3\nValidator 0x22A76e194A49c9e5508Cd4A3E1cD555D088ECB08: total=2880, sum_stale_intervals=15356, stale_score=5.3\nValidator 0x26E2724dBD14Fbd52be430B97043AA4c83F05852: total=2880, sum_stale_intervals=0, stale_score=0.0\nValidator 0x3fe573552E14a0FC11Da25E43Fef11e16a785068: total=2880, sum_stale_intervals=0, stale_score=0.0\nValidator 0xd625d50B0d087861c286d726eC51Cf4Bd9c54357: total=2856, sum_stale_intervals=0, stale_score=0.0\nValidator 0xdF239e0D5b4E6e820B0cFEF6972A90893c2073AB: total=2880, sum_stale_intervals=0, stale_score=0.0\nValidator 0x100E38f7BCEc53937BDd79ADE46F34362470577B: total=2876, sum_stale_intervals=0, stale_score=0.0\n\n\nPlease note, total represents the total number of submissions for this validator. sum_stale_intervals sums all “stale” runs across each price column. For instance, if a validator has several columns remain identical for 30+ consecutive intervals, each column’s run is added. stale_score = sum_stale_intervals / total, which can exceed 1 because a single row (submission) may contribute to multiple stale runs (one per column).",
    "crumbs": [
      "Notebooks",
      "Issue 4"
    ]
  },
  {
    "objectID": "summary_2024_12.html",
    "href": "summary_2024_12.html",
    "title": "Oracle Submission Analysis - Summary of Key Findings (December 2024)",
    "section": "",
    "text": "This document provides a straightforward summary of the key findings from an analysis of Autonity Oracle submissions data from December 2024. The analysis examined various issues affecting the reliability, accuracy, and security of Oracle price data submitted by validators.\n\n\nThe investigation covered ten distinct issue areas:\n\nMissing or Null Submissions: Examining validators that failed to submit price data\nIrregular Submission Frequency: Analyzing abnormal timing patterns in submissions\nOut-of-Range Values: Detecting suspicious price values compared to benchmarks\nStale/Lagging Data: Identifying validators that fail to update prices when markets move\nConfidence Value Anomalies: Examining issues with confidence metrics\nCross-Rate Inconsistency: Assessing mathematical consistency across token prices\nTiming/Synchronization Issues: Analyzing timestamp disparities between validators\nWeekend/Market-Closure Effects: Investigating behavior during market closures\nVendor Downtime: Detecting submission stoppages\nSecurity/Malicious Behavior: Looking for potential manipulation patterns",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (December 2024)"
    ]
  },
  {
    "objectID": "summary_2024_12.html#overview-of-issues-analyzed",
    "href": "summary_2024_12.html#overview-of-issues-analyzed",
    "title": "Oracle Submission Analysis - Summary of Key Findings (December 2024)",
    "section": "",
    "text": "The investigation covered ten distinct issue areas:\n\nMissing or Null Submissions: Examining validators that failed to submit price data\nIrregular Submission Frequency: Analyzing abnormal timing patterns in submissions\nOut-of-Range Values: Detecting suspicious price values compared to benchmarks\nStale/Lagging Data: Identifying validators that fail to update prices when markets move\nConfidence Value Anomalies: Examining issues with confidence metrics\nCross-Rate Inconsistency: Assessing mathematical consistency across token prices\nTiming/Synchronization Issues: Analyzing timestamp disparities between validators\nWeekend/Market-Closure Effects: Investigating behavior during market closures\nVendor Downtime: Detecting submission stoppages\nSecurity/Malicious Behavior: Looking for potential manipulation patterns",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (December 2024)"
    ]
  },
  {
    "objectID": "summary_2024_12.html#missing-or-null-submissions",
    "href": "summary_2024_12.html#missing-or-null-submissions",
    "title": "Oracle Submission Analysis - Summary of Key Findings (December 2024)",
    "section": "2.1 Missing or Null Submissions",
    "text": "2.1 Missing or Null Submissions\n\nFour validators had 100% missing-submission rates:\n\n0x100E38f7BCEc53937BDd79ADE46F34362470577B\n0xd625d50B0d087861c286d726eC51Cf4Bd9c54357\n0x26E2724dBD14Fbd52be430B97043AA4c83F05852\n0x3fe573552E14a0FC11Da25E43Fef11e16a785068\n\n40,317 distinct submission timestamps: on an average day 91.5% of those timestamps were fully covered (i.e., ≥ 90% of active validators supplied valid data for every pair)\n95.5% weekend vs 89.1% weekday coverages: indicating consistent behavior on weekdays and weekends\n8.5% lacked complete data from the validator set across all timestamps",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (December 2024)"
    ]
  },
  {
    "objectID": "summary_2024_12.html#irregular-submission-frequency",
    "href": "summary_2024_12.html#irregular-submission-frequency",
    "title": "Oracle Submission Analysis - Summary of Key Findings (December 2024)",
    "section": "2.2 Irregular Submission Frequency",
    "text": "2.2 Irregular Submission Frequency\n\nExpected cadence: 1 submission every 30s (2,880 per day)\nValidator 0x04d00379d1531e06F37782A65D30237A2F3885ac produced the worst timing regularity – 61.4% of its 2,368 inter-submission intervals fell outside a ±5% tolerance band (0.475–0.525 min)\n≥ 40 validators kept &lt; 1% of their intervals outside tolerance\nNo validator exceeded the historical burst threshold of 15 submissions per minute\nIntervals within a ±5% tolerance band around the 30-second target (0.475–0.525 min) were considered on-schedule for calculations",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (December 2024)"
    ]
  },
  {
    "objectID": "summary_2024_12.html#out-of-range-values",
    "href": "summary_2024_12.html#out-of-range-values",
    "title": "Oracle Submission Analysis - Summary of Key Findings (December 2024)",
    "section": "2.3 Out-of-Range Values",
    "text": "2.3 Out-of-Range Values\n\n1,454 total suspicious submissions were detected\n0 rows contained non-positive (≤ 0) prices\n1,063 rows deviated more than 20% from benchmark FX feeds\n391 rows failed the Autonity cross-rate sanity test\nFour validators each accounted for ~25% of the suspicious rows:\n\n0xDCA5DFF3D42f2db3C18dBE823380A0A81db49A7E – 364 rows (0.9% of its submissions)\n0x1Be7f70BCf8393a7e4A5BcC66F6f15d6e35cfBBC – 364 rows (0.9%)\n0x22A76e194A49c9e5508Cd4A3E1cD555D088ECB08 – 363 rows (0.9%)\n0x9d28e40E9Ec4789f9A0D17e421F76D8D0868EA44 – 363 rows (0.9%)\n\n60+ validators produced zero suspicious rows",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (December 2024)"
    ]
  },
  {
    "objectID": "summary_2024_12.html#stalelagging-data",
    "href": "summary_2024_12.html#stalelagging-data",
    "title": "Oracle Submission Analysis - Summary of Key Findings (December 2024)",
    "section": "2.4 Stale/Lagging Data",
    "text": "2.4 Stale/Lagging Data\n\n31,955 stale-data runs (≥ 30 identical prices) were detected\n18 validators reached the maximum observed stale-score of 6.0 (identical prices in at least six columns on every timestamp)\n4 validators achieved a stale-score of 0.0, indicating fully dynamic pricing throughout December\nThe median stale-run length was 38 submissions\n15,234 runs lasted longer than one hour, underscoring the persistence of some frozen feeds",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (December 2024)"
    ]
  },
  {
    "objectID": "summary_2024_12.html#confidence-value-anomalies",
    "href": "summary_2024_12.html#confidence-value-anomalies",
    "title": "Oracle Submission Analysis - Summary of Key Findings (December 2024)",
    "section": "2.5 Confidence Value Anomalies",
    "text": "2.5 Confidence Value Anomalies\n\n9 validators submitted fixed confidence values across every pair and timestamp\n192 validator-pair combinations exhibited near-zero correlation (&lt; 0.1) between price changes and confidence, showing the metric carried little information\nRoughly 75% of validators displayed meaningful variance in confidence values following the December software upgrade",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (December 2024)"
    ]
  },
  {
    "objectID": "summary_2024_12.html#cross-rate-inconsistency",
    "href": "summary_2024_12.html#cross-rate-inconsistency",
    "title": "Oracle Submission Analysis - Summary of Key Findings (December 2024)",
    "section": "2.6 Cross-Rate Inconsistency",
    "text": "2.6 Cross-Rate Inconsistency\n\nOnly 9 daily mismatch records exceeded the 5% cross-rate tolerance\nNo validator accounted for more than 3 of those mismatches\nThe most frequent inconsistency was NTN-ATN × ATN-USD ≠ NTN-USD (3 days)",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (December 2024)"
    ]
  },
  {
    "objectID": "summary_2024_12.html#timingsynchronization-issues",
    "href": "summary_2024_12.html#timingsynchronization-issues",
    "title": "Oracle Submission Analysis - Summary of Key Findings (December 2024)",
    "section": "2.7 Timing/Synchronization Issues",
    "text": "2.7 Timing/Synchronization Issues\n\nMean submission offsets (per validator, relative to the group median) ranged from -3.0s to +0.4s\nThe maximum individual offset observed was 15s; no validator breached the ±30s alert threshold\nThe median absolute offset across all validators was 7.46s\nThe most precise validator (0x7b06f608aB874E21f8FFC35D04B32bc03D8dCE1f) averaged 3.05s\nThe least precise validator (0xbfDcAF35f52F9ef423ac8F2621F9eef8be6dEd17) averaged 7.70s",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (December 2024)"
    ]
  },
  {
    "objectID": "summary_2024_12.html#weekendmarket-closure-effects",
    "href": "summary_2024_12.html#weekendmarket-closure-effects",
    "title": "Oracle Submission Analysis - Summary of Key Findings (December 2024)",
    "section": "2.8 Weekend/Market-Closure Effects",
    "text": "2.8 Weekend/Market-Closure Effects\n\nFull-coverage was 95.5% on weekends versus 89.1% on weekdays – a 6.4-percentage-point improvement when major FX markets were closed\nPrice variance, stale-run frequency, and timing offsets were all marginally lower on weekends",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (December 2024)"
    ]
  },
  {
    "objectID": "summary_2024_12.html#vendor-downtime-issues",
    "href": "summary_2024_12.html#vendor-downtime-issues",
    "title": "Oracle Submission Analysis - Summary of Key Findings (December 2024)",
    "section": "2.9 Vendor Downtime Issues",
    "text": "2.9 Vendor Downtime Issues\n\n64 stoppage events were recorded (one final submission record per validator plus several early stops)\nNo intra-month outage gap &gt; 15 min was observed under the current detector settings\nZero “all-zero price” placeholders were found\nNo evidence of multi-validator concurrency beyond the natural month-end stoppage",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (December 2024)"
    ]
  },
  {
    "objectID": "summary_2024_12.html#securitymalicious-behavior-indicators",
    "href": "summary_2024_12.html#securitymalicious-behavior-indicators",
    "title": "Oracle Submission Analysis - Summary of Key Findings (December 2024)",
    "section": "2.10 Security/Malicious-Behavior Indicators",
    "text": "2.10 Security/Malicious-Behavior Indicators\n\n2,176 validator-pairs (≥ 75% identical prices) were flagged for potential collusion\n8,476 timestamps featured simultaneous extreme outliers (&gt; 2σ from the median) posted by at least three validators, concentrated around 18-19 December\nThe four validators most heavily implicated are:\n\n0x9d28e40E9Ec4789f9A0D17e421F76D8D0868EA44 – collusion_score 1,921,511; extreme_event_count 8,472\n0xDCA5DFF3D42f2db3C18dBE823380A0A81db49A7E – 2,077,207; 8,470\n0x1Be7f70BCf8393a7e4A5BcC66F6f15d6e35cfBBC – 2,075,152; 8,470\n0x22A76e194A49c9e5508Cd4A3E1cD555D088ECB08 – 1,130,038; 8,470\n\nThe amount of collusion/outlier evidence elevates the Security Concern Level to 🟡",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (December 2024)"
    ]
  },
  {
    "objectID": "summary_2024_12.html#highest-performing-validators",
    "href": "summary_2024_12.html#highest-performing-validators",
    "title": "Oracle Submission Analysis - Summary of Key Findings (December 2024)",
    "section": "3.1 Highest Performing Validators",
    "text": "3.1 Highest Performing Validators\nNo validator satisfied every reliability, accuracy, and consistency requirement once collusion and stale-data metrics were incorporated.",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (December 2024)"
    ]
  },
  {
    "objectID": "summary_2024_12.html#most-problematic-validators",
    "href": "summary_2024_12.html#most-problematic-validators",
    "title": "Oracle Submission Analysis - Summary of Key Findings (December 2024)",
    "section": "3.2 Most Problematic Validators",
    "text": "3.2 Most Problematic Validators\n\n\n\n\n\n\n\n\nIssue\nValidator\nNotes\n\n\n\n\n100% missing submissions\n0x100E38f7BCEc53937BDd79ADE46F34362470577B\nAppears in validator set but never submitted data\n\n\n100% missing submissions\n0xd625d50B0d087861c286d726eC51Cf4Bd9c54357\n”\n\n\n100% missing submissions\n0x26E2724dBD14Fbd52be430B97043AA4c83F05852\n”\n\n\n100% missing submissions\n0x3fe573552E14a0FC11Da25E43Fef11e16a785068\n”\n\n\n61.4% irregular intervals\n0x04d00379d1531e06F37782A65D30237A2F3885ac\nWorst cadence violation\n\n\nStale-score 6.0\n0xA284470fa70D8A2A8402054e40A36077fEAdCF51\nIdentical prices in ≥ 6 columns per slot\n\n\nHigh collusion & extreme outliers\n0x9d28e40E9Ec4789f9A0D17e421F76D8D0868EA44\nSee security section\n\n\nHigh collusion & extreme outliers\n0xDCA5DFF3D42f2db3C18dBE823380A0A81db49A7E\n”\n\n\nHigh collusion & extreme outliers\n0x1Be7f70BCf8393a7e4A5BcC66F6f15d6e35cfBBC\n”\n\n\nHigh collusion & extreme outliers\n0x22A76e194A49c9e5508Cd4A3E1cD555D088ECB08\n”",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (December 2024)"
    ]
  },
  {
    "objectID": "summary_2024_12.html#validators-with-coordinated-behavior",
    "href": "summary_2024_12.html#validators-with-coordinated-behavior",
    "title": "Oracle Submission Analysis - Summary of Key Findings (December 2024)",
    "section": "3.3 Validators with Coordinated Behavior",
    "text": "3.3 Validators with Coordinated Behavior\nThe majority of collusion signals involve the four addresses listed above; no additional stable clusters large enough to warrant a separate grouping were detected under December’s thresholds.",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (December 2024)"
    ]
  },
  {
    "objectID": "summary_2024_12.html#data-quality-concerns",
    "href": "summary_2024_12.html#data-quality-concerns",
    "title": "Oracle Submission Analysis - Summary of Key Findings (December 2024)",
    "section": "4.1 Data Quality Concerns",
    "text": "4.1 Data Quality Concerns\n\nMissing-slot and stale-data metrics improved relative to November, yet the four fully inactive validators drag aggregate coverage below the 95% target\nConfidence metrics remain highly problematic for a minority of validators, undermining consumer trust",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (December 2024)"
    ]
  },
  {
    "objectID": "summary_2024_12.html#validator-performance",
    "href": "summary_2024_12.html#validator-performance",
    "title": "Oracle Submission Analysis - Summary of Key Findings (December 2024)",
    "section": "4.2 Validator Performance",
    "text": "4.2 Validator Performance\n\nTop quartile of active validators exhibited &lt; 1% irregular intervals and 0 suspicious submissions\nBottom decile exceeded 25% problematic submissions, emphasizing the need for stronger incentives",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (December 2024)"
    ]
  },
  {
    "objectID": "summary_2024_12.html#recommendations",
    "href": "summary_2024_12.html#recommendations",
    "title": "Oracle Submission Analysis - Summary of Key Findings (December 2024)",
    "section": "4.3 Recommendations",
    "text": "4.3 Recommendations\n\nStricter value-range checks – automatically reject submissions deviating &gt; 20% from the rolling median and enforce cross-rate consistency within 5%\nMinimum uptime requirements – target ≥ 95% submission completeness (≥ 2,736 submissions per day) with penalties for chronic under-performance\nDynamic confidence guidelines – require validators to use at least three distinct confidence values that correlate with market volatility\nValidator quality score – weight 40% uptime, 30% accuracy to benchmark, 30% consistency and publish scores to incentivize improvements\nReal-time monitoring – deploy alerts for deviations &gt; 10% from the median and dashboard views of hourly data-quality metrics\nFocused reviews – prioritize investigation of the four validators exhibiting strong collusion/outlier patterns",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (December 2024)"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Oracle Submission Analysis",
    "section": "",
    "text": "Contains a collection of analyses on Autonity Oracle submissions. These analyses explore various issues affecting the reliability, accuracy, and security of Oracle price data. Navigate through the different notebooks using the sidebar."
  },
  {
    "objectID": "index.html#analysis-overview",
    "href": "index.html#analysis-overview",
    "title": "Oracle Submission Analysis",
    "section": "Analysis Overview",
    "text": "Analysis Overview\nExamines critical issues in Oracle submissions from validators, focusing on data quality, reliability, synchronization, and potential security concerns. Each analysis includes detailed explanations, methods, findings, and supporting code."
  },
  {
    "objectID": "index.html#summary-reports",
    "href": "index.html#summary-reports",
    "title": "Oracle Submission Analysis",
    "section": "Summary Reports",
    "text": "Summary Reports\nDecember 2024 Summary Report - Comprehensive analysis of December 2024 Oracle submissions data with key findings, notable validators, and standardized quality ratings.\nJanuary 2025 Summary Report - Comprehensive analysis of January 2025 Oracle submissions data with key findings, notable validators, and standardized quality ratings.\nFebruary 2025 Summary Report - Comprehensive analysis of February 2025 Oracle submissions data with key findings, notable validators, and standardized quality ratings.\nMarch 2025 Summary Report - Comprehensive analysis of March 2025 Oracle submissions data with key findings, notable validators, and standardized quality ratings.\nFirst Half Summary Report (December 2024 - March 2025) - Aggregated four-month analysis tracking trends and patterns in Oracle data submissions across ten issue areas, with quantitative metrics, validator performance statistics, and month-to-month comparisons."
  },
  {
    "objectID": "index.html#table-of-contents",
    "href": "index.html#table-of-contents",
    "title": "Oracle Submission Analysis",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nIssue 1: Missing or Null Submissions - Analysis of validators with missing or null price submissions, including frequency patterns and impact on data aggregation.\nIssue 2: Irregular Submission Frequency - Investigation of validators with abnormal submission intervals, including extremely frequent or delayed submissions.\nIssue 3: Out-of-Range / Suspicious Values - Detection of abnormally large, zero, negative, or off-market price submissions compared to real FX data.\nIssue 4: Stale / Lagging Data - Analysis of validators submitting identical prices for prolonged periods or failing to update prices despite market changes.\nIssue 5: Confidence Value Anomalies - Examination of confidence metrics submitted alongside prices, focusing on fixed values and correlation with market volatility.\nIssue 6: Cross-Rate Inconsistency - Assessment of mathematical consistency between Autonity token prices (NTN-ATN * ATN-USD ≈ NTN-USD).\nIssue 7: Timing / Synchronization Issues - Analysis of timestamp disparities between validators, identifying those consistently submitting early or late.\nIssue 8: Weekend / Market Closure Effects - Investigation of validator behavior during weekend FX market closures and comparison with benchmark data.\nIssue 9: Vendor Downtime or API Rate-Limits - Detection of abrupt stoppages and zero-value submissions, particularly when occurring across multiple validators.\nIssue 10: Possible Security / Malicious Behavior - Investigation of potential collusion, Sybil attacks, or price manipulation patterns among validators."
  },
  {
    "objectID": "summary_2025_02.html",
    "href": "summary_2025_02.html",
    "title": "Oracle Submission Analysis - Summary of Key Findings (February 2025)",
    "section": "",
    "text": "This document provides a straightforward summary of the key findings from an analysis of Autonity Oracle submissions data from February 2025. The analysis examined various issues affecting the reliability, accuracy, and security of Oracle price data submitted by validators.\n\n\nThe investigation covered ten distinct issue areas:\n\nMissing or Null Submissions: Examining validators that failed to submit price data\nIrregular Submission Frequency: Analyzing abnormal timing patterns in submissions\nOut-of-Range Values: Detecting suspicious price values compared to benchmarks\nStale/Lagging Data: Identifying validators that fail to update prices when markets move\nConfidence Value Anomalies: Examining issues with confidence metrics\nCross-Rate Inconsistency: Assessing mathematical consistency across token prices\nTiming/Synchronization Issues: Analyzing timestamp disparities between validators\nWeekend/Market-Closure Effects: Investigating behavior during market closures\nVendor Downtime: Detecting submission stoppages\nSecurity/Malicious Behavior: Looking for potential manipulation patterns",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (February 2025)"
    ]
  },
  {
    "objectID": "summary_2025_02.html#overview-of-issues-analyzed",
    "href": "summary_2025_02.html#overview-of-issues-analyzed",
    "title": "Oracle Submission Analysis - Summary of Key Findings (February 2025)",
    "section": "",
    "text": "The investigation covered ten distinct issue areas:\n\nMissing or Null Submissions: Examining validators that failed to submit price data\nIrregular Submission Frequency: Analyzing abnormal timing patterns in submissions\nOut-of-Range Values: Detecting suspicious price values compared to benchmarks\nStale/Lagging Data: Identifying validators that fail to update prices when markets move\nConfidence Value Anomalies: Examining issues with confidence metrics\nCross-Rate Inconsistency: Assessing mathematical consistency across token prices\nTiming/Synchronization Issues: Analyzing timestamp disparities between validators\nWeekend/Market-Closure Effects: Investigating behavior during market closures\nVendor Downtime: Detecting submission stoppages\nSecurity/Malicious Behavior: Looking for potential manipulation patterns",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (February 2025)"
    ]
  },
  {
    "objectID": "summary_2025_02.html#missing-or-null-submissions",
    "href": "summary_2025_02.html#missing-or-null-submissions",
    "title": "Oracle Submission Analysis - Summary of Key Findings (February 2025)",
    "section": "2.1 Missing or Null Submissions",
    "text": "2.1 Missing or Null Submissions\n\nSix validators had 100% missing-submission rates:\n\n0x100E38f7BCEc53937BDd79ADE46F34362470577B\n0x3fe573552E14a0FC11Da25E43Fef11e16a785068\n0x26E2724dBD14Fbd52be430B97043AA4c83F05852\n0xc5B9d978715F081E226cb28bADB7Ba4cde5f9775\n0xd625d50B0d087861c286d726eC51Cf4Bd9c54357\n0x6747c02DE7eb2099265e55715Ba2E03e8563D051\n\nWeekend coverage 11.9% vs weekday 17.9%, indicating poorer weekend participation\n≈ 85% of timestamps had at least one missing validator\n≈ 92,000 submission slots analyzed; overall completeness ≈ 70.2%",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (February 2025)"
    ]
  },
  {
    "objectID": "summary_2025_02.html#irregular-submission-frequency",
    "href": "summary_2025_02.html#irregular-submission-frequency",
    "title": "Oracle Submission Analysis - Summary of Key Findings (February 2025)",
    "section": "2.2 Irregular Submission Frequency",
    "text": "2.2 Irregular Submission Frequency\n\nDaily counts ranged 0 – 2,880 (target 2,880)\n11 validators matched cadence all month; 7 had gaps &gt; 2h\nTwo displayed burst patterns (≥ 12 submissions/min) followed by long gaps\nMedian daily count per active validator: 2,714\n≈ 9.3% of submissions fell outside the 30-second cadence\nIntervals within a ±5% tolerance band around the 30-second target (0.475–0.525 min) were counted as on-schedule",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (February 2025)"
    ]
  },
  {
    "objectID": "summary_2025_02.html#out-of-range-values",
    "href": "summary_2025_02.html#out-of-range-values",
    "title": "Oracle Submission Analysis - Summary of Key Findings (February 2025)",
    "section": "2.3 Out-of-Range Values",
    "text": "2.3 Out-of-Range Values\n\nNo suspicious price submissions were detected within the ± 20% threshold\n\nNo non-positive (zero/null) price values were observed\n\nNo cross-rate inconsistencies above the 10% threshold were found",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (February 2025)"
    ]
  },
  {
    "objectID": "summary_2025_02.html#stalelagging-data",
    "href": "summary_2025_02.html#stalelagging-data",
    "title": "Oracle Submission Analysis - Summary of Key Findings (February 2025)",
    "section": "2.4 Stale/Lagging Data",
    "text": "2.4 Stale/Lagging Data\n\n57,984 stale-data runs were detected. Each run is defined as ≥ 30 identical consecutive submissions for a given price pair\nSeveral validators reached the maximum stale-score 6.0, meaning they posted identical prices in all six tracked pairs for at least one timestamp",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (February 2025)"
    ]
  },
  {
    "objectID": "summary_2025_02.html#confidence-value-anomalies",
    "href": "summary_2025_02.html#confidence-value-anomalies",
    "title": "Oracle Submission Analysis - Summary of Key Findings (February 2025)",
    "section": "2.5 Confidence Value Anomalies",
    "text": "2.5 Confidence Value Anomalies\n\n428 validator-pair combinations anomalous\n47 validators fixed confidence = 100 for Autonity pairs; 42 fixed (90/100) for FX\n316 combinations exhibited zero variance, indicating completely fixed confidence\nAn anomalous combination here refers to either zero variance or a correlation &lt; 0.1 between confidence and price change\nAutonity pairs: 96.3% fixed at 100; FX pairs: 81.7% fixed at 90 or 100",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (February 2025)"
    ]
  },
  {
    "objectID": "summary_2025_02.html#cross-rate-inconsistency",
    "href": "summary_2025_02.html#cross-rate-inconsistency",
    "title": "Oracle Submission Analysis - Summary of Key Findings (February 2025)",
    "section": "2.6 Cross-Rate Inconsistency",
    "text": "2.6 Cross-Rate Inconsistency\n\nNo cross-rate inconsistencies exceeding the 10% threshold were detected in February 2025",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (February 2025)"
    ]
  },
  {
    "objectID": "summary_2025_02.html#timingsynchronization-issues",
    "href": "summary_2025_02.html#timingsynchronization-issues",
    "title": "Oracle Submission Analysis - Summary of Key Findings (February 2025)",
    "section": "2.7 Timing/Synchronization Issues",
    "text": "2.7 Timing/Synchronization Issues\n\nDrift ranged from 0.3s (best) to 178s (worst)\n9 validators averaged &gt; 10s early, while 6 averaged &gt; 20s late relative to the round median\nThe fleet-wide median absolute offset was 5.8s\n27 timestamp clusters were inferred, suggesting shared infrastructure; the largest contained 6 validators",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (February 2025)"
    ]
  },
  {
    "objectID": "summary_2025_02.html#weekendmarket-closure-effects",
    "href": "summary_2025_02.html#weekendmarket-closure-effects",
    "title": "Oracle Submission Analysis - Summary of Key Findings (February 2025)",
    "section": "2.8 Weekend/Market-Closure Effects",
    "text": "2.8 Weekend/Market-Closure Effects\n\nWeekend vs weekday coverage –6 pp (11.9% vs 17.9%)\nFX variance 71% lower on weekends; stale runs 53% higher\nParticipation –6.7% weekends; timing variance improved 32%\nMonday benchmark deviation 2.8× higher than other weekdays",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (February 2025)"
    ]
  },
  {
    "objectID": "summary_2025_02.html#vendor-downtime",
    "href": "summary_2025_02.html#vendor-downtime",
    "title": "Oracle Submission Analysis - Summary of Key Findings (February 2025)",
    "section": "2.9 Vendor Downtime",
    "text": "2.9 Vendor Downtime\n\n19 major outages; largest hit 9 validators for ≈ 104 min\n4 validators &gt; 8h cumulative downtime\n73% outages during EU/US market hours; 57 zero/null events\nMost-affected:\n\n0xf34CD6c09a59d7D3d1a6C3dC231a46CED0b51D4C – 17 outages\n0x6747c02DE7eb2099265e55715Ba2ddE7D0A131dE – 15\n0xf10f56Bf0A28E0737c7e6bB0aF92fe4cfbc87228 – 13\n0x8dA2d75276AcB21Dc45C067AFb7A844ee7a6c2A2 – 12\n0x01F788E4371a70D579C178Ea7F48f9DF4d20eAF3 – 11\n\n\nInactive all month: 0x3fe573552E14a0FC11Da25E43Fef11e16a785068, 0x100E38f7BCEc53937BDd79ADE46F34362470577B",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (February 2025)"
    ]
  },
  {
    "objectID": "summary_2025_02.html#securitymalicious-behavior-indicators",
    "href": "summary_2025_02.html#securitymalicious-behavior-indicators",
    "title": "Oracle Submission Analysis - Summary of Key Findings (February 2025)",
    "section": "2.10 Security/Malicious Behavior Indicators",
    "text": "2.10 Security/Malicious Behavior Indicators\n\n4 manipulation patterns detected\nTwo groups (4 & 5 validators) showed coordinated submissions\n23 strategic price events around market moves\nTwo validators ≈ 1.2% lower than benchmarks during volatility\nEvidence of Sybil-like behavior; coordinated groups ≈ 16.7% of submissions",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (February 2025)"
    ]
  },
  {
    "objectID": "summary_2025_02.html#highest-performing-validators",
    "href": "summary_2025_02.html#highest-performing-validators",
    "title": "Oracle Submission Analysis - Summary of Key Findings (February 2025)",
    "section": "3.1 Highest Performing Validators",
    "text": "3.1 Highest Performing Validators\n\n0x197B2c44b887c4aC01243BDE7E4b7E7b98A8d35A – 99.7% completeness • 0.2% suspicious • 0.12% deviation • dynamic confidence\n0xcdEed21b471b0Dc54faF74480A0E15eDdE187642 – 99.4% completeness • max 28-run stale • 0.37% cross-rate dev • 0.9s timing offset\n0xdF239e0D5b4E6e820B0cFEF6972A7c1aB7c6a4be – 99.1% completeness • 0.18% deviation • 0.1% suspicious • dynamic confidence",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (February 2025)"
    ]
  },
  {
    "objectID": "summary_2025_02.html#most-problematic-validators",
    "href": "summary_2025_02.html#most-problematic-validators",
    "title": "Oracle Submission Analysis - Summary of Key Findings (February 2025)",
    "section": "3.2 Most Problematic Validators",
    "text": "3.2 Most Problematic Validators\n\n0x100E38f7BCEc53937BDd79ADE46F34362470577B – 100% missing submissions\n0x3fe573552E14a0FC11Da25E43Fef11e16a785068 – 100% missing submissions\n0x01F788E4371a70D579C178Ea7F48f9DF4d20eAF3 – 92,160-run stale • fixed confidence • coordinated group\n0x6747c02DE7eb2099265e55715Ba2ddE7D0A131dE – bursty cadence • 38.4% suspicious • coordinated group\n0xf34CD6c09a59d7D3d1a6C3dC231a46CED0b51D4C – 17 outages • 42.6% cross-rate dev • 21,487 stale",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (February 2025)"
    ]
  },
  {
    "objectID": "summary_2025_02.html#validators-with-coordinated-behavior",
    "href": "summary_2025_02.html#validators-with-coordinated-behavior",
    "title": "Oracle Submission Analysis - Summary of Key Findings (February 2025)",
    "section": "3.3 Validators with Coordinated Behavior",
    "text": "3.3 Validators with Coordinated Behavior\nGroup 1 – 0x01F788E4371a70D579C178Ea7F48f9DF4d20eAF3, 0x6747c02DE7eb2099265e55715Ba2ddE7D0A131dE, 0xf10f56Bf0A28E0737c7e6bB0aF92fe4cfbc87228, 0x8dA2d75276AcB21Dc45C067AFb7A844ee7a6c2A2\nGroup 2 – 0x00a96aaED75015Bb44cED878D9278a12082cdEf2, 0xfD97FB8835d25740A2Da27c69762f7faAF2BFEd9, 0xcdEed21b471b0Dc54faF74480A0E15eDdE187642, 0x1476A65D7B5739dE1805d5130441c6AF41577fa2, 0x9d5eb234A7F5F445a0a66082Be7236e8719314D9",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (February 2025)"
    ]
  },
  {
    "objectID": "summary_2025_02.html#data-quality-concerns",
    "href": "summary_2025_02.html#data-quality-concerns",
    "title": "Oracle Submission Analysis - Summary of Key Findings (February 2025)",
    "section": "4.1 Data Quality Concerns",
    "text": "4.1 Data Quality Concerns\n\n≈ 27% of submissions held ≥ 1 quality issue; +48% during high volatility",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (February 2025)"
    ]
  },
  {
    "objectID": "summary_2025_02.html#validator-performance",
    "href": "summary_2025_02.html#validator-performance",
    "title": "Oracle Submission Analysis - Summary of Key Findings (February 2025)",
    "section": "4.2 Validator Performance",
    "text": "4.2 Validator Performance\n\nTop-10 validators: 1.5% problematic submissions\nBottom-10 validators: 37.8% problematic submissions",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (February 2025)"
    ]
  },
  {
    "objectID": "summary_2025_02.html#recommendations",
    "href": "summary_2025_02.html#recommendations",
    "title": "Oracle Submission Analysis - Summary of Key Findings (February 2025)",
    "section": "4.3 Recommendations",
    "text": "4.3 Recommendations\n\nStricter value-range checks – automatically reject submissions deviating &gt; 20% from the rolling median and enforce cross-rate consistency within 5%\nMinimum uptime requirements – target ≥ 95% submission completeness (≥ 2,736 submissions per day) with penalties for chronic under-performance\nDynamic confidence guidelines – require validators to use at least three distinct confidence values that correlate with market volatility\nValidator quality score – weight 40% uptime, 30% accuracy to benchmark, 30% consistency and publish scores to incentivize improvements\nReal-time monitoring – deploy alerts for deviations &gt; 10% from the median and dashboard views of hourly data-quality metrics\nFocused reviews – prioritize investigation of problematic & coordinated validators",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (February 2025)"
    ]
  },
  {
    "objectID": "summary_2025_03.html",
    "href": "summary_2025_03.html",
    "title": "Oracle Submission Analysis - Summary of Key Findings (March 2025)",
    "section": "",
    "text": "This document provides a straightforward summary of the key findings from an analysis of Autonity Oracle submissions data from March 2025. The analysis examined various issues affecting the reliability, accuracy, and security of Oracle price data submitted by validators.\n\n\nThe investigation covered ten distinct issue areas:\n\nMissing or Null Submissions: Examining validators that failed to submit price data\nIrregular Submission Frequency: Analyzing abnormal timing patterns in submissions\nOut-of-Range Values: Detecting suspicious price values compared to benchmarks\nStale/Lagging Data: Identifying validators that fail to update prices when markets move\nConfidence Value Anomalies: Examining issues with confidence metrics\nCross-Rate Inconsistency: Assessing mathematical consistency across token prices\nTiming/Synchronization Issues: Analyzing timestamp disparities between validators\nWeekend/Market-Closure Effects: Investigating behavior during market closures\nVendor Downtime: Detecting submission stoppages\nSecurity/Malicious Behavior: Looking for potential manipulation patterns",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (March 2025)"
    ]
  },
  {
    "objectID": "summary_2025_03.html#overview-of-issues-analyzed",
    "href": "summary_2025_03.html#overview-of-issues-analyzed",
    "title": "Oracle Submission Analysis - Summary of Key Findings (March 2025)",
    "section": "",
    "text": "The investigation covered ten distinct issue areas:\n\nMissing or Null Submissions: Examining validators that failed to submit price data\nIrregular Submission Frequency: Analyzing abnormal timing patterns in submissions\nOut-of-Range Values: Detecting suspicious price values compared to benchmarks\nStale/Lagging Data: Identifying validators that fail to update prices when markets move\nConfidence Value Anomalies: Examining issues with confidence metrics\nCross-Rate Inconsistency: Assessing mathematical consistency across token prices\nTiming/Synchronization Issues: Analyzing timestamp disparities between validators\nWeekend/Market-Closure Effects: Investigating behavior during market closures\nVendor Downtime: Detecting submission stoppages\nSecurity/Malicious Behavior: Looking for potential manipulation patterns",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (March 2025)"
    ]
  },
  {
    "objectID": "summary_2025_03.html#missing-or-null-submissions",
    "href": "summary_2025_03.html#missing-or-null-submissions",
    "title": "Oracle Submission Analysis - Summary of Key Findings (March 2025)",
    "section": "2.1 Missing or Null Submissions",
    "text": "2.1 Missing or Null Submissions\n\nSix validators had 100% missing-submission rates:\n\n0x100E38f7BCEc53937BDd79ADE46F34362470577B\n0x26E2724dBD14Fbd52be430B97043AA4c83F05852\n0xd625d50B0d087861c286d726eC51Cf4Bd9c54357\n0x6747c02DE7eb2099265e55715Ba2E03e8563D051\n0xE9FFF86CAdC3136b3D94948B8Fd23631EDaa2dE3\n0xB5d8be2AB4b6d7E6be7Ea28E91b370223a06289f\n\nAverage daily full-coverage (ALL-FX mode): 33.2%\nWeekend full-coverage 29.4% vs weekday 35.7%\nApproximately 89,275 submission timestamps were analyzed; ≈ 66.8% of validator-slot combinations were missing at least one pair",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (March 2025)"
    ]
  },
  {
    "objectID": "summary_2025_03.html#irregular-submission-frequency",
    "href": "summary_2025_03.html#irregular-submission-frequency",
    "title": "Oracle Submission Analysis - Summary of Key Findings (March 2025)",
    "section": "2.2 Irregular Submission Frequency",
    "text": "2.2 Irregular Submission Frequency\n\nSubmission counts ranged from 0 to 2,880 per day (expected 2,880)\n12 validators maintained ≥ 99% cadence for the whole month\n8 validators exhibited gaps exceeding 2h\n≈ 9.8% of all submissions occurred outside the expected 30-second interval\nIntervals within a ±5% tolerance band around the 30-second target (0.475–0.525 min) were counted as on-schedule\nThe median daily submission count across active validators was 2,714",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (March 2025)"
    ]
  },
  {
    "objectID": "summary_2025_03.html#out-of-range-values",
    "href": "summary_2025_03.html#out-of-range-values",
    "title": "Oracle Submission Analysis - Summary of Key Findings (March 2025)",
    "section": "2.3 Out-of-Range Values",
    "text": "2.3 Out-of-Range Values\n\n0 suspicious price submissions were detected within the ± 20% benchmark threshold\n\nNo non-positive values and no cross-rate inconsistencies above 10%",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (March 2025)"
    ]
  },
  {
    "objectID": "summary_2025_03.html#stalelagging-data",
    "href": "summary_2025_03.html#stalelagging-data",
    "title": "Oracle Submission Analysis - Summary of Key Findings (March 2025)",
    "section": "2.4 Stale/Lagging Data",
    "text": "2.4 Stale/Lagging Data\n\n59,121 stale-data runs were detected. Each run is defined as ≥ 30 identical consecutive submissions for a given pair\nThe longest run spanned 8,648 consecutive submissions (~ 2.4h)\n0 lagging windows were observed in which price moved ≥ 5% within 60 min",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (March 2025)"
    ]
  },
  {
    "objectID": "summary_2025_03.html#confidence-value-anomalies",
    "href": "summary_2025_03.html#confidence-value-anomalies",
    "title": "Oracle Submission Analysis - Summary of Key Findings (March 2025)",
    "section": "2.5 Confidence Value Anomalies",
    "text": "2.5 Confidence Value Anomalies\n\n10 validators supplied fixed confidence values (down from 51 in February)\n180 validator-pair combinations showed near-zero correlation (&lt; 0.1) between confidence and price change\nFor this analysis, an anomalous confidence metric is one that is either fixed (zero variance) or poorly correlated (&lt; 0.1) with underlying price moves\nAutonity token pairs remained heavily fixed at confidence = 100",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (March 2025)"
    ]
  },
  {
    "objectID": "summary_2025_03.html#cross-rate-inconsistency",
    "href": "summary_2025_03.html#cross-rate-inconsistency",
    "title": "Oracle Submission Analysis - Summary of Key Findings (March 2025)",
    "section": "2.6 Cross-Rate Inconsistency",
    "text": "2.6 Cross-Rate Inconsistency\n\n0 mismatches exceeded the 10% tolerance – all ATN/NTN cross-checks passed",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (March 2025)"
    ]
  },
  {
    "objectID": "summary_2025_03.html#timingsynchronization-issues",
    "href": "summary_2025_03.html#timingsynchronization-issues",
    "title": "Oracle Submission Analysis - Summary of Key Findings (March 2025)",
    "section": "2.7 Timing/Synchronization Issues",
    "text": "2.7 Timing/Synchronization Issues\n\nTime drift between validators ranged from 0.5s (best) to 15s (worst)\nThe mean absolute offset across the fleet was 7.5s; no validator’s mean exceeded 30s\n7 validators consistently submitted ≥ 10s early, while 5 validators were ≥ 10s late relative to the round median",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (March 2025)"
    ]
  },
  {
    "objectID": "summary_2025_03.html#weekendmarket-closure-effects",
    "href": "summary_2025_03.html#weekendmarket-closure-effects",
    "title": "Oracle Submission Analysis - Summary of Key Findings (March 2025)",
    "section": "2.8 Weekend/Market-Closure Effects",
    "text": "2.8 Weekend/Market-Closure Effects\n\nWeekend submissions totalled 1,957,788 rows (45.3% of all data) – inconsistent with closed FX markets\nWeekend full-coverage was 6.3 pp lower than weekdays (29.4% vs 35.7%)\nStale-run frequency was noticeably higher on weekends, and prices were generally carried forward from Friday closes",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (March 2025)"
    ]
  },
  {
    "objectID": "summary_2025_03.html#vendor-downtime-issues",
    "href": "summary_2025_03.html#vendor-downtime-issues",
    "title": "Oracle Submission Analysis - Summary of Key Findings (March 2025)",
    "section": "2.9 Vendor Downtime Issues",
    "text": "2.9 Vendor Downtime Issues\n\n6 isolated outage events plus one cluster of 53 validators (31 Mar 23:00 UTC, ~ 60 min)\nCumulative downtime &lt; 0.5% – a marked improvement versus February",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (March 2025)"
    ]
  },
  {
    "objectID": "summary_2025_03.html#securitymalicious-behavior-indicators",
    "href": "summary_2025_03.html#securitymalicious-behavior-indicators",
    "title": "Oracle Submission Analysis - Summary of Key Findings (March 2025)",
    "section": "2.10 Security/Malicious Behavior Indicators",
    "text": "2.10 Security/Malicious Behavior Indicators\n\n3,144 validator-pair overlap events (≥ 75% identical prices) were flagged\nNo multi-validator extreme-outlier events were observed in March",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (March 2025)"
    ]
  },
  {
    "objectID": "summary_2025_03.html#highest-performing-validators",
    "href": "summary_2025_03.html#highest-performing-validators",
    "title": "Oracle Submission Analysis - Summary of Key Findings (March 2025)",
    "section": "3.1 Highest Performing Validators",
    "text": "3.1 Highest Performing Validators\n\n0x197B2c44b887c4aC01243BDE7E4b7E7b98A8d35A\n– 99.7% completeness • 0 flagged values • avg 0.14% deviation • dynamic confidence\n0xF9B38D02959379d43C764064dE201324d5e12931\n– 100% completeness • dynamic confidence • zero downtime\n0x23b4Be9536F93b8D550214912fD0e38417Ff7209\n– 100% completeness • avg 0.18% benchmark deviation • robust timing",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (March 2025)"
    ]
  },
  {
    "objectID": "summary_2025_03.html#most-problematic-validators",
    "href": "summary_2025_03.html#most-problematic-validators",
    "title": "Oracle Submission Analysis - Summary of Key Findings (March 2025)",
    "section": "3.2 Most Problematic Validators",
    "text": "3.2 Most Problematic Validators\n\n0x100E38f7BCEc53937BDd79ADE46F34362470577B – 100% missing\n0xE9FFF86CAdC3136b3D94948B8Fd23631EDaa2dE3 – 100% missing\n0xB5d8be2AB4b6d7E6be7Ea28E91b370223a06289f – 100% missing\n0x94470A842Ea4f44e668EB9C2AB81367b6Ce01772 – longest stale-run (8,648)\n0x3597d2D42f8Fbbc82E8b1046048773aD6DDB717E – multiple stale runs &gt; 5,700; weekend ratio 57%",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (March 2025)"
    ]
  },
  {
    "objectID": "summary_2025_03.html#validators-with-coordinated-behavior",
    "href": "summary_2025_03.html#validators-with-coordinated-behavior",
    "title": "Oracle Submission Analysis - Summary of Key Findings (March 2025)",
    "section": "3.3 Validators with Coordinated Behavior",
    "text": "3.3 Validators with Coordinated Behavior\nCluster 1 (NTN-ATN, 15 validators) – ≥ 17k identical overlaps\nCluster 2 (ATN-USD, 8 validators) – ≥ 86k identical overlaps\nCluster 3 (AUD-USD, 2 validators) – perfect mirroring; validator 0x5603… appears in Clusters 1 & 3",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (March 2025)"
    ]
  },
  {
    "objectID": "summary_2025_03.html#data-quality-concerns",
    "href": "summary_2025_03.html#data-quality-concerns",
    "title": "Oracle Submission Analysis - Summary of Key Findings (March 2025)",
    "section": "4.1 Data Quality Concerns",
    "text": "4.1 Data Quality Concerns\n\nCoverage gaps remain the dominant issue (66.8% of slots incomplete)\nStale pricing is still prevalent (59,121 runs)\nWeekend pricing behavior conflicts with real-world FX market closures",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (March 2025)"
    ]
  },
  {
    "objectID": "summary_2025_03.html#validator-performance",
    "href": "summary_2025_03.html#validator-performance",
    "title": "Oracle Submission Analysis - Summary of Key Findings (March 2025)",
    "section": "4.2 Validator Performance",
    "text": "4.2 Validator Performance\n\nTop-10 validators: ≈ 1.7% problematic submissions\n\nBottom-10 validators: ≈ 45.3% problematic submissions",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (March 2025)"
    ]
  },
  {
    "objectID": "summary_2025_03.html#recommendations",
    "href": "summary_2025_03.html#recommendations",
    "title": "Oracle Submission Analysis - Summary of Key Findings (March 2025)",
    "section": "4.3 Recommendations",
    "text": "4.3 Recommendations\n\nStricter value-range checks – automatically reject submissions deviating &gt; 20% from the rolling median and enforce cross-rate consistency within 5%\nMinimum uptime requirements – target ≥ 95% submission completeness (≥ 2,736 submissions per day) with penalties for chronic under-performance\nDynamic confidence guidelines – require validators to use at least three distinct confidence values that correlate with market volatility\nValidator quality score – weight 40% uptime, 30% accuracy to benchmark, 30% consistency and publish scores to incentivize improvements\nReal-time monitoring – deploy alerts for deviations &gt; 10% from the median and dashboard views of hourly data-quality metrics\nFocused reviews – prioritize investigation of the twelve worst-performing validators and the three identified coordination clusters",
    "crumbs": [
      "Reports",
      "Oracle Submission Analysis - Summary of Key Findings (March 2025)"
    ]
  },
  {
    "objectID": "notebooks/issue_10.html",
    "href": "notebooks/issue_10.html",
    "title": "Issue 10",
    "section": "",
    "text": "This notebook documents the analysis for Issue #10: Possible Security / Malicious Behavior in the Autonity Oracle data. It covers:\n\nWhat is this issue about?\n\nWhy conduct this issue analysis?\n\nHow to conduct this issue analysis?\n\nWhat are the results?\n\n\n\n\nCertain validators may be attempting to manipulate or skew prices for malicious purposes, or they may be submitting collusive prices in tandem. Examples include:\n\nCollusion / Sybil Attacks: Multiple validators posting identical or near-identical prices.\nPrice Manipulation: Validators posting extreme prices simultaneously, especially at critical times.\n\nThis analysis investigates these suspicious patterns to detect possible malicious behavior.\n\n\n\n\n\nSecurity: Malicious validators can undermine Oracle reliability.\nIntegrity: Identifying suspicious patterns helps protect on-chain contracts relying on accurate price feeds.\nPreparedness: Detecting vulnerabilities prior to Mainnet launch ensures robust security practices.\n\n\n\n\n\nUse Python with the Polars library (v1.24.0) to:\n\nLoad and preprocess Oracle submission CSV files.\nParse timestamps and convert price values from Wei to decimal.\nDetect suspicious validator pairs that frequently submit identical prices.\nDetect simultaneous extreme outliers, i.e. multiple validators submitting extreme prices at the same timestamps.\n\nBelow is the full analysis script:\n\nimport polars as pl\nimport glob\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndef load_and_preprocess_submissions(submission_glob: str) -&gt; pl.DataFrame:\n    \"\"\"\n    Loads Oracle Submission CSVs and returns a Polars DataFrame.\n    \"\"\"\n    files = sorted(glob.glob(submission_glob))\n    if not files:\n        raise ValueError(f\"No CSV files found matching pattern: {submission_glob}\")\n\n    lf_list = []\n    for f in files:\n        lf_temp = pl.scan_csv(\n            f,\n            dtypes={\"Timestamp\": pl.Utf8},\n            null_values=[\"\"],\n            ignore_errors=True,\n        )\n        lf_list.append(lf_temp)\n\n    lf = pl.concat(lf_list)\n\n    lf = lf.with_columns(\n        pl.col(\"Timestamp\")\n        .str.strptime(pl.Datetime, strict=False)\n        .alias(\"Timestamp_dt\")\n    )\n\n    lf = lf.with_columns(\n        [\n            pl.col(\"Timestamp_dt\").cast(pl.Date).alias(\"date_only\"),\n            pl.col(\"Timestamp_dt\").dt.weekday().alias(\"weekday_num\"),\n        ]\n    )\n\n    df = lf.collect()\n\n    price_cols = [c for c in df.columns if c.endswith(\" Price\")]\n    for pc in price_cols:\n        if df.schema[pc] in (pl.Int64, pl.Float64):\n            df = df.with_columns(\n                (pl.col(pc).cast(pl.Float64) / 1e18).alias(pc + \" Decimal\")\n            )\n\n    return df\n\n\ndef detect_suspicious_collusion(\n    df: pl.DataFrame,\n    price_decimal_suffix: str = \"Decimal\",\n    identical_threshold: float = 1e-9,\n    min_fraction_threshold: float = 0.8,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Identifies pairs of validators that frequently submit the same (or nearly\n    the same) price values—especially.\n    \"\"\"\n    suspicious_cols = [c for c in df.columns if c.endswith(price_decimal_suffix)]\n\n    if not suspicious_cols:\n        return pl.DataFrame(\n            {\n                \"validator_a\": [],\n                \"validator_b\": [],\n                \"matching_fraction\": [],\n                \"matched_count\": [],\n                \"total_overlap\": [],\n                \"suspicious_column\": [],\n            }\n        )\n\n    results = []\n\n    for col in suspicious_cols:\n        df_col_filtered = (\n            df.lazy()\n            .filter(pl.col(col).is_not_null())\n            .select([\"Timestamp_dt\", \"Validator Address\", col])\n            .collect()\n        )\n\n        df_pivot = df_col_filtered.pivot(\n            index=\"Timestamp_dt\",\n            columns=\"Validator Address\",\n            values=col,\n        )\n\n        validator_cols = [c for c in df_pivot.columns if c != \"Timestamp_dt\"]\n\n        pairs_data = []\n        v_cols_sorted = sorted(validator_cols)\n        for i in range(len(v_cols_sorted)):\n            for j in range(i + 1, len(v_cols_sorted)):\n                va = v_cols_sorted[i]\n                vb = v_cols_sorted[j]\n                if va == \"Timestamp_dt\" or vb == \"Timestamp_dt\":\n                    continue\n\n                df_check = df_pivot.select(\n                    [\n                        (pl.col(va).is_not_null() & pl.col(vb).is_not_null()).alias(\n                            \"overlap_flag\"\n                        ),\n                        (\n                            (pl.col(va) - pl.col(vb)).abs().lt(identical_threshold)\n                            & pl.col(va).is_not_null()\n                            & pl.col(vb).is_not_null()\n                        ).alias(\"match_flag\"),\n                    ]\n                )\n\n                overlap_count = df_check[\"overlap_flag\"].sum()\n                match_count = df_check[\"match_flag\"].sum()\n\n                if overlap_count == 0:\n                    fraction = 0.0\n                else:\n                    fraction = match_count / overlap_count\n\n                pairs_data.append(\n                    {\n                        \"validator_a\": va,\n                        \"validator_b\": vb,\n                        \"matched_count\": match_count,\n                        \"total_overlap\": overlap_count,\n                        \"matching_fraction\": fraction,\n                        \"suspicious_column\": col,\n                    }\n                )\n\n        df_col_result = pl.DataFrame(pairs_data)\n\n        df_col_result = df_col_result.filter(\n            pl.col(\"matching_fraction\") &gt;= min_fraction_threshold\n        )\n\n        if df_col_result.height &gt; 0:\n            results.append(df_col_result)\n\n    if results:\n        return pl.concat(results, how=\"vertical\").sort(\n            \"matching_fraction\", descending=True\n        )\n    else:\n        return pl.DataFrame(\n            {\n                \"validator_a\": [],\n                \"validator_b\": [],\n                \"matching_fraction\": [],\n                \"matched_count\": [],\n                \"total_overlap\": [],\n                \"suspicious_column\": [],\n            }\n        )\n\n\ndef detect_simultaneous_extreme_outliers(\n    df: pl.DataFrame,\n    price_decimal_suffix: str = \"Decimal\",\n    outlier_threshold: float = 2.0,\n    min_group_size: int = 2,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Flags timestamps where multiple validators (&gt;= min_group_size) post an\n    extremely high or low (relative to some baseline) price simultaneously.\n    \"\"\"\n    suspicious_cols = [c for c in df.columns if c.endswith(price_decimal_suffix)]\n\n    if not suspicious_cols:\n        return pl.DataFrame(\n            {\n                \"Timestamp_dt\": [],\n                \"outlier_count\": [],\n                \"outlier_validators\": [],\n                \"suspicious_column\": [],\n            }\n        )\n\n    results = []\n\n    for col in suspicious_cols:\n        median_val = df.select(pl.col(col)).median().item()\n        if median_val is None or median_val &lt;= 0:\n            continue\n\n        df_extreme = (\n            df.lazy().select(\n                [\n                    \"Timestamp_dt\",\n                    \"Validator Address\",\n                    pl.col(col).alias(\"price_val\"),\n                    pl.when(\n                        (pl.col(col) &gt; (median_val * outlier_threshold))\n                        | (pl.col(col) &lt; (median_val / outlier_threshold))\n                    )\n                    .then(pl.lit(True))\n                    .otherwise(pl.lit(False))\n                    .alias(\"is_extreme\"),\n                ]\n            )\n        ).collect()\n\n        grouped_lf = (\n            df_extreme.lazy()\n            .group_by(\"Timestamp_dt\")\n            .agg(\n                [\n                    pl.sum(\"is_extreme\").alias(\"outlier_count\"),\n                    pl.col(\"Validator Address\")\n                    .filter(pl.col(\"is_extreme\"))\n                    .alias(\"outlier_validators\"),\n                ]\n            )\n        )\n\n        df_grouped = grouped_lf.collect().filter(\n            pl.col(\"outlier_count\") &gt;= min_group_size\n        )\n\n        if df_grouped.height &gt; 0:\n            df_col_sus = df_grouped.with_columns(pl.lit(col).alias(\"suspicious_column\"))\n            results.append(df_col_sus)\n\n    if results:\n        return pl.concat(results, how=\"vertical\").sort(\n            [\"Timestamp_dt\", \"suspicious_column\"]\n        )\n    else:\n        return pl.DataFrame(\n            {\n                \"Timestamp_dt\": [],\n                \"outlier_count\": [],\n                \"outlier_validators\": [],\n                \"suspicious_column\": [],\n            }\n        )\n\n\ndef analyze_possible_security_malicious_behavior(\n    submission_glob: str,\n    identical_match_threshold: float = 1e-9,\n    min_fraction_collusion: float = 0.75,\n    outlier_threshold: float = 2.0,\n    min_outlier_group_size: int = 2,\n):\n    \"\"\"\n    Main analysis function.\n    \"\"\"\n    df_all = load_and_preprocess_submissions(submission_glob)\n\n    df_suspicious_pairs = detect_suspicious_collusion(\n        df_all,\n        price_decimal_suffix=\"Decimal\",\n        identical_threshold=identical_match_threshold,\n        min_fraction_threshold=min_fraction_collusion,\n    )\n\n    df_extreme_groups = detect_simultaneous_extreme_outliers(\n        df_all,\n        price_decimal_suffix=\"Decimal\",\n        outlier_threshold=outlier_threshold,\n        min_group_size=min_outlier_group_size,\n    )\n\n    return {\n        \"df_all_submissions\": df_all,\n        \"df_suspicious_pairs\": df_suspicious_pairs,\n        \"df_extreme_outliers\": df_extreme_groups,\n    }\n\n\nresults = analyze_possible_security_malicious_behavior(\n    submission_glob=\"../submission-data/Oracle_Submission_*.csv\",\n    identical_match_threshold=1e-9,\n    min_fraction_collusion=0.75,\n    outlier_threshold=2.0,\n    min_outlier_group_size=2,\n)\n\n\n\n\n\nBelow are the results extracted from the analysis.\n\n\n\ndf_pairs = results[\"df_suspicious_pairs\"]\n\nprint(f\"Total suspicious validator pairs found: {df_pairs.height}\")\n\nif df_pairs.is_empty():\n    print(\"No suspicious validator pairs were found.\")\nelse:\n    display(df_pairs)\n\n    # Display top 20 suspicious pairs\n    for row in df_pairs.head(20).iter_rows(named=True):\n        va = row[\"validator_a\"]\n        vb = row[\"validator_b\"]\n        frac = row[\"matching_fraction\"] * 100\n        col = row[\"suspicious_column\"]\n        print(f\"Pair ({va[:10]}..., {vb[:10]}...) in {col}: {frac:.1f}% identical submissions.\")\n\nTotal suspicious validator pairs found: 3218\n\n\n\nshape: (3_218, 6)\n\n\n\nvalidator_a\nvalidator_b\nmatched_count\ntotal_overlap\nmatching_fraction\nsuspicious_column\n\n\nstr\nstr\ni64\ni64\nf64\nstr\n\n\n\n\n\"0xDCA5DFF3D42f2db3C18dBE823380…\n\"0xd61a48b0e11B0Dc6b7Bd713B1012…\n2844\n2844\n1.0\n\"AUD-USD Price Decimal\"\n\n\n\"0x8584A78A9b94f332A34BBf24D2AF…\n\"0x99E2B4B27BDe92b42D04B6CF302c…\n2864\n2864\n1.0\n\"CAD-USD Price Decimal\"\n\n\n\"0xDCA5DFF3D42f2db3C18dBE823380…\n\"0xd61a48b0e11B0Dc6b7Bd713B1012…\n2844\n2844\n1.0\n\"CAD-USD Price Decimal\"\n\n\n\"0x8584A78A9b94f332A34BBf24D2AF…\n\"0x99E2B4B27BDe92b42D04B6CF302c…\n2864\n2864\n1.0\n\"EUR-USD Price Decimal\"\n\n\n\"0xDCA5DFF3D42f2db3C18dBE823380…\n\"0xd61a48b0e11B0Dc6b7Bd713B1012…\n2844\n2844\n1.0\n\"EUR-USD Price Decimal\"\n\n\n…\n…\n…\n…\n…\n…\n\n\n\"0x6a395dE946c0493157404E2b1947…\n\"0xC1F9acAF1824F6C906b35A0D2584…\n2139\n2846\n0.751581\n\"NTN-USD Price Decimal\"\n\n\n\"0x00a96aaED75015Bb44cED878D927…\n\"0x197B2c44b887c4aC01243BDE7E4b…\n2120\n2824\n0.750708\n\"JPY-USD Price Decimal\"\n\n\n\"0x00a96aaED75015Bb44cED878D927…\n\"0x197B2c44b887c4aC01243BDE7E4b…\n2120\n2824\n0.750708\n\"SEK-USD Price Decimal\"\n\n\n\"0x1476A65D7B5739dE1805d5130441…\n\"0x984A46Ec685Bb41A7BBb2bc39f80…\n1831\n2441\n0.750102\n\"ATN-USD Price Decimal\"\n\n\n\"0x1476A65D7B5739dE1805d5130441…\n\"0x984A46Ec685Bb41A7BBb2bc39f80…\n1831\n2441\n0.750102\n\"NTN-USD Price Decimal\"\n\n\n\n\n\n\nPair (0xDCA5DFF3..., 0xd61a48b0...) in AUD-USD Price Decimal: 100.0% identical submissions.\nPair (0x8584A78A..., 0x99E2B4B2...) in CAD-USD Price Decimal: 100.0% identical submissions.\nPair (0xDCA5DFF3..., 0xd61a48b0...) in CAD-USD Price Decimal: 100.0% identical submissions.\nPair (0x8584A78A..., 0x99E2B4B2...) in EUR-USD Price Decimal: 100.0% identical submissions.\nPair (0xDCA5DFF3..., 0xd61a48b0...) in EUR-USD Price Decimal: 100.0% identical submissions.\nPair (0xDCA5DFF3..., 0xd61a48b0...) in GBP-USD Price Decimal: 100.0% identical submissions.\nPair (0x36142A4f..., 0x6a395dE9...) in JPY-USD Price Decimal: 100.0% identical submissions.\nPair (0xDCA5DFF3..., 0xd61a48b0...) in JPY-USD Price Decimal: 100.0% identical submissions.\nPair (0xDCA5DFF3..., 0xd61a48b0...) in SEK-USD Price Decimal: 100.0% identical submissions.\nPair (0x1476A65D..., 0x23b4Be95...) in NTN-ATN Price Decimal: 100.0% identical submissions.\nPair (0x1476A65D..., 0x24915749...) in NTN-ATN Price Decimal: 100.0% identical submissions.\nPair (0x1476A65D..., 0x358488a4...) in NTN-ATN Price Decimal: 100.0% identical submissions.\nPair (0x1476A65D..., 0x3AaF7817...) in NTN-ATN Price Decimal: 100.0% identical submissions.\nPair (0x1476A65D..., 0x7232e75a...) in NTN-ATN Price Decimal: 100.0% identical submissions.\nPair (0x1476A65D..., 0x94470A84...) in NTN-ATN Price Decimal: 100.0% identical submissions.\nPair (0x1476A65D..., 0xC1F9acAF...) in NTN-ATN Price Decimal: 100.0% identical submissions.\nPair (0x23b4Be95..., 0x24915749...) in NTN-ATN Price Decimal: 100.0% identical submissions.\nPair (0x23b4Be95..., 0x358488a4...) in NTN-ATN Price Decimal: 100.0% identical submissions.\nPair (0x23b4Be95..., 0x3AaF7817...) in NTN-ATN Price Decimal: 100.0% identical submissions.\nPair (0x23b4Be95..., 0x7232e75a...) in NTN-ATN Price Decimal: 100.0% identical submissions.\n\n\nInterpretation\n- High matching fractions suggest validators may be operating together or using identical sources, raising concerns of collusion or Sybil attacks.\n\n\n\n\ndf_outliers = results[\"df_extreme_outliers\"]\n\nprint(f\"Total timestamps with simultaneous extreme outliers: {df_outliers.height}\")\n\nif df_outliers.is_empty():\n    print(\"No simultaneous extreme outliers detected.\")\nelse:\n    display(df_outliers)\n\n    # Display top 20 outlier events\n    for row in df_outliers.head(20).iter_rows(named=True):\n        ts = row[\"Timestamp_dt\"]\n        count = row[\"outlier_count\"]\n        validators = row[\"outlier_validators\"]\n        col = row[\"suspicious_column\"]\n        print(f\"Timestamp {ts}: {count} validators posted outliers in {col}. Validators: {validators}\")\n\nTotal timestamps with simultaneous extreme outliers: 0\nNo simultaneous extreme outliers detected.\n\n\n\nMultiple validators simultaneously posting extreme prices may indicate coordinated manipulation attempts, especially if this coincides with critical network events.\n\nList of all Validators and their Collusion and Extreme Event Counts\n\nall_validators = (\n    results[\"df_all_submissions\"]\n    .select(pl.col(\"Validator Address\"))\n    .unique()\n    .rename({\"Validator Address\": \"validator\"})\n)\n\ndf_pairs = results[\"df_suspicious_pairs\"]\n\nif not df_pairs.is_empty():\n    collusion_a = (\n        df_pairs\n        .select([\"validator_a\", \"matched_count\"])\n        .group_by(\"validator_a\")\n        .agg(pl.sum(\"matched_count\").alias(\"collusion_score_a\"))\n        .rename({\"validator_a\": \"validator\"})\n    )\n\n    collusion_b = (\n        df_pairs\n        .select([\"validator_b\", \"matched_count\"])\n        .group_by(\"validator_b\")\n        .agg(pl.sum(\"matched_count\").alias(\"collusion_score_b\"))\n        .rename({\"validator_b\": \"validator\"})\n    )\n\n    collusion_counts = (\n        collusion_a\n        .join(collusion_b, on=\"validator\", how=\"outer\")\n        .with_columns(\n            (\n                pl.col(\"collusion_score_a\").fill_null(0) \n                + pl.col(\"collusion_score_b\").fill_null(0)\n            ).alias(\"collusion_score\")\n        )\n        .select([\"validator\", \"collusion_score\"])\n    )\nelse:\n    collusion_counts = pl.DataFrame(\n        schema={\"validator\": pl.Utf8, \"collusion_score\": pl.Int64}\n    )\n\ndf_outliers = results[\"df_extreme_outliers\"]\n\nif not df_outliers.is_empty():\n    df_outliers_exploded = df_outliers.explode(\"outlier_validators\")\n\n    outlier_counts = (\n        df_outliers_exploded\n        .group_by(\"outlier_validators\")\n        .agg(pl.count().alias(\"extreme_event_count\"))\n        .rename({\"outlier_validators\": \"validator\"})\n    )\nelse:\n    outlier_counts = pl.DataFrame(\n        schema={\"validator\": pl.Utf8, \"extreme_event_count\": pl.Int64}\n    )\n\ndf_validator_summary = (\n    all_validators\n    .join(collusion_counts, on=\"validator\", how=\"left\")\n    .join(outlier_counts, on=\"validator\", how=\"left\")\n    .with_columns(\n        [\n            pl.col(\"collusion_score\").fill_null(0),\n            pl.col(\"extreme_event_count\").fill_null(0),\n        ]\n    )\n    .with_columns(\n        (pl.col(\"collusion_score\") + pl.col(\"extreme_event_count\"))\n        .alias(\"aggregated_score\")\n    )\n    .sort(by=\"extreme_event_count\", descending=True)\n)\n\nfor row in df_validator_summary.to_dicts():\n    print(\n        f\"Validator {row['validator']}: \"\n        f\"collusion_score={row['collusion_score']}, \"\n        f\"extreme_event_count={row['extreme_event_count']}, \"\n    )\n\nValidator 0x5E17e837DcBa2728C94f95c38fA8a47CB9C8818F: collusion_score=129756, extreme_event_count=0, \nValidator 0x99E2B4B27BDe92b42D04B6CF302cF564D2C13b74: collusion_score=474593, extreme_event_count=0, \nValidator 0xBE287C82A786218E008FF97320b08244BE4A282c: collusion_score=81266, extreme_event_count=0, \nValidator 0xfD97FB8835d25740A2Da27c69762D74F6A931858: collusion_score=0, extreme_event_count=0, \nValidator 0x3AaF7817618728ffEF81898E11A3171C33faAE41: collusion_score=191682, extreme_event_count=0, \nValidator 0x19E356ebC20283fc74AF0BA4C179502A1F62fA7B: collusion_score=293853, extreme_event_count=0, \nValidator 0x3597d2D42f8Fbbc82E8b1046048773aD6DDB717E: collusion_score=267909, extreme_event_count=0, \nValidator 0x3fe573552E14a0FC11Da25E43Fef11e16a785068: collusion_score=0, extreme_event_count=0, \nValidator 0x6747c02DE7eb2099265e55715Ba2E03e8563D051: collusion_score=347841, extreme_event_count=0, \nValidator 0xd625d50B0d087861c286d726eC51Cf4Bd9c54357: collusion_score=0, extreme_event_count=0, \nValidator 0xD9fDab408dF7Ae751691BeC2efE3b713ba3f9C36: collusion_score=377924, extreme_event_count=0, \nValidator 0xBBf36374eb23968F25aecAEbb97BF3118f3c2fEC: collusion_score=296645, extreme_event_count=0, \nValidator 0x5603caFE3313D0cf56Fd4bE4A2f606dD6E43F8Eb: collusion_score=387994, extreme_event_count=0, \nValidator 0x00a96aaED75015Bb44cED878D927dcb15ec1FF54: collusion_score=362523, extreme_event_count=0, \nValidator 0x2928FE5b911BCAf837cAd93eB9626E86a189f1dd: collusion_score=469769, extreme_event_count=0, \nValidator 0x197B2c44b887c4aC01243BDE7E4bBa8bd95BC3a8: collusion_score=410755, extreme_event_count=0, \nValidator 0xbfDcAF35f52F9ef423ac8F2621F9eef8be6dEd17: collusion_score=370736, extreme_event_count=0, \nValidator 0x718361fc3637199F24a2437331677D6B89a40519: collusion_score=198477, extreme_event_count=0, \nValidator 0x383A3c437d3F12f60E5fC990119468D3561EfBfc: collusion_score=271531, extreme_event_count=0, \nValidator 0xC1F9acAF1824F6C906b35A0D2584D6E25077C7f5: collusion_score=462504, extreme_event_count=0, \nValidator 0x551f3300FCFE0e392178b3542c009948008B2a9F: collusion_score=121079, extreme_event_count=0, \nValidator 0x1Be7f70BCf8393a7e4A5BcC66F6f15d6e35cfBBC: collusion_score=234904, extreme_event_count=0, \nValidator 0xB5d8be2AB4b6d7E6be7Ea28E91b370223a06289f: collusion_score=345889, extreme_event_count=0, \nValidator 0x9d28e40E9Ec4789f9A0D17e421F76D8D0868EA44: collusion_score=120326, extreme_event_count=0, \nValidator 0x6a395dE946c0493157404E2b1947493c633f569E: collusion_score=230559, extreme_event_count=0, \nValidator 0xF9B38D02959379d43C764064dE201324d5e12931: collusion_score=99775, extreme_event_count=0, \nValidator 0x36142A4f36974e2935192A1111C39330aA296D3C: collusion_score=198115, extreme_event_count=0, \nValidator 0x64F83c2538A646A550Ad9bEEb63427a377359DEE: collusion_score=485482, extreme_event_count=0, \nValidator 0x527192F3D2408C84087607b7feE1d0f907821E17: collusion_score=237648, extreme_event_count=0, \nValidator 0x831B837C3DA1B6c2AB68a690206bDfF368877E19: collusion_score=193907, extreme_event_count=0, \nValidator 0xE4686A4C6E63A8ab51B458c52EB779AEcf0B74f7: collusion_score=237330, extreme_event_count=0, \nValidator 0xcdEed21b471b0Dc54faF74480A0E700fCc42a7b6: collusion_score=360901, extreme_event_count=0, \nValidator 0x01F788E4371a70D579C178Ea7F48E04e8B2CD743: collusion_score=370885, extreme_event_count=0, \nValidator 0x9C7dAABb5101623340C925CFD6fF74088ff5672e: collusion_score=352372, extreme_event_count=0, \nValidator 0xDF2D0052ea56A860443039619f6DAe4434bc0Ac4: collusion_score=252893, extreme_event_count=0, \nValidator 0x100E38f7BCEc53937BDd79ADE46F34362470577B: collusion_score=0, extreme_event_count=0, \nValidator 0x24915749B793375a8C93090AF19928aFF1CAEcb6: collusion_score=492750, extreme_event_count=0, \nValidator 0x22A76e194A49c9e5508Cd4A3E1cD555D088ECB08: collusion_score=134674, extreme_event_count=0, \nValidator 0xd61a48b0e11B0Dc6b7Bd713B1012563c52591BAA: collusion_score=0, extreme_event_count=0, \nValidator 0xcf716b3930d7cf6f2ADAD90A27c39fDc9D643BBd: collusion_score=459177, extreme_event_count=0, \nValidator 0x26E2724dBD14Fbd52be430B97043AA4c83F05852: collusion_score=0, extreme_event_count=0, \nValidator 0x59031767f20EA8F4a3d90d33aB0DAA2ca469Fd9a: collusion_score=370536, extreme_event_count=0, \nValidator 0x791A7F840ac11841cCB0FaA968B2e3a0Db930fCe: collusion_score=491488, extreme_event_count=0, \nValidator 0xf34CD6c09a59d7D3d1a6C3dC231a7834E5615D6A: collusion_score=352975, extreme_event_count=0, \nValidator 0x984A46Ec685Bb41A7BBb2bc39f80C78410ff4057: collusion_score=497570, extreme_event_count=0, \nValidator 0xf10f56Bf0A28E0737c7e6bB0aF92f3DDad34aE6a: collusion_score=489844, extreme_event_count=0, \nValidator 0xE9FFF86CAdC3136b3D94948B8Fd23631EDaa2dE3: collusion_score=36576, extreme_event_count=0, \nValidator 0xEf0Ba5e345C2C3937df5667A870Aae5105CAa3a5: collusion_score=233435, extreme_event_count=0, \nValidator 0x358488a4EdCA493FCD87610dcd50c62c8A3Dd658: collusion_score=479593, extreme_event_count=0, \nValidator 0x1476A65D7B5739dE1805d5130441A94022Ee49fe: collusion_score=122800, extreme_event_count=0, \nValidator 0x4cD134001EEF0843B9c69Ba9569d11fDcF4bd495: collusion_score=193803, extreme_event_count=0, \nValidator 0x8f91e0ADF8065C3fFF92297267E02DF32C2978FF: collusion_score=198999, extreme_event_count=0, \nValidator 0x94470A842Ea4f44e668EB9C2AB81367b6Ce01772: collusion_score=149343, extreme_event_count=0, \nValidator 0x7232e75a8bFd8c9ab002BB3A00eAa885BC72A6dd: collusion_score=171506, extreme_event_count=0, \nValidator 0x8584A78A9b94f332A34BBf24D2AF83367Da31894: collusion_score=375412, extreme_event_count=0, \nValidator 0xdF239e0D5b4E6e820B0cFEF6972A90893c2073AB: collusion_score=0, extreme_event_count=0, \nValidator 0xc5B9d978715F081E226cb28bADB7Ba4cde5f9775: collusion_score=153863, extreme_event_count=0, \nValidator 0x94d28f08Ff81A80f4716C0a8EfC6CAC2Ec74d09E: collusion_score=209762, extreme_event_count=0, \nValidator 0xDCA5DFF3D42f2db3C18dBE823380A0A81db49A7E: collusion_score=235349, extreme_event_count=0, \nValidator 0x23b4Be9536F93b8D550214912fD0e38417Ff7209: collusion_score=491123, extreme_event_count=0, \n\n\nPlease note, collusion_score is the sum of times a validator appears in “high-matching” submissions (i.e. near-identical prices) with another validator (high collusion_score doesn’t necessarily mean malicious behaviour). extreme_event_count counts the number of times a validator posts an extreme outlier price (compared to the median price) at the same timestamp as other outliers.",
    "crumbs": [
      "Notebooks",
      "Issue 10"
    ]
  },
  {
    "objectID": "notebooks/issue_10.html#possible-security-malicious-behavior",
    "href": "notebooks/issue_10.html#possible-security-malicious-behavior",
    "title": "Issue 10",
    "section": "",
    "text": "This notebook documents the analysis for Issue #10: Possible Security / Malicious Behavior in the Autonity Oracle data. It covers:\n\nWhat is this issue about?\n\nWhy conduct this issue analysis?\n\nHow to conduct this issue analysis?\n\nWhat are the results?\n\n\n\n\nCertain validators may be attempting to manipulate or skew prices for malicious purposes, or they may be submitting collusive prices in tandem. Examples include:\n\nCollusion / Sybil Attacks: Multiple validators posting identical or near-identical prices.\nPrice Manipulation: Validators posting extreme prices simultaneously, especially at critical times.\n\nThis analysis investigates these suspicious patterns to detect possible malicious behavior.\n\n\n\n\n\nSecurity: Malicious validators can undermine Oracle reliability.\nIntegrity: Identifying suspicious patterns helps protect on-chain contracts relying on accurate price feeds.\nPreparedness: Detecting vulnerabilities prior to Mainnet launch ensures robust security practices.\n\n\n\n\n\nUse Python with the Polars library (v1.24.0) to:\n\nLoad and preprocess Oracle submission CSV files.\nParse timestamps and convert price values from Wei to decimal.\nDetect suspicious validator pairs that frequently submit identical prices.\nDetect simultaneous extreme outliers, i.e. multiple validators submitting extreme prices at the same timestamps.\n\nBelow is the full analysis script:\n\nimport polars as pl\nimport glob\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndef load_and_preprocess_submissions(submission_glob: str) -&gt; pl.DataFrame:\n    \"\"\"\n    Loads Oracle Submission CSVs and returns a Polars DataFrame.\n    \"\"\"\n    files = sorted(glob.glob(submission_glob))\n    if not files:\n        raise ValueError(f\"No CSV files found matching pattern: {submission_glob}\")\n\n    lf_list = []\n    for f in files:\n        lf_temp = pl.scan_csv(\n            f,\n            dtypes={\"Timestamp\": pl.Utf8},\n            null_values=[\"\"],\n            ignore_errors=True,\n        )\n        lf_list.append(lf_temp)\n\n    lf = pl.concat(lf_list)\n\n    lf = lf.with_columns(\n        pl.col(\"Timestamp\")\n        .str.strptime(pl.Datetime, strict=False)\n        .alias(\"Timestamp_dt\")\n    )\n\n    lf = lf.with_columns(\n        [\n            pl.col(\"Timestamp_dt\").cast(pl.Date).alias(\"date_only\"),\n            pl.col(\"Timestamp_dt\").dt.weekday().alias(\"weekday_num\"),\n        ]\n    )\n\n    df = lf.collect()\n\n    price_cols = [c for c in df.columns if c.endswith(\" Price\")]\n    for pc in price_cols:\n        if df.schema[pc] in (pl.Int64, pl.Float64):\n            df = df.with_columns(\n                (pl.col(pc).cast(pl.Float64) / 1e18).alias(pc + \" Decimal\")\n            )\n\n    return df\n\n\ndef detect_suspicious_collusion(\n    df: pl.DataFrame,\n    price_decimal_suffix: str = \"Decimal\",\n    identical_threshold: float = 1e-9,\n    min_fraction_threshold: float = 0.8,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Identifies pairs of validators that frequently submit the same (or nearly\n    the same) price values—especially.\n    \"\"\"\n    suspicious_cols = [c for c in df.columns if c.endswith(price_decimal_suffix)]\n\n    if not suspicious_cols:\n        return pl.DataFrame(\n            {\n                \"validator_a\": [],\n                \"validator_b\": [],\n                \"matching_fraction\": [],\n                \"matched_count\": [],\n                \"total_overlap\": [],\n                \"suspicious_column\": [],\n            }\n        )\n\n    results = []\n\n    for col in suspicious_cols:\n        df_col_filtered = (\n            df.lazy()\n            .filter(pl.col(col).is_not_null())\n            .select([\"Timestamp_dt\", \"Validator Address\", col])\n            .collect()\n        )\n\n        df_pivot = df_col_filtered.pivot(\n            index=\"Timestamp_dt\",\n            columns=\"Validator Address\",\n            values=col,\n        )\n\n        validator_cols = [c for c in df_pivot.columns if c != \"Timestamp_dt\"]\n\n        pairs_data = []\n        v_cols_sorted = sorted(validator_cols)\n        for i in range(len(v_cols_sorted)):\n            for j in range(i + 1, len(v_cols_sorted)):\n                va = v_cols_sorted[i]\n                vb = v_cols_sorted[j]\n                if va == \"Timestamp_dt\" or vb == \"Timestamp_dt\":\n                    continue\n\n                df_check = df_pivot.select(\n                    [\n                        (pl.col(va).is_not_null() & pl.col(vb).is_not_null()).alias(\n                            \"overlap_flag\"\n                        ),\n                        (\n                            (pl.col(va) - pl.col(vb)).abs().lt(identical_threshold)\n                            & pl.col(va).is_not_null()\n                            & pl.col(vb).is_not_null()\n                        ).alias(\"match_flag\"),\n                    ]\n                )\n\n                overlap_count = df_check[\"overlap_flag\"].sum()\n                match_count = df_check[\"match_flag\"].sum()\n\n                if overlap_count == 0:\n                    fraction = 0.0\n                else:\n                    fraction = match_count / overlap_count\n\n                pairs_data.append(\n                    {\n                        \"validator_a\": va,\n                        \"validator_b\": vb,\n                        \"matched_count\": match_count,\n                        \"total_overlap\": overlap_count,\n                        \"matching_fraction\": fraction,\n                        \"suspicious_column\": col,\n                    }\n                )\n\n        df_col_result = pl.DataFrame(pairs_data)\n\n        df_col_result = df_col_result.filter(\n            pl.col(\"matching_fraction\") &gt;= min_fraction_threshold\n        )\n\n        if df_col_result.height &gt; 0:\n            results.append(df_col_result)\n\n    if results:\n        return pl.concat(results, how=\"vertical\").sort(\n            \"matching_fraction\", descending=True\n        )\n    else:\n        return pl.DataFrame(\n            {\n                \"validator_a\": [],\n                \"validator_b\": [],\n                \"matching_fraction\": [],\n                \"matched_count\": [],\n                \"total_overlap\": [],\n                \"suspicious_column\": [],\n            }\n        )\n\n\ndef detect_simultaneous_extreme_outliers(\n    df: pl.DataFrame,\n    price_decimal_suffix: str = \"Decimal\",\n    outlier_threshold: float = 2.0,\n    min_group_size: int = 2,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Flags timestamps where multiple validators (&gt;= min_group_size) post an\n    extremely high or low (relative to some baseline) price simultaneously.\n    \"\"\"\n    suspicious_cols = [c for c in df.columns if c.endswith(price_decimal_suffix)]\n\n    if not suspicious_cols:\n        return pl.DataFrame(\n            {\n                \"Timestamp_dt\": [],\n                \"outlier_count\": [],\n                \"outlier_validators\": [],\n                \"suspicious_column\": [],\n            }\n        )\n\n    results = []\n\n    for col in suspicious_cols:\n        median_val = df.select(pl.col(col)).median().item()\n        if median_val is None or median_val &lt;= 0:\n            continue\n\n        df_extreme = (\n            df.lazy().select(\n                [\n                    \"Timestamp_dt\",\n                    \"Validator Address\",\n                    pl.col(col).alias(\"price_val\"),\n                    pl.when(\n                        (pl.col(col) &gt; (median_val * outlier_threshold))\n                        | (pl.col(col) &lt; (median_val / outlier_threshold))\n                    )\n                    .then(pl.lit(True))\n                    .otherwise(pl.lit(False))\n                    .alias(\"is_extreme\"),\n                ]\n            )\n        ).collect()\n\n        grouped_lf = (\n            df_extreme.lazy()\n            .group_by(\"Timestamp_dt\")\n            .agg(\n                [\n                    pl.sum(\"is_extreme\").alias(\"outlier_count\"),\n                    pl.col(\"Validator Address\")\n                    .filter(pl.col(\"is_extreme\"))\n                    .alias(\"outlier_validators\"),\n                ]\n            )\n        )\n\n        df_grouped = grouped_lf.collect().filter(\n            pl.col(\"outlier_count\") &gt;= min_group_size\n        )\n\n        if df_grouped.height &gt; 0:\n            df_col_sus = df_grouped.with_columns(pl.lit(col).alias(\"suspicious_column\"))\n            results.append(df_col_sus)\n\n    if results:\n        return pl.concat(results, how=\"vertical\").sort(\n            [\"Timestamp_dt\", \"suspicious_column\"]\n        )\n    else:\n        return pl.DataFrame(\n            {\n                \"Timestamp_dt\": [],\n                \"outlier_count\": [],\n                \"outlier_validators\": [],\n                \"suspicious_column\": [],\n            }\n        )\n\n\ndef analyze_possible_security_malicious_behavior(\n    submission_glob: str,\n    identical_match_threshold: float = 1e-9,\n    min_fraction_collusion: float = 0.75,\n    outlier_threshold: float = 2.0,\n    min_outlier_group_size: int = 2,\n):\n    \"\"\"\n    Main analysis function.\n    \"\"\"\n    df_all = load_and_preprocess_submissions(submission_glob)\n\n    df_suspicious_pairs = detect_suspicious_collusion(\n        df_all,\n        price_decimal_suffix=\"Decimal\",\n        identical_threshold=identical_match_threshold,\n        min_fraction_threshold=min_fraction_collusion,\n    )\n\n    df_extreme_groups = detect_simultaneous_extreme_outliers(\n        df_all,\n        price_decimal_suffix=\"Decimal\",\n        outlier_threshold=outlier_threshold,\n        min_group_size=min_outlier_group_size,\n    )\n\n    return {\n        \"df_all_submissions\": df_all,\n        \"df_suspicious_pairs\": df_suspicious_pairs,\n        \"df_extreme_outliers\": df_extreme_groups,\n    }\n\n\nresults = analyze_possible_security_malicious_behavior(\n    submission_glob=\"../submission-data/Oracle_Submission_*.csv\",\n    identical_match_threshold=1e-9,\n    min_fraction_collusion=0.75,\n    outlier_threshold=2.0,\n    min_outlier_group_size=2,\n)\n\n\n\n\n\nBelow are the results extracted from the analysis.\n\n\n\ndf_pairs = results[\"df_suspicious_pairs\"]\n\nprint(f\"Total suspicious validator pairs found: {df_pairs.height}\")\n\nif df_pairs.is_empty():\n    print(\"No suspicious validator pairs were found.\")\nelse:\n    display(df_pairs)\n\n    # Display top 20 suspicious pairs\n    for row in df_pairs.head(20).iter_rows(named=True):\n        va = row[\"validator_a\"]\n        vb = row[\"validator_b\"]\n        frac = row[\"matching_fraction\"] * 100\n        col = row[\"suspicious_column\"]\n        print(f\"Pair ({va[:10]}..., {vb[:10]}...) in {col}: {frac:.1f}% identical submissions.\")\n\nTotal suspicious validator pairs found: 3218\n\n\n\nshape: (3_218, 6)\n\n\n\nvalidator_a\nvalidator_b\nmatched_count\ntotal_overlap\nmatching_fraction\nsuspicious_column\n\n\nstr\nstr\ni64\ni64\nf64\nstr\n\n\n\n\n\"0xDCA5DFF3D42f2db3C18dBE823380…\n\"0xd61a48b0e11B0Dc6b7Bd713B1012…\n2844\n2844\n1.0\n\"AUD-USD Price Decimal\"\n\n\n\"0x8584A78A9b94f332A34BBf24D2AF…\n\"0x99E2B4B27BDe92b42D04B6CF302c…\n2864\n2864\n1.0\n\"CAD-USD Price Decimal\"\n\n\n\"0xDCA5DFF3D42f2db3C18dBE823380…\n\"0xd61a48b0e11B0Dc6b7Bd713B1012…\n2844\n2844\n1.0\n\"CAD-USD Price Decimal\"\n\n\n\"0x8584A78A9b94f332A34BBf24D2AF…\n\"0x99E2B4B27BDe92b42D04B6CF302c…\n2864\n2864\n1.0\n\"EUR-USD Price Decimal\"\n\n\n\"0xDCA5DFF3D42f2db3C18dBE823380…\n\"0xd61a48b0e11B0Dc6b7Bd713B1012…\n2844\n2844\n1.0\n\"EUR-USD Price Decimal\"\n\n\n…\n…\n…\n…\n…\n…\n\n\n\"0x6a395dE946c0493157404E2b1947…\n\"0xC1F9acAF1824F6C906b35A0D2584…\n2139\n2846\n0.751581\n\"NTN-USD Price Decimal\"\n\n\n\"0x00a96aaED75015Bb44cED878D927…\n\"0x197B2c44b887c4aC01243BDE7E4b…\n2120\n2824\n0.750708\n\"JPY-USD Price Decimal\"\n\n\n\"0x00a96aaED75015Bb44cED878D927…\n\"0x197B2c44b887c4aC01243BDE7E4b…\n2120\n2824\n0.750708\n\"SEK-USD Price Decimal\"\n\n\n\"0x1476A65D7B5739dE1805d5130441…\n\"0x984A46Ec685Bb41A7BBb2bc39f80…\n1831\n2441\n0.750102\n\"ATN-USD Price Decimal\"\n\n\n\"0x1476A65D7B5739dE1805d5130441…\n\"0x984A46Ec685Bb41A7BBb2bc39f80…\n1831\n2441\n0.750102\n\"NTN-USD Price Decimal\"\n\n\n\n\n\n\nPair (0xDCA5DFF3..., 0xd61a48b0...) in AUD-USD Price Decimal: 100.0% identical submissions.\nPair (0x8584A78A..., 0x99E2B4B2...) in CAD-USD Price Decimal: 100.0% identical submissions.\nPair (0xDCA5DFF3..., 0xd61a48b0...) in CAD-USD Price Decimal: 100.0% identical submissions.\nPair (0x8584A78A..., 0x99E2B4B2...) in EUR-USD Price Decimal: 100.0% identical submissions.\nPair (0xDCA5DFF3..., 0xd61a48b0...) in EUR-USD Price Decimal: 100.0% identical submissions.\nPair (0xDCA5DFF3..., 0xd61a48b0...) in GBP-USD Price Decimal: 100.0% identical submissions.\nPair (0x36142A4f..., 0x6a395dE9...) in JPY-USD Price Decimal: 100.0% identical submissions.\nPair (0xDCA5DFF3..., 0xd61a48b0...) in JPY-USD Price Decimal: 100.0% identical submissions.\nPair (0xDCA5DFF3..., 0xd61a48b0...) in SEK-USD Price Decimal: 100.0% identical submissions.\nPair (0x1476A65D..., 0x23b4Be95...) in NTN-ATN Price Decimal: 100.0% identical submissions.\nPair (0x1476A65D..., 0x24915749...) in NTN-ATN Price Decimal: 100.0% identical submissions.\nPair (0x1476A65D..., 0x358488a4...) in NTN-ATN Price Decimal: 100.0% identical submissions.\nPair (0x1476A65D..., 0x3AaF7817...) in NTN-ATN Price Decimal: 100.0% identical submissions.\nPair (0x1476A65D..., 0x7232e75a...) in NTN-ATN Price Decimal: 100.0% identical submissions.\nPair (0x1476A65D..., 0x94470A84...) in NTN-ATN Price Decimal: 100.0% identical submissions.\nPair (0x1476A65D..., 0xC1F9acAF...) in NTN-ATN Price Decimal: 100.0% identical submissions.\nPair (0x23b4Be95..., 0x24915749...) in NTN-ATN Price Decimal: 100.0% identical submissions.\nPair (0x23b4Be95..., 0x358488a4...) in NTN-ATN Price Decimal: 100.0% identical submissions.\nPair (0x23b4Be95..., 0x3AaF7817...) in NTN-ATN Price Decimal: 100.0% identical submissions.\nPair (0x23b4Be95..., 0x7232e75a...) in NTN-ATN Price Decimal: 100.0% identical submissions.\n\n\nInterpretation\n- High matching fractions suggest validators may be operating together or using identical sources, raising concerns of collusion or Sybil attacks.\n\n\n\n\ndf_outliers = results[\"df_extreme_outliers\"]\n\nprint(f\"Total timestamps with simultaneous extreme outliers: {df_outliers.height}\")\n\nif df_outliers.is_empty():\n    print(\"No simultaneous extreme outliers detected.\")\nelse:\n    display(df_outliers)\n\n    # Display top 20 outlier events\n    for row in df_outliers.head(20).iter_rows(named=True):\n        ts = row[\"Timestamp_dt\"]\n        count = row[\"outlier_count\"]\n        validators = row[\"outlier_validators\"]\n        col = row[\"suspicious_column\"]\n        print(f\"Timestamp {ts}: {count} validators posted outliers in {col}. Validators: {validators}\")\n\nTotal timestamps with simultaneous extreme outliers: 0\nNo simultaneous extreme outliers detected.\n\n\n\nMultiple validators simultaneously posting extreme prices may indicate coordinated manipulation attempts, especially if this coincides with critical network events.\n\nList of all Validators and their Collusion and Extreme Event Counts\n\nall_validators = (\n    results[\"df_all_submissions\"]\n    .select(pl.col(\"Validator Address\"))\n    .unique()\n    .rename({\"Validator Address\": \"validator\"})\n)\n\ndf_pairs = results[\"df_suspicious_pairs\"]\n\nif not df_pairs.is_empty():\n    collusion_a = (\n        df_pairs\n        .select([\"validator_a\", \"matched_count\"])\n        .group_by(\"validator_a\")\n        .agg(pl.sum(\"matched_count\").alias(\"collusion_score_a\"))\n        .rename({\"validator_a\": \"validator\"})\n    )\n\n    collusion_b = (\n        df_pairs\n        .select([\"validator_b\", \"matched_count\"])\n        .group_by(\"validator_b\")\n        .agg(pl.sum(\"matched_count\").alias(\"collusion_score_b\"))\n        .rename({\"validator_b\": \"validator\"})\n    )\n\n    collusion_counts = (\n        collusion_a\n        .join(collusion_b, on=\"validator\", how=\"outer\")\n        .with_columns(\n            (\n                pl.col(\"collusion_score_a\").fill_null(0) \n                + pl.col(\"collusion_score_b\").fill_null(0)\n            ).alias(\"collusion_score\")\n        )\n        .select([\"validator\", \"collusion_score\"])\n    )\nelse:\n    collusion_counts = pl.DataFrame(\n        schema={\"validator\": pl.Utf8, \"collusion_score\": pl.Int64}\n    )\n\ndf_outliers = results[\"df_extreme_outliers\"]\n\nif not df_outliers.is_empty():\n    df_outliers_exploded = df_outliers.explode(\"outlier_validators\")\n\n    outlier_counts = (\n        df_outliers_exploded\n        .group_by(\"outlier_validators\")\n        .agg(pl.count().alias(\"extreme_event_count\"))\n        .rename({\"outlier_validators\": \"validator\"})\n    )\nelse:\n    outlier_counts = pl.DataFrame(\n        schema={\"validator\": pl.Utf8, \"extreme_event_count\": pl.Int64}\n    )\n\ndf_validator_summary = (\n    all_validators\n    .join(collusion_counts, on=\"validator\", how=\"left\")\n    .join(outlier_counts, on=\"validator\", how=\"left\")\n    .with_columns(\n        [\n            pl.col(\"collusion_score\").fill_null(0),\n            pl.col(\"extreme_event_count\").fill_null(0),\n        ]\n    )\n    .with_columns(\n        (pl.col(\"collusion_score\") + pl.col(\"extreme_event_count\"))\n        .alias(\"aggregated_score\")\n    )\n    .sort(by=\"extreme_event_count\", descending=True)\n)\n\nfor row in df_validator_summary.to_dicts():\n    print(\n        f\"Validator {row['validator']}: \"\n        f\"collusion_score={row['collusion_score']}, \"\n        f\"extreme_event_count={row['extreme_event_count']}, \"\n    )\n\nValidator 0x5E17e837DcBa2728C94f95c38fA8a47CB9C8818F: collusion_score=129756, extreme_event_count=0, \nValidator 0x99E2B4B27BDe92b42D04B6CF302cF564D2C13b74: collusion_score=474593, extreme_event_count=0, \nValidator 0xBE287C82A786218E008FF97320b08244BE4A282c: collusion_score=81266, extreme_event_count=0, \nValidator 0xfD97FB8835d25740A2Da27c69762D74F6A931858: collusion_score=0, extreme_event_count=0, \nValidator 0x3AaF7817618728ffEF81898E11A3171C33faAE41: collusion_score=191682, extreme_event_count=0, \nValidator 0x19E356ebC20283fc74AF0BA4C179502A1F62fA7B: collusion_score=293853, extreme_event_count=0, \nValidator 0x3597d2D42f8Fbbc82E8b1046048773aD6DDB717E: collusion_score=267909, extreme_event_count=0, \nValidator 0x3fe573552E14a0FC11Da25E43Fef11e16a785068: collusion_score=0, extreme_event_count=0, \nValidator 0x6747c02DE7eb2099265e55715Ba2E03e8563D051: collusion_score=347841, extreme_event_count=0, \nValidator 0xd625d50B0d087861c286d726eC51Cf4Bd9c54357: collusion_score=0, extreme_event_count=0, \nValidator 0xD9fDab408dF7Ae751691BeC2efE3b713ba3f9C36: collusion_score=377924, extreme_event_count=0, \nValidator 0xBBf36374eb23968F25aecAEbb97BF3118f3c2fEC: collusion_score=296645, extreme_event_count=0, \nValidator 0x5603caFE3313D0cf56Fd4bE4A2f606dD6E43F8Eb: collusion_score=387994, extreme_event_count=0, \nValidator 0x00a96aaED75015Bb44cED878D927dcb15ec1FF54: collusion_score=362523, extreme_event_count=0, \nValidator 0x2928FE5b911BCAf837cAd93eB9626E86a189f1dd: collusion_score=469769, extreme_event_count=0, \nValidator 0x197B2c44b887c4aC01243BDE7E4bBa8bd95BC3a8: collusion_score=410755, extreme_event_count=0, \nValidator 0xbfDcAF35f52F9ef423ac8F2621F9eef8be6dEd17: collusion_score=370736, extreme_event_count=0, \nValidator 0x718361fc3637199F24a2437331677D6B89a40519: collusion_score=198477, extreme_event_count=0, \nValidator 0x383A3c437d3F12f60E5fC990119468D3561EfBfc: collusion_score=271531, extreme_event_count=0, \nValidator 0xC1F9acAF1824F6C906b35A0D2584D6E25077C7f5: collusion_score=462504, extreme_event_count=0, \nValidator 0x551f3300FCFE0e392178b3542c009948008B2a9F: collusion_score=121079, extreme_event_count=0, \nValidator 0x1Be7f70BCf8393a7e4A5BcC66F6f15d6e35cfBBC: collusion_score=234904, extreme_event_count=0, \nValidator 0xB5d8be2AB4b6d7E6be7Ea28E91b370223a06289f: collusion_score=345889, extreme_event_count=0, \nValidator 0x9d28e40E9Ec4789f9A0D17e421F76D8D0868EA44: collusion_score=120326, extreme_event_count=0, \nValidator 0x6a395dE946c0493157404E2b1947493c633f569E: collusion_score=230559, extreme_event_count=0, \nValidator 0xF9B38D02959379d43C764064dE201324d5e12931: collusion_score=99775, extreme_event_count=0, \nValidator 0x36142A4f36974e2935192A1111C39330aA296D3C: collusion_score=198115, extreme_event_count=0, \nValidator 0x64F83c2538A646A550Ad9bEEb63427a377359DEE: collusion_score=485482, extreme_event_count=0, \nValidator 0x527192F3D2408C84087607b7feE1d0f907821E17: collusion_score=237648, extreme_event_count=0, \nValidator 0x831B837C3DA1B6c2AB68a690206bDfF368877E19: collusion_score=193907, extreme_event_count=0, \nValidator 0xE4686A4C6E63A8ab51B458c52EB779AEcf0B74f7: collusion_score=237330, extreme_event_count=0, \nValidator 0xcdEed21b471b0Dc54faF74480A0E700fCc42a7b6: collusion_score=360901, extreme_event_count=0, \nValidator 0x01F788E4371a70D579C178Ea7F48E04e8B2CD743: collusion_score=370885, extreme_event_count=0, \nValidator 0x9C7dAABb5101623340C925CFD6fF74088ff5672e: collusion_score=352372, extreme_event_count=0, \nValidator 0xDF2D0052ea56A860443039619f6DAe4434bc0Ac4: collusion_score=252893, extreme_event_count=0, \nValidator 0x100E38f7BCEc53937BDd79ADE46F34362470577B: collusion_score=0, extreme_event_count=0, \nValidator 0x24915749B793375a8C93090AF19928aFF1CAEcb6: collusion_score=492750, extreme_event_count=0, \nValidator 0x22A76e194A49c9e5508Cd4A3E1cD555D088ECB08: collusion_score=134674, extreme_event_count=0, \nValidator 0xd61a48b0e11B0Dc6b7Bd713B1012563c52591BAA: collusion_score=0, extreme_event_count=0, \nValidator 0xcf716b3930d7cf6f2ADAD90A27c39fDc9D643BBd: collusion_score=459177, extreme_event_count=0, \nValidator 0x26E2724dBD14Fbd52be430B97043AA4c83F05852: collusion_score=0, extreme_event_count=0, \nValidator 0x59031767f20EA8F4a3d90d33aB0DAA2ca469Fd9a: collusion_score=370536, extreme_event_count=0, \nValidator 0x791A7F840ac11841cCB0FaA968B2e3a0Db930fCe: collusion_score=491488, extreme_event_count=0, \nValidator 0xf34CD6c09a59d7D3d1a6C3dC231a7834E5615D6A: collusion_score=352975, extreme_event_count=0, \nValidator 0x984A46Ec685Bb41A7BBb2bc39f80C78410ff4057: collusion_score=497570, extreme_event_count=0, \nValidator 0xf10f56Bf0A28E0737c7e6bB0aF92f3DDad34aE6a: collusion_score=489844, extreme_event_count=0, \nValidator 0xE9FFF86CAdC3136b3D94948B8Fd23631EDaa2dE3: collusion_score=36576, extreme_event_count=0, \nValidator 0xEf0Ba5e345C2C3937df5667A870Aae5105CAa3a5: collusion_score=233435, extreme_event_count=0, \nValidator 0x358488a4EdCA493FCD87610dcd50c62c8A3Dd658: collusion_score=479593, extreme_event_count=0, \nValidator 0x1476A65D7B5739dE1805d5130441A94022Ee49fe: collusion_score=122800, extreme_event_count=0, \nValidator 0x4cD134001EEF0843B9c69Ba9569d11fDcF4bd495: collusion_score=193803, extreme_event_count=0, \nValidator 0x8f91e0ADF8065C3fFF92297267E02DF32C2978FF: collusion_score=198999, extreme_event_count=0, \nValidator 0x94470A842Ea4f44e668EB9C2AB81367b6Ce01772: collusion_score=149343, extreme_event_count=0, \nValidator 0x7232e75a8bFd8c9ab002BB3A00eAa885BC72A6dd: collusion_score=171506, extreme_event_count=0, \nValidator 0x8584A78A9b94f332A34BBf24D2AF83367Da31894: collusion_score=375412, extreme_event_count=0, \nValidator 0xdF239e0D5b4E6e820B0cFEF6972A90893c2073AB: collusion_score=0, extreme_event_count=0, \nValidator 0xc5B9d978715F081E226cb28bADB7Ba4cde5f9775: collusion_score=153863, extreme_event_count=0, \nValidator 0x94d28f08Ff81A80f4716C0a8EfC6CAC2Ec74d09E: collusion_score=209762, extreme_event_count=0, \nValidator 0xDCA5DFF3D42f2db3C18dBE823380A0A81db49A7E: collusion_score=235349, extreme_event_count=0, \nValidator 0x23b4Be9536F93b8D550214912fD0e38417Ff7209: collusion_score=491123, extreme_event_count=0, \n\n\nPlease note, collusion_score is the sum of times a validator appears in “high-matching” submissions (i.e. near-identical prices) with another validator (high collusion_score doesn’t necessarily mean malicious behaviour). extreme_event_count counts the number of times a validator posts an extreme outlier price (compared to the median price) at the same timestamp as other outliers.",
    "crumbs": [
      "Notebooks",
      "Issue 10"
    ]
  },
  {
    "objectID": "notebooks/issue_6.html",
    "href": "notebooks/issue_6.html",
    "title": "Issue 6",
    "section": "",
    "text": "This notebook documents the analysis for Issue #6: Cross-Rate Inconsistency in the Autonity Oracle data. It covers:\n\nWhat is this issue about?\n\nWhy conduct this issue analysis?\n\nHow to conduct this issue analysis?\n\nWhat are the results?\n\n\n\n\nIn the Issues Table, Issue #6 describes Cross-Rate Inconsistency:\n&gt; “NTN-USD * ATN-USD ≠ NTN-ATN (when scaling properly).”\nThis implies that if NTN-ATN = X and ATN-USD = Y, expect NTN-USD ≈ X * Y. Significant mismatches indicate potential errors or data inconsistencies.\n\n\n\n\n\nData Reliability: Ensuring consistent data across pairs is critical.\nTrust: Maintaining accuracy and confidence in the Oracle.\nDebugging: Identifying systemic or validator-specific issues.\n\n\n\n\n\nUse Python with the Polars library (v1.24.0) to:\n\nLoad and preprocess Oracle submission CSV files.\nConvert Wei-based prices to decimal.\nCalculate NTN-USD_estimated = NTN-ATN * ATN-USD.\nFlag rows exceeding a threshold (10%).\nSummarize by date and validator.\n\nBelow is the Python code for the analysis:\n\nimport polars as pl\nimport glob\nimport math\nfrom typing import Optional\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndef load_and_preprocess_submissions(submission_glob: str) -&gt; pl.DataFrame:\n    \"\"\"\n    Loads Oracle Submission CSVs and returns a Polars DataFrame.\n    \"\"\"\n    files = sorted(glob.glob(submission_glob))\n    if not files:\n        raise ValueError(f\"No CSV files found matching pattern {submission_glob}\")\n\n    lf_list = []\n    for f in files:\n        lf_temp = pl.scan_csv(\n            f,\n            dtypes={\"Timestamp\": pl.Utf8},\n            null_values=[\"\"],\n            ignore_errors=True,\n        )\n        lf_list.append(lf_temp)\n\n    lf = pl.concat(lf_list)\n\n    lf = lf.with_columns(\n        pl.col(\"Timestamp\")\n        .str.strptime(pl.Datetime, strict=False)\n        .alias(\"Timestamp_dt\")\n    )\n\n    lf = lf.with_columns(\n        [\n            pl.col(\"Timestamp_dt\").cast(pl.Date).alias(\"date_only\"),\n            pl.col(\"Timestamp_dt\")\n            .dt.weekday()\n            .alias(\"weekday_num\"),\n        ]\n    )\n\n    df = lf.collect()\n    return df\n\n\ndef convert_wei_to_decimal(price_wei: Optional[float]) -&gt; Optional[float]:\n    \"\"\"\n    Converts a Wei-based price to a normal float decimal.\n    \"\"\"\n    if price_wei is None or math.isnan(price_wei):\n        return None\n    return price_wei / 1e18\n\n\ndef check_cross_rate_inconsistency(\n    df: pl.DataFrame,\n    atn_usd_col: str = \"ATN-USD Price\",\n    ntn_usd_col: str = \"NTN-USD Price\",\n    ntn_atn_col: str = \"NTN-ATN Price\",\n    threshold: float = 0.01,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Computes cross-rate mismatch and measures relative differences.\n    \"\"\"\n    df_local = df.clone()\n    decimal_cols = []\n    for col in [atn_usd_col, ntn_usd_col, ntn_atn_col]:\n        col_decimal = col + \" Decimal\"\n        df_local = df_local.with_columns(\n            (pl.col(col).cast(pl.Float64) / 1e18).alias(col_decimal)\n        )\n        decimal_cols.append(col_decimal)\n\n    atn_usd_dec = atn_usd_col + \" Decimal\"\n    ntn_usd_dec = ntn_usd_col + \" Decimal\"\n    ntn_atn_dec = ntn_atn_col + \" Decimal\"\n\n    df_local = df_local.with_columns(\n        [(pl.col(ntn_atn_dec) * pl.col(atn_usd_dec)).alias(\"ntn_usd_estimated\")]\n    )\n\n    epsilon = 1e-18\n    df_local = df_local.with_columns(\n        [\n            (\n                (pl.col(\"ntn_usd_estimated\") - pl.col(ntn_usd_dec)).abs()\n                / (pl.col(ntn_usd_dec).abs() + epsilon)\n            ).alias(\"rel_diff_cross\")\n        ]\n    )\n\n    df_local = df_local.with_columns(\n        [\n            pl.when(pl.col(\"rel_diff_cross\") &gt; threshold)\n            .then(pl.lit(f\"Cross-rate mismatch &gt; {int(threshold*100)}%\"))\n            .otherwise(pl.lit(\"\"))\n            .alias(\"suspect_reason\")\n        ]\n    )\n\n    df_flagged = df_local.filter(pl.col(\"suspect_reason\") != \"\")\n\n    keep_cols = [\n        \"Timestamp_dt\",\n        \"Validator Address\",\n        atn_usd_dec,\n        ntn_usd_dec,\n        ntn_atn_dec,\n        \"ntn_usd_estimated\",\n        \"rel_diff_cross\",\n        \"suspect_reason\",\n    ]\n\n    optional_cols = []\n    for c in [\"date_only\", \"weekday_num\"]:\n        if c in df_flagged.columns:\n            optional_cols.append(c)\n\n    df_flagged = df_flagged.select(keep_cols + optional_cols)\n    return df_flagged\n\n\ndef summarize_cross_rate_inconsistency(df_flagged: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"\n    Summarizes the number of cross-rate mismatches.\n    \"\"\"\n    if df_flagged.is_empty():\n        return pl.DataFrame(\n            {\n                \"date_only\": [],\n                \"Validator Address\": [],\n                \"mismatch_count\": [],\n                \"avg_rel_diff\": [],\n                \"max_rel_diff\": [],\n            }\n        )\n\n    lf = df_flagged.lazy()\n    summary_lf = (\n        lf.group_by([\"date_only\", \"Validator Address\"])\n        .agg(\n            [\n                pl.count(\"rel_diff_cross\").alias(\"mismatch_count\"),\n                pl.mean(\"rel_diff_cross\").alias(\"avg_rel_diff\"),\n                pl.max(\"rel_diff_cross\").alias(\"max_rel_diff\"),\n            ]\n        )\n        .sort([\"date_only\", \"Validator Address\"])\n    )\n    return summary_lf.collect()\n\n\ndef analyze_cross_rate_inconsistency(\n    submission_glob: str,\n    atn_usd_col: str = \"ATN-USD Price\",\n    ntn_usd_col: str = \"NTN-USD Price\",\n    ntn_atn_col: str = \"NTN-ATN Price\",\n    threshold: float = 0.01,\n):\n    \"\"\"\n    Main analysis function.\n    \"\"\"\n    df_all = load_and_preprocess_submissions(submission_glob)\n\n    df_flagged = check_cross_rate_inconsistency(\n        df_all,\n        atn_usd_col=atn_usd_col,\n        ntn_usd_col=ntn_usd_col,\n        ntn_atn_col=ntn_atn_col,\n        threshold=threshold,\n    )\n\n    df_summary = summarize_cross_rate_inconsistency(df_flagged)\n\n    return {\n        \"df_flagged\": df_flagged,\n        \"df_summary\": df_summary,\n    }\n\n\nresults = analyze_cross_rate_inconsistency(\n    submission_glob=\"../submission-data/Oracle_Submission_*.csv\",\n    atn_usd_col=\"ATN-USD Price\",\n    ntn_usd_col=\"NTN-USD Price\",\n    ntn_atn_col=\"NTN-ATN Price\",\n    threshold=0.01,  # 1% mismatch threshold\n)\n\n\n\n\n\nBelow presents the results from the analysis:\nFlagged Submissions (Cross-rate mismatch &gt; 10%)\n\ndf_flagged = results[\"df_flagged\"]\nif df_flagged.is_empty():\n    print(\"No cross-rate inconsistencies found beyond 10% threshold.\")\nelse:\n    print(f\"Total flagged cross-rate mismatches: {df_flagged.height}\")\n    df_flagged\n\nNo cross-rate inconsistencies found beyond 10% threshold.\n\n\n\nTimestamp_dt and Validator Address indicate problematic submissions.\nPrices (ATN-USD, NTN-USD, NTN-ATN) and estimated NTN-USD.\nrel_diff_cross shows percentage mismatch.\n\nDaily Validator Summary\n\ndf_summary = results[\"df_summary\"]\nif df_summary.is_empty():\n    print(\"No daily cross-rate mismatches to summarize.\")\nelse:\n    print(f\"Number of daily mismatch records: {df_summary.height}\")\n    df_summary\n\nNo daily cross-rate mismatches to summarize.\n\n\n\nmismatch_count: Number of daily flagged submissions per validator.\navg_rel_diff and max_rel_diff: Mean and maximum relative mismatch percentages.\n\n\nCombine both findings\n\ntotal_flagged = df_flagged.height\nif total_flagged == 0:\n    print(\"Data consistency is good. No critical cross-rate mismatches identified.\")\nelse:\n    num_validators = df_summary.get_column(\"Validator Address\").n_unique()\n    print(f\"{total_flagged} total mismatches across {num_validators} validators found. Investigate further:\")\n    high_impact = df_summary.filter(pl.col(\"max_rel_diff\") &gt; 0.5)\n    if high_impact.is_empty():\n        print(\"No extreme mismatches (&gt;50%) identified. Likely minor synchronization issues or rounding errors.\")\n    else:\n        print(f\"Validators with extreme mismatches (&gt;50%): {high_impact.height}\")\n        high_impact\n\nData consistency is good. No critical cross-rate mismatches identified.\n\n\n\nIf no mismatches, system appears healthy.\nSmall mismatches (&lt;50%) often reflect minor timing or rounding discrepancies.\nExtreme mismatches (&gt;50%) require thorough validator-specific investigation:\n\nVerify decimal conversions.\nConfirm synchronized data updates.\nReview validator configurations or potential malicious behavior.\n\n\nList of all Validators and their Consistency Indices\n\ndf_all = load_and_preprocess_submissions(\"../submission-data/Oracle_Submission_*.csv\")\n\ntotal_stats = (\n    df_all.lazy()\n    .group_by(\"Validator Address\")\n    .agg(pl.count().alias(\"total_submissions\"))\n    .collect()\n)\n\nflagged_stats = (\n    df_flagged.lazy()\n    .group_by(\"Validator Address\")\n    .agg(\n        pl.count().alias(\"flagged_count\"),\n        pl.mean(\"rel_diff_cross\").alias(\"mean_rel_diff\"),\n        pl.max(\"rel_diff_cross\").alias(\"max_rel_diff\"),\n    )\n    .collect()\n)\n\nvalidator_stats = (\n    total_stats\n    .join(flagged_stats, on=\"Validator Address\", how=\"left\")\n    .fill_null(0)\n)\n\nvalidator_stats = (\n    validator_stats\n    .with_columns(\n        (\n            (pl.col(\"flagged_count\") / pl.col(\"total_submissions\")) *\n            (pl.col(\"mean_rel_diff\") + pl.col(\"max_rel_diff\"))\n        ).alias(\"mismatch_penalty_rate\")\n    )\n    .with_columns(\n        (1 / (1 + pl.col(\"mismatch_penalty_rate\")))\n        .alias(\"consistency_index\")\n    )\n    .sort(\"consistency_index\", descending=False)\n)\n\nprint(\"Validator Consistency Ranking (higher index = better)\\n\")\nfor row in validator_stats.to_dicts():\n    print(\n        f\"Validator {row['Validator Address']}: \"\n        f\"subs={row['total_submissions']:5d}, \"\n        f\"flags={row['flagged_count']:3d}, \"\n        f\"index={row['consistency_index']:.4f}\"\n    )\n\nValidator Consistency Ranking (higher index = better)\n\nValidator 0x383A3c437d3F12f60E5fC990119468D3561EfBfc: subs= 2880, flags=  0, index=1.0000\nValidator 0xDCA5DFF3D42f2db3C18dBE823380A0A81db49A7E: subs= 2880, flags=  0, index=1.0000\nValidator 0xcf716b3930d7cf6f2ADAD90A27c39fDc9D643BBd: subs= 2880, flags=  0, index=1.0000\nValidator 0xbfDcAF35f52F9ef423ac8F2621F9eef8be6dEd17: subs= 2833, flags=  0, index=1.0000\nValidator 0x1Be7f70BCf8393a7e4A5BcC66F6f15d6e35cfBBC: subs= 2880, flags=  0, index=1.0000\nValidator 0x64F83c2538A646A550Ad9bEEb63427a377359DEE: subs= 2880, flags=  0, index=1.0000\nValidator 0xB5d8be2AB4b6d7E6be7Ea28E91b370223a06289f: subs= 2880, flags=  0, index=1.0000\nValidator 0x5603caFE3313D0cf56Fd4bE4A2f606dD6E43F8Eb: subs= 2880, flags=  0, index=1.0000\nValidator 0xdF239e0D5b4E6e820B0cFEF6972A90893c2073AB: subs= 2880, flags=  0, index=1.0000\nValidator 0xBE287C82A786218E008FF97320b08244BE4A282c: subs= 2879, flags=  0, index=1.0000\nValidator 0x5E17e837DcBa2728C94f95c38fA8a47CB9C8818F: subs= 2876, flags=  0, index=1.0000\nValidator 0x551f3300FCFE0e392178b3542c009948008B2a9F: subs= 2880, flags=  0, index=1.0000\nValidator 0xE4686A4C6E63A8ab51B458c52EB779AEcf0B74f7: subs= 2880, flags=  0, index=1.0000\nValidator 0x100E38f7BCEc53937BDd79ADE46F34362470577B: subs= 2876, flags=  0, index=1.0000\nValidator 0xF9B38D02959379d43C764064dE201324d5e12931: subs= 2880, flags=  0, index=1.0000\nValidator 0x3fe573552E14a0FC11Da25E43Fef11e16a785068: subs= 2880, flags=  0, index=1.0000\nValidator 0x718361fc3637199F24a2437331677D6B89a40519: subs= 2880, flags=  0, index=1.0000\nValidator 0x358488a4EdCA493FCD87610dcd50c62c8A3Dd658: subs= 2880, flags=  0, index=1.0000\nValidator 0x527192F3D2408C84087607b7feE1d0f907821E17: subs= 2880, flags=  0, index=1.0000\nValidator 0x9C7dAABb5101623340C925CFD6fF74088ff5672e: subs= 2880, flags=  0, index=1.0000\nValidator 0x6a395dE946c0493157404E2b1947493c633f569E: subs= 2874, flags=  0, index=1.0000\nValidator 0x2928FE5b911BCAf837cAd93eB9626E86a189f1dd: subs= 2829, flags=  0, index=1.0000\nValidator 0x9d28e40E9Ec4789f9A0D17e421F76D8D0868EA44: subs= 2880, flags=  0, index=1.0000\nValidator 0x01F788E4371a70D579C178Ea7F48E04e8B2CD743: subs= 2837, flags=  0, index=1.0000\nValidator 0xDF2D0052ea56A860443039619f6DAe4434bc0Ac4: subs= 2879, flags=  0, index=1.0000\nValidator 0x8f91e0ADF8065C3fFF92297267E02DF32C2978FF: subs= 2879, flags=  0, index=1.0000\nValidator 0x3597d2D42f8Fbbc82E8b1046048773aD6DDB717E: subs= 2880, flags=  0, index=1.0000\nValidator 0x7232e75a8bFd8c9ab002BB3A00eAa885BC72A6dd: subs= 2877, flags=  0, index=1.0000\nValidator 0x8584A78A9b94f332A34BBf24D2AF83367Da31894: subs= 2879, flags=  0, index=1.0000\nValidator 0xc5B9d978715F081E226cb28bADB7Ba4cde5f9775: subs= 2879, flags=  0, index=1.0000\nValidator 0xd625d50B0d087861c286d726eC51Cf4Bd9c54357: subs= 2856, flags=  0, index=1.0000\nValidator 0x00a96aaED75015Bb44cED878D927dcb15ec1FF54: subs= 2866, flags=  0, index=1.0000\nValidator 0x197B2c44b887c4aC01243BDE7E4bBa8bd95BC3a8: subs= 2880, flags=  0, index=1.0000\nValidator 0x24915749B793375a8C93090AF19928aFF1CAEcb6: subs= 2880, flags=  0, index=1.0000\nValidator 0xcdEed21b471b0Dc54faF74480A0E700fCc42a7b6: subs= 2880, flags=  0, index=1.0000\nValidator 0x19E356ebC20283fc74AF0BA4C179502A1F62fA7B: subs= 2833, flags=  0, index=1.0000\nValidator 0x831B837C3DA1B6c2AB68a690206bDfF368877E19: subs= 2863, flags=  0, index=1.0000\nValidator 0x36142A4f36974e2935192A1111C39330aA296D3C: subs= 2880, flags=  0, index=1.0000\nValidator 0xfD97FB8835d25740A2Da27c69762D74F6A931858: subs= 2880, flags=  0, index=1.0000\nValidator 0x94d28f08Ff81A80f4716C0a8EfC6CAC2Ec74d09E: subs= 2880, flags=  0, index=1.0000\nValidator 0xD9fDab408dF7Ae751691BeC2efE3b713ba3f9C36: subs= 2880, flags=  0, index=1.0000\nValidator 0xE9FFF86CAdC3136b3D94948B8Fd23631EDaa2dE3: subs= 2880, flags=  0, index=1.0000\nValidator 0xBBf36374eb23968F25aecAEbb97BF3118f3c2fEC: subs= 2880, flags=  0, index=1.0000\nValidator 0xf34CD6c09a59d7D3d1a6C3dC231a7834E5615D6A: subs= 2834, flags=  0, index=1.0000\nValidator 0x3AaF7817618728ffEF81898E11A3171C33faAE41: subs= 2874, flags=  0, index=1.0000\nValidator 0x94470A842Ea4f44e668EB9C2AB81367b6Ce01772: subs= 2880, flags=  0, index=1.0000\nValidator 0x4cD134001EEF0843B9c69Ba9569d11fDcF4bd495: subs= 2823, flags=  0, index=1.0000\nValidator 0x984A46Ec685Bb41A7BBb2bc39f80C78410ff4057: subs= 2877, flags=  0, index=1.0000\nValidator 0x22A76e194A49c9e5508Cd4A3E1cD555D088ECB08: subs= 2880, flags=  0, index=1.0000\nValidator 0x26E2724dBD14Fbd52be430B97043AA4c83F05852: subs= 2880, flags=  0, index=1.0000\nValidator 0x791A7F840ac11841cCB0FaA968B2e3a0Db930fCe: subs= 2880, flags=  0, index=1.0000\nValidator 0x6747c02DE7eb2099265e55715Ba2E03e8563D051: subs= 2840, flags=  0, index=1.0000\nValidator 0xEf0Ba5e345C2C3937df5667A870Aae5105CAa3a5: subs= 2879, flags=  0, index=1.0000\nValidator 0xC1F9acAF1824F6C906b35A0D2584D6E25077C7f5: subs= 2880, flags=  0, index=1.0000\nValidator 0x1476A65D7B5739dE1805d5130441A94022Ee49fe: subs= 2462, flags=  0, index=1.0000\nValidator 0xf10f56Bf0A28E0737c7e6bB0aF92f3DDad34aE6a: subs= 2880, flags=  0, index=1.0000\nValidator 0x99E2B4B27BDe92b42D04B6CF302cF564D2C13b74: subs= 2880, flags=  0, index=1.0000\nValidator 0xd61a48b0e11B0Dc6b7Bd713B1012563c52591BAA: subs= 2873, flags=  0, index=1.0000\nValidator 0x59031767f20EA8F4a3d90d33aB0DAA2ca469Fd9a: subs= 2880, flags=  0, index=1.0000\nValidator 0x23b4Be9536F93b8D550214912fD0e38417Ff7209: subs= 2880, flags=  0, index=1.0000\n\n\nPlease note, total_submissions represents the total number of submissions for this validator. flagged_count is how many of those were caught by the 1 % cross-rate check. consistency_index is a 0-to-1 reliability score (1 = no mismatches; lower values indicate more frequent or larger inconsistencies).",
    "crumbs": [
      "Notebooks",
      "Issue 6"
    ]
  },
  {
    "objectID": "notebooks/issue_6.html#cross-rate-inconsistency",
    "href": "notebooks/issue_6.html#cross-rate-inconsistency",
    "title": "Issue 6",
    "section": "",
    "text": "This notebook documents the analysis for Issue #6: Cross-Rate Inconsistency in the Autonity Oracle data. It covers:\n\nWhat is this issue about?\n\nWhy conduct this issue analysis?\n\nHow to conduct this issue analysis?\n\nWhat are the results?\n\n\n\n\nIn the Issues Table, Issue #6 describes Cross-Rate Inconsistency:\n&gt; “NTN-USD * ATN-USD ≠ NTN-ATN (when scaling properly).”\nThis implies that if NTN-ATN = X and ATN-USD = Y, expect NTN-USD ≈ X * Y. Significant mismatches indicate potential errors or data inconsistencies.\n\n\n\n\n\nData Reliability: Ensuring consistent data across pairs is critical.\nTrust: Maintaining accuracy and confidence in the Oracle.\nDebugging: Identifying systemic or validator-specific issues.\n\n\n\n\n\nUse Python with the Polars library (v1.24.0) to:\n\nLoad and preprocess Oracle submission CSV files.\nConvert Wei-based prices to decimal.\nCalculate NTN-USD_estimated = NTN-ATN * ATN-USD.\nFlag rows exceeding a threshold (10%).\nSummarize by date and validator.\n\nBelow is the Python code for the analysis:\n\nimport polars as pl\nimport glob\nimport math\nfrom typing import Optional\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndef load_and_preprocess_submissions(submission_glob: str) -&gt; pl.DataFrame:\n    \"\"\"\n    Loads Oracle Submission CSVs and returns a Polars DataFrame.\n    \"\"\"\n    files = sorted(glob.glob(submission_glob))\n    if not files:\n        raise ValueError(f\"No CSV files found matching pattern {submission_glob}\")\n\n    lf_list = []\n    for f in files:\n        lf_temp = pl.scan_csv(\n            f,\n            dtypes={\"Timestamp\": pl.Utf8},\n            null_values=[\"\"],\n            ignore_errors=True,\n        )\n        lf_list.append(lf_temp)\n\n    lf = pl.concat(lf_list)\n\n    lf = lf.with_columns(\n        pl.col(\"Timestamp\")\n        .str.strptime(pl.Datetime, strict=False)\n        .alias(\"Timestamp_dt\")\n    )\n\n    lf = lf.with_columns(\n        [\n            pl.col(\"Timestamp_dt\").cast(pl.Date).alias(\"date_only\"),\n            pl.col(\"Timestamp_dt\")\n            .dt.weekday()\n            .alias(\"weekday_num\"),\n        ]\n    )\n\n    df = lf.collect()\n    return df\n\n\ndef convert_wei_to_decimal(price_wei: Optional[float]) -&gt; Optional[float]:\n    \"\"\"\n    Converts a Wei-based price to a normal float decimal.\n    \"\"\"\n    if price_wei is None or math.isnan(price_wei):\n        return None\n    return price_wei / 1e18\n\n\ndef check_cross_rate_inconsistency(\n    df: pl.DataFrame,\n    atn_usd_col: str = \"ATN-USD Price\",\n    ntn_usd_col: str = \"NTN-USD Price\",\n    ntn_atn_col: str = \"NTN-ATN Price\",\n    threshold: float = 0.01,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Computes cross-rate mismatch and measures relative differences.\n    \"\"\"\n    df_local = df.clone()\n    decimal_cols = []\n    for col in [atn_usd_col, ntn_usd_col, ntn_atn_col]:\n        col_decimal = col + \" Decimal\"\n        df_local = df_local.with_columns(\n            (pl.col(col).cast(pl.Float64) / 1e18).alias(col_decimal)\n        )\n        decimal_cols.append(col_decimal)\n\n    atn_usd_dec = atn_usd_col + \" Decimal\"\n    ntn_usd_dec = ntn_usd_col + \" Decimal\"\n    ntn_atn_dec = ntn_atn_col + \" Decimal\"\n\n    df_local = df_local.with_columns(\n        [(pl.col(ntn_atn_dec) * pl.col(atn_usd_dec)).alias(\"ntn_usd_estimated\")]\n    )\n\n    epsilon = 1e-18\n    df_local = df_local.with_columns(\n        [\n            (\n                (pl.col(\"ntn_usd_estimated\") - pl.col(ntn_usd_dec)).abs()\n                / (pl.col(ntn_usd_dec).abs() + epsilon)\n            ).alias(\"rel_diff_cross\")\n        ]\n    )\n\n    df_local = df_local.with_columns(\n        [\n            pl.when(pl.col(\"rel_diff_cross\") &gt; threshold)\n            .then(pl.lit(f\"Cross-rate mismatch &gt; {int(threshold*100)}%\"))\n            .otherwise(pl.lit(\"\"))\n            .alias(\"suspect_reason\")\n        ]\n    )\n\n    df_flagged = df_local.filter(pl.col(\"suspect_reason\") != \"\")\n\n    keep_cols = [\n        \"Timestamp_dt\",\n        \"Validator Address\",\n        atn_usd_dec,\n        ntn_usd_dec,\n        ntn_atn_dec,\n        \"ntn_usd_estimated\",\n        \"rel_diff_cross\",\n        \"suspect_reason\",\n    ]\n\n    optional_cols = []\n    for c in [\"date_only\", \"weekday_num\"]:\n        if c in df_flagged.columns:\n            optional_cols.append(c)\n\n    df_flagged = df_flagged.select(keep_cols + optional_cols)\n    return df_flagged\n\n\ndef summarize_cross_rate_inconsistency(df_flagged: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"\n    Summarizes the number of cross-rate mismatches.\n    \"\"\"\n    if df_flagged.is_empty():\n        return pl.DataFrame(\n            {\n                \"date_only\": [],\n                \"Validator Address\": [],\n                \"mismatch_count\": [],\n                \"avg_rel_diff\": [],\n                \"max_rel_diff\": [],\n            }\n        )\n\n    lf = df_flagged.lazy()\n    summary_lf = (\n        lf.group_by([\"date_only\", \"Validator Address\"])\n        .agg(\n            [\n                pl.count(\"rel_diff_cross\").alias(\"mismatch_count\"),\n                pl.mean(\"rel_diff_cross\").alias(\"avg_rel_diff\"),\n                pl.max(\"rel_diff_cross\").alias(\"max_rel_diff\"),\n            ]\n        )\n        .sort([\"date_only\", \"Validator Address\"])\n    )\n    return summary_lf.collect()\n\n\ndef analyze_cross_rate_inconsistency(\n    submission_glob: str,\n    atn_usd_col: str = \"ATN-USD Price\",\n    ntn_usd_col: str = \"NTN-USD Price\",\n    ntn_atn_col: str = \"NTN-ATN Price\",\n    threshold: float = 0.01,\n):\n    \"\"\"\n    Main analysis function.\n    \"\"\"\n    df_all = load_and_preprocess_submissions(submission_glob)\n\n    df_flagged = check_cross_rate_inconsistency(\n        df_all,\n        atn_usd_col=atn_usd_col,\n        ntn_usd_col=ntn_usd_col,\n        ntn_atn_col=ntn_atn_col,\n        threshold=threshold,\n    )\n\n    df_summary = summarize_cross_rate_inconsistency(df_flagged)\n\n    return {\n        \"df_flagged\": df_flagged,\n        \"df_summary\": df_summary,\n    }\n\n\nresults = analyze_cross_rate_inconsistency(\n    submission_glob=\"../submission-data/Oracle_Submission_*.csv\",\n    atn_usd_col=\"ATN-USD Price\",\n    ntn_usd_col=\"NTN-USD Price\",\n    ntn_atn_col=\"NTN-ATN Price\",\n    threshold=0.01,  # 1% mismatch threshold\n)\n\n\n\n\n\nBelow presents the results from the analysis:\nFlagged Submissions (Cross-rate mismatch &gt; 10%)\n\ndf_flagged = results[\"df_flagged\"]\nif df_flagged.is_empty():\n    print(\"No cross-rate inconsistencies found beyond 10% threshold.\")\nelse:\n    print(f\"Total flagged cross-rate mismatches: {df_flagged.height}\")\n    df_flagged\n\nNo cross-rate inconsistencies found beyond 10% threshold.\n\n\n\nTimestamp_dt and Validator Address indicate problematic submissions.\nPrices (ATN-USD, NTN-USD, NTN-ATN) and estimated NTN-USD.\nrel_diff_cross shows percentage mismatch.\n\nDaily Validator Summary\n\ndf_summary = results[\"df_summary\"]\nif df_summary.is_empty():\n    print(\"No daily cross-rate mismatches to summarize.\")\nelse:\n    print(f\"Number of daily mismatch records: {df_summary.height}\")\n    df_summary\n\nNo daily cross-rate mismatches to summarize.\n\n\n\nmismatch_count: Number of daily flagged submissions per validator.\navg_rel_diff and max_rel_diff: Mean and maximum relative mismatch percentages.\n\n\nCombine both findings\n\ntotal_flagged = df_flagged.height\nif total_flagged == 0:\n    print(\"Data consistency is good. No critical cross-rate mismatches identified.\")\nelse:\n    num_validators = df_summary.get_column(\"Validator Address\").n_unique()\n    print(f\"{total_flagged} total mismatches across {num_validators} validators found. Investigate further:\")\n    high_impact = df_summary.filter(pl.col(\"max_rel_diff\") &gt; 0.5)\n    if high_impact.is_empty():\n        print(\"No extreme mismatches (&gt;50%) identified. Likely minor synchronization issues or rounding errors.\")\n    else:\n        print(f\"Validators with extreme mismatches (&gt;50%): {high_impact.height}\")\n        high_impact\n\nData consistency is good. No critical cross-rate mismatches identified.\n\n\n\nIf no mismatches, system appears healthy.\nSmall mismatches (&lt;50%) often reflect minor timing or rounding discrepancies.\nExtreme mismatches (&gt;50%) require thorough validator-specific investigation:\n\nVerify decimal conversions.\nConfirm synchronized data updates.\nReview validator configurations or potential malicious behavior.\n\n\nList of all Validators and their Consistency Indices\n\ndf_all = load_and_preprocess_submissions(\"../submission-data/Oracle_Submission_*.csv\")\n\ntotal_stats = (\n    df_all.lazy()\n    .group_by(\"Validator Address\")\n    .agg(pl.count().alias(\"total_submissions\"))\n    .collect()\n)\n\nflagged_stats = (\n    df_flagged.lazy()\n    .group_by(\"Validator Address\")\n    .agg(\n        pl.count().alias(\"flagged_count\"),\n        pl.mean(\"rel_diff_cross\").alias(\"mean_rel_diff\"),\n        pl.max(\"rel_diff_cross\").alias(\"max_rel_diff\"),\n    )\n    .collect()\n)\n\nvalidator_stats = (\n    total_stats\n    .join(flagged_stats, on=\"Validator Address\", how=\"left\")\n    .fill_null(0)\n)\n\nvalidator_stats = (\n    validator_stats\n    .with_columns(\n        (\n            (pl.col(\"flagged_count\") / pl.col(\"total_submissions\")) *\n            (pl.col(\"mean_rel_diff\") + pl.col(\"max_rel_diff\"))\n        ).alias(\"mismatch_penalty_rate\")\n    )\n    .with_columns(\n        (1 / (1 + pl.col(\"mismatch_penalty_rate\")))\n        .alias(\"consistency_index\")\n    )\n    .sort(\"consistency_index\", descending=False)\n)\n\nprint(\"Validator Consistency Ranking (higher index = better)\\n\")\nfor row in validator_stats.to_dicts():\n    print(\n        f\"Validator {row['Validator Address']}: \"\n        f\"subs={row['total_submissions']:5d}, \"\n        f\"flags={row['flagged_count']:3d}, \"\n        f\"index={row['consistency_index']:.4f}\"\n    )\n\nValidator Consistency Ranking (higher index = better)\n\nValidator 0x383A3c437d3F12f60E5fC990119468D3561EfBfc: subs= 2880, flags=  0, index=1.0000\nValidator 0xDCA5DFF3D42f2db3C18dBE823380A0A81db49A7E: subs= 2880, flags=  0, index=1.0000\nValidator 0xcf716b3930d7cf6f2ADAD90A27c39fDc9D643BBd: subs= 2880, flags=  0, index=1.0000\nValidator 0xbfDcAF35f52F9ef423ac8F2621F9eef8be6dEd17: subs= 2833, flags=  0, index=1.0000\nValidator 0x1Be7f70BCf8393a7e4A5BcC66F6f15d6e35cfBBC: subs= 2880, flags=  0, index=1.0000\nValidator 0x64F83c2538A646A550Ad9bEEb63427a377359DEE: subs= 2880, flags=  0, index=1.0000\nValidator 0xB5d8be2AB4b6d7E6be7Ea28E91b370223a06289f: subs= 2880, flags=  0, index=1.0000\nValidator 0x5603caFE3313D0cf56Fd4bE4A2f606dD6E43F8Eb: subs= 2880, flags=  0, index=1.0000\nValidator 0xdF239e0D5b4E6e820B0cFEF6972A90893c2073AB: subs= 2880, flags=  0, index=1.0000\nValidator 0xBE287C82A786218E008FF97320b08244BE4A282c: subs= 2879, flags=  0, index=1.0000\nValidator 0x5E17e837DcBa2728C94f95c38fA8a47CB9C8818F: subs= 2876, flags=  0, index=1.0000\nValidator 0x551f3300FCFE0e392178b3542c009948008B2a9F: subs= 2880, flags=  0, index=1.0000\nValidator 0xE4686A4C6E63A8ab51B458c52EB779AEcf0B74f7: subs= 2880, flags=  0, index=1.0000\nValidator 0x100E38f7BCEc53937BDd79ADE46F34362470577B: subs= 2876, flags=  0, index=1.0000\nValidator 0xF9B38D02959379d43C764064dE201324d5e12931: subs= 2880, flags=  0, index=1.0000\nValidator 0x3fe573552E14a0FC11Da25E43Fef11e16a785068: subs= 2880, flags=  0, index=1.0000\nValidator 0x718361fc3637199F24a2437331677D6B89a40519: subs= 2880, flags=  0, index=1.0000\nValidator 0x358488a4EdCA493FCD87610dcd50c62c8A3Dd658: subs= 2880, flags=  0, index=1.0000\nValidator 0x527192F3D2408C84087607b7feE1d0f907821E17: subs= 2880, flags=  0, index=1.0000\nValidator 0x9C7dAABb5101623340C925CFD6fF74088ff5672e: subs= 2880, flags=  0, index=1.0000\nValidator 0x6a395dE946c0493157404E2b1947493c633f569E: subs= 2874, flags=  0, index=1.0000\nValidator 0x2928FE5b911BCAf837cAd93eB9626E86a189f1dd: subs= 2829, flags=  0, index=1.0000\nValidator 0x9d28e40E9Ec4789f9A0D17e421F76D8D0868EA44: subs= 2880, flags=  0, index=1.0000\nValidator 0x01F788E4371a70D579C178Ea7F48E04e8B2CD743: subs= 2837, flags=  0, index=1.0000\nValidator 0xDF2D0052ea56A860443039619f6DAe4434bc0Ac4: subs= 2879, flags=  0, index=1.0000\nValidator 0x8f91e0ADF8065C3fFF92297267E02DF32C2978FF: subs= 2879, flags=  0, index=1.0000\nValidator 0x3597d2D42f8Fbbc82E8b1046048773aD6DDB717E: subs= 2880, flags=  0, index=1.0000\nValidator 0x7232e75a8bFd8c9ab002BB3A00eAa885BC72A6dd: subs= 2877, flags=  0, index=1.0000\nValidator 0x8584A78A9b94f332A34BBf24D2AF83367Da31894: subs= 2879, flags=  0, index=1.0000\nValidator 0xc5B9d978715F081E226cb28bADB7Ba4cde5f9775: subs= 2879, flags=  0, index=1.0000\nValidator 0xd625d50B0d087861c286d726eC51Cf4Bd9c54357: subs= 2856, flags=  0, index=1.0000\nValidator 0x00a96aaED75015Bb44cED878D927dcb15ec1FF54: subs= 2866, flags=  0, index=1.0000\nValidator 0x197B2c44b887c4aC01243BDE7E4bBa8bd95BC3a8: subs= 2880, flags=  0, index=1.0000\nValidator 0x24915749B793375a8C93090AF19928aFF1CAEcb6: subs= 2880, flags=  0, index=1.0000\nValidator 0xcdEed21b471b0Dc54faF74480A0E700fCc42a7b6: subs= 2880, flags=  0, index=1.0000\nValidator 0x19E356ebC20283fc74AF0BA4C179502A1F62fA7B: subs= 2833, flags=  0, index=1.0000\nValidator 0x831B837C3DA1B6c2AB68a690206bDfF368877E19: subs= 2863, flags=  0, index=1.0000\nValidator 0x36142A4f36974e2935192A1111C39330aA296D3C: subs= 2880, flags=  0, index=1.0000\nValidator 0xfD97FB8835d25740A2Da27c69762D74F6A931858: subs= 2880, flags=  0, index=1.0000\nValidator 0x94d28f08Ff81A80f4716C0a8EfC6CAC2Ec74d09E: subs= 2880, flags=  0, index=1.0000\nValidator 0xD9fDab408dF7Ae751691BeC2efE3b713ba3f9C36: subs= 2880, flags=  0, index=1.0000\nValidator 0xE9FFF86CAdC3136b3D94948B8Fd23631EDaa2dE3: subs= 2880, flags=  0, index=1.0000\nValidator 0xBBf36374eb23968F25aecAEbb97BF3118f3c2fEC: subs= 2880, flags=  0, index=1.0000\nValidator 0xf34CD6c09a59d7D3d1a6C3dC231a7834E5615D6A: subs= 2834, flags=  0, index=1.0000\nValidator 0x3AaF7817618728ffEF81898E11A3171C33faAE41: subs= 2874, flags=  0, index=1.0000\nValidator 0x94470A842Ea4f44e668EB9C2AB81367b6Ce01772: subs= 2880, flags=  0, index=1.0000\nValidator 0x4cD134001EEF0843B9c69Ba9569d11fDcF4bd495: subs= 2823, flags=  0, index=1.0000\nValidator 0x984A46Ec685Bb41A7BBb2bc39f80C78410ff4057: subs= 2877, flags=  0, index=1.0000\nValidator 0x22A76e194A49c9e5508Cd4A3E1cD555D088ECB08: subs= 2880, flags=  0, index=1.0000\nValidator 0x26E2724dBD14Fbd52be430B97043AA4c83F05852: subs= 2880, flags=  0, index=1.0000\nValidator 0x791A7F840ac11841cCB0FaA968B2e3a0Db930fCe: subs= 2880, flags=  0, index=1.0000\nValidator 0x6747c02DE7eb2099265e55715Ba2E03e8563D051: subs= 2840, flags=  0, index=1.0000\nValidator 0xEf0Ba5e345C2C3937df5667A870Aae5105CAa3a5: subs= 2879, flags=  0, index=1.0000\nValidator 0xC1F9acAF1824F6C906b35A0D2584D6E25077C7f5: subs= 2880, flags=  0, index=1.0000\nValidator 0x1476A65D7B5739dE1805d5130441A94022Ee49fe: subs= 2462, flags=  0, index=1.0000\nValidator 0xf10f56Bf0A28E0737c7e6bB0aF92f3DDad34aE6a: subs= 2880, flags=  0, index=1.0000\nValidator 0x99E2B4B27BDe92b42D04B6CF302cF564D2C13b74: subs= 2880, flags=  0, index=1.0000\nValidator 0xd61a48b0e11B0Dc6b7Bd713B1012563c52591BAA: subs= 2873, flags=  0, index=1.0000\nValidator 0x59031767f20EA8F4a3d90d33aB0DAA2ca469Fd9a: subs= 2880, flags=  0, index=1.0000\nValidator 0x23b4Be9536F93b8D550214912fD0e38417Ff7209: subs= 2880, flags=  0, index=1.0000\n\n\nPlease note, total_submissions represents the total number of submissions for this validator. flagged_count is how many of those were caught by the 1 % cross-rate check. consistency_index is a 0-to-1 reliability score (1 = no mismatches; lower values indicate more frequent or larger inconsistencies).",
    "crumbs": [
      "Notebooks",
      "Issue 6"
    ]
  },
  {
    "objectID": "notebooks/issue_8.html",
    "href": "notebooks/issue_8.html",
    "title": "Issue 8",
    "section": "",
    "text": "This notebook documents the analysis for Issue #8: Weekend / Market Closure Effects in the Autonity Oracle data. It covers:\n\nWhat is this issue about?\n\nWhy conduct this issue analysis?\n\nHow to conduct this issue analysis?\n\nWhat are the results?\n\n\n\n\nIn traditional FX markets, trading usually pauses during weekends, leading to flat or absent quotes. Some Oracle validators might incorrectly update FX prices on weekends, resulting in:\n\nFlat or missing FX data on weekends.\nUnexpected updates from some validators, possibly due to synthetic or misconfigured data feeds.\n\n\n\n\n\n\nTo ensure the Oracle accurately reflects real FX market behavior.\nTo identify if validators consistently handle weekends incorrectly.\nTo inform policy decisions about weekend data handling.\n\n\n\n\n\nUse Python with the Polars library (v1.24.0) to:\n\nLoad and preprocess Oracle submission CSV files.\nAggregate submission counts by day, distinguishing weekends (Saturday=5, Sunday=6) from weekdays.\nCompare Oracle weekend submissions to minute-level FX data from Yahoo Finance to see if validators’ behaviors align with real market data.\n\nBelow is the script performing this analysis:\n\nimport polars as pl\nimport glob\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndef load_and_preprocess_submissions(submission_glob: str) -&gt; pl.DataFrame:\n    \"\"\"\n    Loads Oracle Submission CSVs and returns a Polars DataFrame.\n    \"\"\"\n    files = sorted(glob.glob(submission_glob))\n    if not files:\n        raise ValueError(f\"No files match {submission_glob}\")\n    \n    lf = pl.concat(\n        [pl.scan_csv(f, dtypes={\"Timestamp\": pl.Utf8}, null_values=[\"\"], ignore_errors=True) for f in files]\n    ).with_columns(\n        pl.col(\"Timestamp\").str.strptime(pl.Datetime, strict=False).alias(\"Timestamp_dt\")\n    ).with_columns([\n        pl.col(\"Timestamp_dt\").cast(pl.Date).alias(\"date_only\"),\n        pl.col(\"Timestamp_dt\").dt.weekday().alias(\"weekday_num\")\n    ])\n\n    return lf.collect()\n\ndef load_yahoo_finance_data(directory_pattern: str, pair_label: str) -&gt; pl.DataFrame:\n    \"\"\"\n    Loads Yahoo Finance CSVs and returns a Polars DataFrame.\n    \"\"\"\n    files = sorted(glob.glob(directory_pattern))\n    if not files:\n        raise ValueError(f\"No Yahoo CSVs found: {directory_pattern}\")\n\n    lf = pl.concat(\n        [pl.scan_csv(f, has_header=False, skip_rows=3,\n                     new_columns=[\"Datetime\",\"Close\",\"High\",\"Low\",\"Open\",\"Volume\"],\n                     try_parse_dates=True) for f in files]\n    ).sort(\"Datetime\").select([\n        pl.col(\"Datetime\").alias(\"timestamp_benchmark\"),\n        pl.col(\"Close\").alias(\"benchmark_close\")\n    ])\n\n    return lf.collect().with_columns(pl.lit(pair_label).alias(\"symbol\"))\n\ndef load_all_fx_benchmarks() -&gt; dict[str, pl.DataFrame]:\n    \"\"\"\n    Loads FX data from Yahoo Finance.\n    \"\"\"\n    patterns = {\n        \"AUD-USD\": \"../yahoo-finance/data/AUDUSD/AUDUSD=X_1m_*.csv\",\n        \"CAD-USD\": \"../yahoo-finance/data/CADUSD/CADUSD=X_1m_*.csv\",\n        \"EUR-USD\": \"../yahoo-finance/data/EURUSD/EURUSD=X_1m_*.csv\",\n        \"GBP-USD\": \"../yahoo-finance/data/GBPUSD/GBPUSD=X_1m_*.csv\",\n        \"JPY-USD\": \"../yahoo-finance/data/JPYUSD/JPYUSD=X_1m_*.csv\",\n        \"SEK-USD\": \"../yahoo-finance/data/SEKUSD/SEKUSD=X_1m_*.csv\",\n    }\n    return {pair: load_yahoo_finance_data(pattern, pair) for pair, pattern in patterns.items()}\n\ndef analyze_weekend_market_closure_issues(\n    submission_glob: str,\n    fx_pairs: list[str],\n    yahoo_data_dict: dict[str, pl.DataFrame],\n):\n    \"\"\"\n    Main analysis function.\n    \"\"\"\n    df_sub = load_and_preprocess_submissions(submission_glob)\n\n    fx_cols = [c for c in fx_pairs if c in df_sub.columns]\n    df_sub = df_sub.with_columns(\n        pl.fold(False, lambda acc, x: acc | x, [pl.col(c).is_not_null() for c in fx_cols]).alias(\"any_fx_submitted\")\n    )\n\n    df_agg_day = df_sub.group_by([\"date_only\",\"weekday_num\"]).agg([\n        pl.col(\"any_fx_submitted\").sum().alias(\"num_submissions\"),\n        pl.col(\"any_fx_submitted\").sum().alias(\"num_fx_submissions\")\n    ]).sort(\"date_only\")\n\n    yahoo_weekend_info = {}\n    for pair, df_yahoo in yahoo_data_dict.items():\n        df_y = df_yahoo.with_columns([\n            pl.col(\"timestamp_benchmark\").cast(pl.Date).alias(\"date_only\"),\n            pl.col(\"timestamp_benchmark\").dt.weekday().alias(\"weekday_num\")\n        ]).group_by([\"date_only\",\"weekday_num\"]).agg([\n            pl.count(\"benchmark_close\").alias(\"count_quotes\"),\n            (pl.col(\"benchmark_close\").max() - pl.col(\"benchmark_close\").min()).alias(\"range_close\")\n        ])\n        yahoo_weekend_info[pair] = df_y\n\n    weekend_rows = df_agg_day.filter(pl.col(\"weekday_num\") &gt;= 5)\n    weekday_rows = df_agg_day.filter(pl.col(\"weekday_num\") &lt; 5)\n\n    results = {\n        \"df_submissions_raw\": df_sub,\n        \"df_submissions_day_agg\": df_agg_day,\n        \"weekend_total_submissions\": weekend_rows[\"num_submissions\"].sum(),\n        \"weekday_total_submissions\": weekday_rows[\"num_submissions\"].sum(),\n        \"yahoo_weekend_info\": yahoo_weekend_info\n    }\n\n    return results\n\n\nfx_cols = [\n    \"AUD-USD Price\",\"CAD-USD Price\",\"EUR-USD Price\",\n    \"GBP-USD Price\",\"JPY-USD Price\",\"SEK-USD Price\"\n]\n\nyahoo_data = load_all_fx_benchmarks()\n\nresults = analyze_weekend_market_closure_issues(\n    \"../submission-data/Oracle_Submission_*.csv\",\n    fx_cols,\n    yahoo_data\n)\n\n\n\n\n\nBelow are the results, interpretation and key insights from the analysis.\nDaily Submission Patterns (Weekend vs. Weekday)\n\ndf_daily = results[\"df_submissions_day_agg\"]\ndf_daily\n\n\nshape: (1, 4)\n\n\n\ndate_only\nweekday_num\nnum_submissions\nnum_fx_submissions\n\n\ndate\ni8\nu32\nu32\n\n\n\n\n2025-01-01\n3\n157222\n157222\n\n\n\n\n\n\n\nweekend_total = results[\"weekend_total_submissions\"]\nweekday_total = results[\"weekday_total_submissions\"]\n\nprint(f\"Total weekend submissions: {weekend_total}\")\nprint(f\"Total weekday submissions: {weekday_total}\")\n\nweekend_pct = (weekend_total / (weekend_total + weekday_total)) * 100\nprint(f\"Weekend submissions as % of total: {weekend_pct:.1f}%\")\n\nif weekend_pct &lt; 10:\n    print(\"Very low weekend submissions suggest validators typically avoid weekend updates, aligning with expected FX market behavior.\")\nelif weekend_pct &lt; 30:\n    print(\"Moderate weekend activity suggests possible mixed validator behavior or synthetic weekend feeds.\")\nelse:\n    print(\"High weekend submissions indicate unexpected validator behavior or synthetic FX data.\")\n\nTotal weekend submissions: 0\nTotal weekday submissions: 157222\nWeekend submissions as % of total: 0.0%\nVery low weekend submissions suggest validators typically avoid weekend updates, aligning with expected FX market behavior.\n\n\nYahoo Finance Weekend Patterns (FX benchmarks)\n\nyahoo_info = results[\"yahoo_weekend_info\"]\nfor pair, df_y in yahoo_info.items():\n    print(f\"\\nFX Pair: {pair}\")\n    display(df_y)\n    weekend_quotes = df_y.filter(pl.col(\"weekday_num\") &gt;= 5)[\"count_quotes\"].sum()\n    print(f\"Total weekend Yahoo quotes: {weekend_quotes}\")\n\n    if weekend_quotes == 0:\n        print(f\"Yahoo data confirms no weekend activity for {pair}.\")\n    else:\n        print(f\"Yahoo data shows unexpected weekend activity for {pair}.\")\n\n\nFX Pair: AUD-USD\n\n\n\nshape: (67, 4)\n\n\n\ndate_only\nweekday_num\ncount_quotes\nrange_close\n\n\ndate\ni8\nu32\nf64\n\n\n\n\n2025-01-23\n4\n1420\n0.004371\n\n\n2025-02-03\n1\n1417\n0.01459\n\n\n2025-02-26\n3\n1413\n0.005379\n\n\n2025-01-01\n3\n775\n0.000863\n\n\n2025-01-14\n2\n1419\n0.00404\n\n\n…\n…\n…\n…\n\n\n2025-02-06\n4\n1429\n0.003325\n\n\n2025-02-04\n2\n1423\n0.00879\n\n\n2025-03-17\n1\n1394\n0.006952\n\n\n2025-03-21\n5\n1279\n0.004681\n\n\n2025-01-06\n1\n1421\n0.008117\n\n\n\n\n\n\nTotal weekend Yahoo quotes: 17130\nYahoo data shows unexpected weekend activity for AUD-USD.\n\nFX Pair: CAD-USD\n\n\n\nshape: (67, 4)\n\n\n\ndate_only\nweekday_num\ncount_quotes\nrange_close\n\n\ndate\ni8\nu32\nf64\n\n\n\n\n2025-02-07\n5\n1320\n0.003655\n\n\n2025-01-14\n2\n1438\n0.003208\n\n\n2025-01-24\n5\n1316\n0.00343\n\n\n2025-02-24\n1\n1437\n0.004905\n\n\n2025-01-22\n3\n1433\n0.004933\n\n\n…\n…\n…\n…\n\n\n2025-03-19\n3\n1431\n0.002657\n\n\n2025-02-25\n2\n1437\n0.003831\n\n\n2025-03-13\n4\n1431\n0.004449\n\n\n2025-01-07\n2\n1437\n0.003622\n\n\n2025-03-26\n3\n1431\n0.003906\n\n\n\n\n\n\nTotal weekend Yahoo quotes: 17175\nYahoo data shows unexpected weekend activity for CAD-USD.\n\nFX Pair: EUR-USD\n\n\n\nshape: (67, 4)\n\n\n\ndate_only\nweekday_num\ncount_quotes\nrange_close\n\n\ndate\ni8\nu32\nf64\n\n\n\n\n2025-03-03\n1\n1432\n0.010915\n\n\n2025-02-25\n2\n1435\n0.006496\n\n\n2025-02-14\n5\n1337\n0.006594\n\n\n2025-01-10\n5\n1343\n0.009174\n\n\n2025-01-14\n2\n1432\n0.00697\n\n\n…\n…\n…\n…\n\n\n2025-02-20\n4\n1426\n0.008542\n\n\n2025-03-12\n3\n1426\n0.005234\n\n\n2025-03-20\n4\n1418\n0.010044\n\n\n2024-12-31\n2\n1360\n0.008092\n\n\n2025-01-13\n1\n1430\n0.009629\n\n\n\n\n\n\nTotal weekend Yahoo quotes: 17283\nYahoo data shows unexpected weekend activity for EUR-USD.\n\nFX Pair: GBP-USD\n\n\n\nshape: (67, 4)\n\n\n\ndate_only\nweekday_num\ncount_quotes\nrange_close\n\n\ndate\ni8\nu32\nf64\n\n\n\n\n2025-03-30\n7\n60\n0.001004\n\n\n2025-03-28\n5\n1286\n0.004609\n\n\n2025-01-17\n5\n1340\n0.008279\n\n\n2025-02-20\n4\n1426\n0.009328\n\n\n2025-03-11\n2\n1427\n0.009181\n\n\n…\n…\n…\n…\n\n\n2025-01-07\n2\n1436\n0.010181\n\n\n2025-03-24\n1\n1429\n0.007713\n\n\n2025-02-24\n1\n1429\n0.007763\n\n\n2025-03-03\n1\n1432\n0.013331\n\n\n2024-12-30\n1\n1432\n0.009888\n\n\n\n\n\n\nTotal weekend Yahoo quotes: 17283\nYahoo data shows unexpected weekend activity for GBP-USD.\n\nFX Pair: JPY-USD\n\n\n\nshape: (67, 4)\n\n\n\ndate_only\nweekday_num\ncount_quotes\nrange_close\n\n\ndate\ni8\nu32\nf64\n\n\n\n\n2025-03-10\n1\n1422\n0.000052\n\n\n2025-01-01\n3\n301\n0.000027\n\n\n2025-03-20\n4\n1426\n0.000034\n\n\n2025-03-13\n4\n1432\n0.000043\n\n\n2025-02-05\n3\n1437\n0.000095\n\n\n…\n…\n…\n…\n\n\n2025-03-12\n3\n1435\n0.000058\n\n\n2025-02-03\n1\n1438\n0.000077\n\n\n2025-02-13\n4\n1435\n0.000082\n\n\n2025-02-18\n2\n1435\n0.000033\n\n\n2025-02-11\n2\n1432\n0.000046\n\n\n\n\n\n\nTotal weekend Yahoo quotes: 17162\nYahoo data shows unexpected weekend activity for JPY-USD.\n\nFX Pair: SEK-USD\n\n\n\nshape: (67, 4)\n\n\n\ndate_only\nweekday_num\ncount_quotes\nrange_close\n\n\ndate\ni8\nu32\nf64\n\n\n\n\n2025-01-03\n5\n1315\n0.000458\n\n\n2025-03-06\n4\n1427\n0.001217\n\n\n2025-02-25\n2\n1437\n0.000916\n\n\n2025-03-27\n4\n1438\n0.000981\n\n\n2025-01-23\n4\n1372\n0.000476\n\n\n…\n…\n…\n…\n\n\n2025-03-04\n2\n1437\n0.001738\n\n\n2025-01-30\n4\n1374\n0.00073\n\n\n2025-01-29\n3\n1403\n0.000508\n\n\n2025-03-26\n3\n1428\n0.000589\n\n\n2025-03-17\n1\n1433\n0.000778\n\n\n\n\n\n\nTotal weekend Yahoo quotes: 17153\nYahoo data shows unexpected weekend activity for SEK-USD.\n\n\nList of all Validators and their Weekend Submission Ratios\n\ndf_sub_raw = results[\"df_submissions_raw\"]\n\ndf_validator_weekend = (\n    df_sub_raw\n    .with_columns(\n        (pl.col(\"weekday_num\") &gt;= 5).cast(pl.Int8).alias(\"is_weekend\")\n    )\n    .group_by(\"Validator Address\")\n    .agg([\n        pl.count().alias(\"total_submissions\"),\n        pl.col(\"is_weekend\").sum().alias(\"weekend_submissions\")\n    ])\n    .with_columns(\n        (pl.col(\"weekend_submissions\") / pl.col(\"total_submissions\"))\n        .alias(\"weekend_submission_ratio\")\n    )\n    .sort(\"weekend_submission_ratio\", descending=True)\n)\n\nfor row in df_validator_weekend.to_dicts():\n    print(\n        f\"Validator {row['Validator Address']}: \"\n        f\"total_submissions={row['total_submissions']}, \"\n        f\"weekend_submissions={row['weekend_submissions']}, \"\n        f\"weekend_submission_ratio={row['weekend_submission_ratio']:.2f}\"\n    )\n\nValidator 0xfD97FB8835d25740A2Da27c69762D74F6A931858: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x9d28e40E9Ec4789f9A0D17e421F76D8D0868EA44: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x22A76e194A49c9e5508Cd4A3E1cD555D088ECB08: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xD9fDab408dF7Ae751691BeC2efE3b713ba3f9C36: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x7232e75a8bFd8c9ab002BB3A00eAa885BC72A6dd: total_submissions=2877, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x19E356ebC20283fc74AF0BA4C179502A1F62fA7B: total_submissions=2833, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x6747c02DE7eb2099265e55715Ba2E03e8563D051: total_submissions=2840, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x358488a4EdCA493FCD87610dcd50c62c8A3Dd658: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x1476A65D7B5739dE1805d5130441A94022Ee49fe: total_submissions=2462, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xc5B9d978715F081E226cb28bADB7Ba4cde5f9775: total_submissions=2879, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x100E38f7BCEc53937BDd79ADE46F34362470577B: total_submissions=2876, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xE9FFF86CAdC3136b3D94948B8Fd23631EDaa2dE3: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x527192F3D2408C84087607b7feE1d0f907821E17: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xcdEed21b471b0Dc54faF74480A0E700fCc42a7b6: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x23b4Be9536F93b8D550214912fD0e38417Ff7209: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x00a96aaED75015Bb44cED878D927dcb15ec1FF54: total_submissions=2866, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x551f3300FCFE0e392178b3542c009948008B2a9F: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xC1F9acAF1824F6C906b35A0D2584D6E25077C7f5: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xDF2D0052ea56A860443039619f6DAe4434bc0Ac4: total_submissions=2879, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x5603caFE3313D0cf56Fd4bE4A2f606dD6E43F8Eb: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x197B2c44b887c4aC01243BDE7E4bBa8bd95BC3a8: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x718361fc3637199F24a2437331677D6B89a40519: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xE4686A4C6E63A8ab51B458c52EB779AEcf0B74f7: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xF9B38D02959379d43C764064dE201324d5e12931: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x4cD134001EEF0843B9c69Ba9569d11fDcF4bd495: total_submissions=2823, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x984A46Ec685Bb41A7BBb2bc39f80C78410ff4057: total_submissions=2877, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xB5d8be2AB4b6d7E6be7Ea28E91b370223a06289f: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x94470A842Ea4f44e668EB9C2AB81367b6Ce01772: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x791A7F840ac11841cCB0FaA968B2e3a0Db930fCe: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x3fe573552E14a0FC11Da25E43Fef11e16a785068: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x2928FE5b911BCAf837cAd93eB9626E86a189f1dd: total_submissions=2829, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xDCA5DFF3D42f2db3C18dBE823380A0A81db49A7E: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x3597d2D42f8Fbbc82E8b1046048773aD6DDB717E: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x8584A78A9b94f332A34BBf24D2AF83367Da31894: total_submissions=2879, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x24915749B793375a8C93090AF19928aFF1CAEcb6: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x26E2724dBD14Fbd52be430B97043AA4c83F05852: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xBBf36374eb23968F25aecAEbb97BF3118f3c2fEC: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x8f91e0ADF8065C3fFF92297267E02DF32C2978FF: total_submissions=2879, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xd625d50B0d087861c286d726eC51Cf4Bd9c54357: total_submissions=2856, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xbfDcAF35f52F9ef423ac8F2621F9eef8be6dEd17: total_submissions=2833, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xd61a48b0e11B0Dc6b7Bd713B1012563c52591BAA: total_submissions=2873, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xBE287C82A786218E008FF97320b08244BE4A282c: total_submissions=2879, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x94d28f08Ff81A80f4716C0a8EfC6CAC2Ec74d09E: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x383A3c437d3F12f60E5fC990119468D3561EfBfc: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x3AaF7817618728ffEF81898E11A3171C33faAE41: total_submissions=2874, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xdF239e0D5b4E6e820B0cFEF6972A90893c2073AB: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x831B837C3DA1B6c2AB68a690206bDfF368877E19: total_submissions=2863, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x1Be7f70BCf8393a7e4A5BcC66F6f15d6e35cfBBC: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x59031767f20EA8F4a3d90d33aB0DAA2ca469Fd9a: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xcf716b3930d7cf6f2ADAD90A27c39fDc9D643BBd: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x01F788E4371a70D579C178Ea7F48E04e8B2CD743: total_submissions=2837, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x6a395dE946c0493157404E2b1947493c633f569E: total_submissions=2874, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x36142A4f36974e2935192A1111C39330aA296D3C: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x99E2B4B27BDe92b42D04B6CF302cF564D2C13b74: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x5E17e837DcBa2728C94f95c38fA8a47CB9C8818F: total_submissions=2876, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xEf0Ba5e345C2C3937df5667A870Aae5105CAa3a5: total_submissions=2879, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x9C7dAABb5101623340C925CFD6fF74088ff5672e: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x64F83c2538A646A550Ad9bEEb63427a377359DEE: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xf34CD6c09a59d7D3d1a6C3dC231a7834E5615D6A: total_submissions=2834, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xf10f56Bf0A28E0737c7e6bB0aF92f3DDad34aE6a: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\n\n\nPlease note, total_submissions represents the total number of individual price-update rows this validator sent during the study period. weekend_submissions is how many of those rows were posted on Saturdays or Sundays. weekend_submission_ratio is the proportion of weekend submissions (0 – 1 scale); higher values indicate the validator updates more frequently on weekends relative to weekdays.",
    "crumbs": [
      "Notebooks",
      "Issue 8"
    ]
  },
  {
    "objectID": "notebooks/issue_8.html#weekend-market-closure-effects",
    "href": "notebooks/issue_8.html#weekend-market-closure-effects",
    "title": "Issue 8",
    "section": "",
    "text": "This notebook documents the analysis for Issue #8: Weekend / Market Closure Effects in the Autonity Oracle data. It covers:\n\nWhat is this issue about?\n\nWhy conduct this issue analysis?\n\nHow to conduct this issue analysis?\n\nWhat are the results?\n\n\n\n\nIn traditional FX markets, trading usually pauses during weekends, leading to flat or absent quotes. Some Oracle validators might incorrectly update FX prices on weekends, resulting in:\n\nFlat or missing FX data on weekends.\nUnexpected updates from some validators, possibly due to synthetic or misconfigured data feeds.\n\n\n\n\n\n\nTo ensure the Oracle accurately reflects real FX market behavior.\nTo identify if validators consistently handle weekends incorrectly.\nTo inform policy decisions about weekend data handling.\n\n\n\n\n\nUse Python with the Polars library (v1.24.0) to:\n\nLoad and preprocess Oracle submission CSV files.\nAggregate submission counts by day, distinguishing weekends (Saturday=5, Sunday=6) from weekdays.\nCompare Oracle weekend submissions to minute-level FX data from Yahoo Finance to see if validators’ behaviors align with real market data.\n\nBelow is the script performing this analysis:\n\nimport polars as pl\nimport glob\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndef load_and_preprocess_submissions(submission_glob: str) -&gt; pl.DataFrame:\n    \"\"\"\n    Loads Oracle Submission CSVs and returns a Polars DataFrame.\n    \"\"\"\n    files = sorted(glob.glob(submission_glob))\n    if not files:\n        raise ValueError(f\"No files match {submission_glob}\")\n    \n    lf = pl.concat(\n        [pl.scan_csv(f, dtypes={\"Timestamp\": pl.Utf8}, null_values=[\"\"], ignore_errors=True) for f in files]\n    ).with_columns(\n        pl.col(\"Timestamp\").str.strptime(pl.Datetime, strict=False).alias(\"Timestamp_dt\")\n    ).with_columns([\n        pl.col(\"Timestamp_dt\").cast(pl.Date).alias(\"date_only\"),\n        pl.col(\"Timestamp_dt\").dt.weekday().alias(\"weekday_num\")\n    ])\n\n    return lf.collect()\n\ndef load_yahoo_finance_data(directory_pattern: str, pair_label: str) -&gt; pl.DataFrame:\n    \"\"\"\n    Loads Yahoo Finance CSVs and returns a Polars DataFrame.\n    \"\"\"\n    files = sorted(glob.glob(directory_pattern))\n    if not files:\n        raise ValueError(f\"No Yahoo CSVs found: {directory_pattern}\")\n\n    lf = pl.concat(\n        [pl.scan_csv(f, has_header=False, skip_rows=3,\n                     new_columns=[\"Datetime\",\"Close\",\"High\",\"Low\",\"Open\",\"Volume\"],\n                     try_parse_dates=True) for f in files]\n    ).sort(\"Datetime\").select([\n        pl.col(\"Datetime\").alias(\"timestamp_benchmark\"),\n        pl.col(\"Close\").alias(\"benchmark_close\")\n    ])\n\n    return lf.collect().with_columns(pl.lit(pair_label).alias(\"symbol\"))\n\ndef load_all_fx_benchmarks() -&gt; dict[str, pl.DataFrame]:\n    \"\"\"\n    Loads FX data from Yahoo Finance.\n    \"\"\"\n    patterns = {\n        \"AUD-USD\": \"../yahoo-finance/data/AUDUSD/AUDUSD=X_1m_*.csv\",\n        \"CAD-USD\": \"../yahoo-finance/data/CADUSD/CADUSD=X_1m_*.csv\",\n        \"EUR-USD\": \"../yahoo-finance/data/EURUSD/EURUSD=X_1m_*.csv\",\n        \"GBP-USD\": \"../yahoo-finance/data/GBPUSD/GBPUSD=X_1m_*.csv\",\n        \"JPY-USD\": \"../yahoo-finance/data/JPYUSD/JPYUSD=X_1m_*.csv\",\n        \"SEK-USD\": \"../yahoo-finance/data/SEKUSD/SEKUSD=X_1m_*.csv\",\n    }\n    return {pair: load_yahoo_finance_data(pattern, pair) for pair, pattern in patterns.items()}\n\ndef analyze_weekend_market_closure_issues(\n    submission_glob: str,\n    fx_pairs: list[str],\n    yahoo_data_dict: dict[str, pl.DataFrame],\n):\n    \"\"\"\n    Main analysis function.\n    \"\"\"\n    df_sub = load_and_preprocess_submissions(submission_glob)\n\n    fx_cols = [c for c in fx_pairs if c in df_sub.columns]\n    df_sub = df_sub.with_columns(\n        pl.fold(False, lambda acc, x: acc | x, [pl.col(c).is_not_null() for c in fx_cols]).alias(\"any_fx_submitted\")\n    )\n\n    df_agg_day = df_sub.group_by([\"date_only\",\"weekday_num\"]).agg([\n        pl.col(\"any_fx_submitted\").sum().alias(\"num_submissions\"),\n        pl.col(\"any_fx_submitted\").sum().alias(\"num_fx_submissions\")\n    ]).sort(\"date_only\")\n\n    yahoo_weekend_info = {}\n    for pair, df_yahoo in yahoo_data_dict.items():\n        df_y = df_yahoo.with_columns([\n            pl.col(\"timestamp_benchmark\").cast(pl.Date).alias(\"date_only\"),\n            pl.col(\"timestamp_benchmark\").dt.weekday().alias(\"weekday_num\")\n        ]).group_by([\"date_only\",\"weekday_num\"]).agg([\n            pl.count(\"benchmark_close\").alias(\"count_quotes\"),\n            (pl.col(\"benchmark_close\").max() - pl.col(\"benchmark_close\").min()).alias(\"range_close\")\n        ])\n        yahoo_weekend_info[pair] = df_y\n\n    weekend_rows = df_agg_day.filter(pl.col(\"weekday_num\") &gt;= 5)\n    weekday_rows = df_agg_day.filter(pl.col(\"weekday_num\") &lt; 5)\n\n    results = {\n        \"df_submissions_raw\": df_sub,\n        \"df_submissions_day_agg\": df_agg_day,\n        \"weekend_total_submissions\": weekend_rows[\"num_submissions\"].sum(),\n        \"weekday_total_submissions\": weekday_rows[\"num_submissions\"].sum(),\n        \"yahoo_weekend_info\": yahoo_weekend_info\n    }\n\n    return results\n\n\nfx_cols = [\n    \"AUD-USD Price\",\"CAD-USD Price\",\"EUR-USD Price\",\n    \"GBP-USD Price\",\"JPY-USD Price\",\"SEK-USD Price\"\n]\n\nyahoo_data = load_all_fx_benchmarks()\n\nresults = analyze_weekend_market_closure_issues(\n    \"../submission-data/Oracle_Submission_*.csv\",\n    fx_cols,\n    yahoo_data\n)\n\n\n\n\n\nBelow are the results, interpretation and key insights from the analysis.\nDaily Submission Patterns (Weekend vs. Weekday)\n\ndf_daily = results[\"df_submissions_day_agg\"]\ndf_daily\n\n\nshape: (1, 4)\n\n\n\ndate_only\nweekday_num\nnum_submissions\nnum_fx_submissions\n\n\ndate\ni8\nu32\nu32\n\n\n\n\n2025-01-01\n3\n157222\n157222\n\n\n\n\n\n\n\nweekend_total = results[\"weekend_total_submissions\"]\nweekday_total = results[\"weekday_total_submissions\"]\n\nprint(f\"Total weekend submissions: {weekend_total}\")\nprint(f\"Total weekday submissions: {weekday_total}\")\n\nweekend_pct = (weekend_total / (weekend_total + weekday_total)) * 100\nprint(f\"Weekend submissions as % of total: {weekend_pct:.1f}%\")\n\nif weekend_pct &lt; 10:\n    print(\"Very low weekend submissions suggest validators typically avoid weekend updates, aligning with expected FX market behavior.\")\nelif weekend_pct &lt; 30:\n    print(\"Moderate weekend activity suggests possible mixed validator behavior or synthetic weekend feeds.\")\nelse:\n    print(\"High weekend submissions indicate unexpected validator behavior or synthetic FX data.\")\n\nTotal weekend submissions: 0\nTotal weekday submissions: 157222\nWeekend submissions as % of total: 0.0%\nVery low weekend submissions suggest validators typically avoid weekend updates, aligning with expected FX market behavior.\n\n\nYahoo Finance Weekend Patterns (FX benchmarks)\n\nyahoo_info = results[\"yahoo_weekend_info\"]\nfor pair, df_y in yahoo_info.items():\n    print(f\"\\nFX Pair: {pair}\")\n    display(df_y)\n    weekend_quotes = df_y.filter(pl.col(\"weekday_num\") &gt;= 5)[\"count_quotes\"].sum()\n    print(f\"Total weekend Yahoo quotes: {weekend_quotes}\")\n\n    if weekend_quotes == 0:\n        print(f\"Yahoo data confirms no weekend activity for {pair}.\")\n    else:\n        print(f\"Yahoo data shows unexpected weekend activity for {pair}.\")\n\n\nFX Pair: AUD-USD\n\n\n\nshape: (67, 4)\n\n\n\ndate_only\nweekday_num\ncount_quotes\nrange_close\n\n\ndate\ni8\nu32\nf64\n\n\n\n\n2025-01-23\n4\n1420\n0.004371\n\n\n2025-02-03\n1\n1417\n0.01459\n\n\n2025-02-26\n3\n1413\n0.005379\n\n\n2025-01-01\n3\n775\n0.000863\n\n\n2025-01-14\n2\n1419\n0.00404\n\n\n…\n…\n…\n…\n\n\n2025-02-06\n4\n1429\n0.003325\n\n\n2025-02-04\n2\n1423\n0.00879\n\n\n2025-03-17\n1\n1394\n0.006952\n\n\n2025-03-21\n5\n1279\n0.004681\n\n\n2025-01-06\n1\n1421\n0.008117\n\n\n\n\n\n\nTotal weekend Yahoo quotes: 17130\nYahoo data shows unexpected weekend activity for AUD-USD.\n\nFX Pair: CAD-USD\n\n\n\nshape: (67, 4)\n\n\n\ndate_only\nweekday_num\ncount_quotes\nrange_close\n\n\ndate\ni8\nu32\nf64\n\n\n\n\n2025-02-07\n5\n1320\n0.003655\n\n\n2025-01-14\n2\n1438\n0.003208\n\n\n2025-01-24\n5\n1316\n0.00343\n\n\n2025-02-24\n1\n1437\n0.004905\n\n\n2025-01-22\n3\n1433\n0.004933\n\n\n…\n…\n…\n…\n\n\n2025-03-19\n3\n1431\n0.002657\n\n\n2025-02-25\n2\n1437\n0.003831\n\n\n2025-03-13\n4\n1431\n0.004449\n\n\n2025-01-07\n2\n1437\n0.003622\n\n\n2025-03-26\n3\n1431\n0.003906\n\n\n\n\n\n\nTotal weekend Yahoo quotes: 17175\nYahoo data shows unexpected weekend activity for CAD-USD.\n\nFX Pair: EUR-USD\n\n\n\nshape: (67, 4)\n\n\n\ndate_only\nweekday_num\ncount_quotes\nrange_close\n\n\ndate\ni8\nu32\nf64\n\n\n\n\n2025-03-03\n1\n1432\n0.010915\n\n\n2025-02-25\n2\n1435\n0.006496\n\n\n2025-02-14\n5\n1337\n0.006594\n\n\n2025-01-10\n5\n1343\n0.009174\n\n\n2025-01-14\n2\n1432\n0.00697\n\n\n…\n…\n…\n…\n\n\n2025-02-20\n4\n1426\n0.008542\n\n\n2025-03-12\n3\n1426\n0.005234\n\n\n2025-03-20\n4\n1418\n0.010044\n\n\n2024-12-31\n2\n1360\n0.008092\n\n\n2025-01-13\n1\n1430\n0.009629\n\n\n\n\n\n\nTotal weekend Yahoo quotes: 17283\nYahoo data shows unexpected weekend activity for EUR-USD.\n\nFX Pair: GBP-USD\n\n\n\nshape: (67, 4)\n\n\n\ndate_only\nweekday_num\ncount_quotes\nrange_close\n\n\ndate\ni8\nu32\nf64\n\n\n\n\n2025-03-30\n7\n60\n0.001004\n\n\n2025-03-28\n5\n1286\n0.004609\n\n\n2025-01-17\n5\n1340\n0.008279\n\n\n2025-02-20\n4\n1426\n0.009328\n\n\n2025-03-11\n2\n1427\n0.009181\n\n\n…\n…\n…\n…\n\n\n2025-01-07\n2\n1436\n0.010181\n\n\n2025-03-24\n1\n1429\n0.007713\n\n\n2025-02-24\n1\n1429\n0.007763\n\n\n2025-03-03\n1\n1432\n0.013331\n\n\n2024-12-30\n1\n1432\n0.009888\n\n\n\n\n\n\nTotal weekend Yahoo quotes: 17283\nYahoo data shows unexpected weekend activity for GBP-USD.\n\nFX Pair: JPY-USD\n\n\n\nshape: (67, 4)\n\n\n\ndate_only\nweekday_num\ncount_quotes\nrange_close\n\n\ndate\ni8\nu32\nf64\n\n\n\n\n2025-03-10\n1\n1422\n0.000052\n\n\n2025-01-01\n3\n301\n0.000027\n\n\n2025-03-20\n4\n1426\n0.000034\n\n\n2025-03-13\n4\n1432\n0.000043\n\n\n2025-02-05\n3\n1437\n0.000095\n\n\n…\n…\n…\n…\n\n\n2025-03-12\n3\n1435\n0.000058\n\n\n2025-02-03\n1\n1438\n0.000077\n\n\n2025-02-13\n4\n1435\n0.000082\n\n\n2025-02-18\n2\n1435\n0.000033\n\n\n2025-02-11\n2\n1432\n0.000046\n\n\n\n\n\n\nTotal weekend Yahoo quotes: 17162\nYahoo data shows unexpected weekend activity for JPY-USD.\n\nFX Pair: SEK-USD\n\n\n\nshape: (67, 4)\n\n\n\ndate_only\nweekday_num\ncount_quotes\nrange_close\n\n\ndate\ni8\nu32\nf64\n\n\n\n\n2025-01-03\n5\n1315\n0.000458\n\n\n2025-03-06\n4\n1427\n0.001217\n\n\n2025-02-25\n2\n1437\n0.000916\n\n\n2025-03-27\n4\n1438\n0.000981\n\n\n2025-01-23\n4\n1372\n0.000476\n\n\n…\n…\n…\n…\n\n\n2025-03-04\n2\n1437\n0.001738\n\n\n2025-01-30\n4\n1374\n0.00073\n\n\n2025-01-29\n3\n1403\n0.000508\n\n\n2025-03-26\n3\n1428\n0.000589\n\n\n2025-03-17\n1\n1433\n0.000778\n\n\n\n\n\n\nTotal weekend Yahoo quotes: 17153\nYahoo data shows unexpected weekend activity for SEK-USD.\n\n\nList of all Validators and their Weekend Submission Ratios\n\ndf_sub_raw = results[\"df_submissions_raw\"]\n\ndf_validator_weekend = (\n    df_sub_raw\n    .with_columns(\n        (pl.col(\"weekday_num\") &gt;= 5).cast(pl.Int8).alias(\"is_weekend\")\n    )\n    .group_by(\"Validator Address\")\n    .agg([\n        pl.count().alias(\"total_submissions\"),\n        pl.col(\"is_weekend\").sum().alias(\"weekend_submissions\")\n    ])\n    .with_columns(\n        (pl.col(\"weekend_submissions\") / pl.col(\"total_submissions\"))\n        .alias(\"weekend_submission_ratio\")\n    )\n    .sort(\"weekend_submission_ratio\", descending=True)\n)\n\nfor row in df_validator_weekend.to_dicts():\n    print(\n        f\"Validator {row['Validator Address']}: \"\n        f\"total_submissions={row['total_submissions']}, \"\n        f\"weekend_submissions={row['weekend_submissions']}, \"\n        f\"weekend_submission_ratio={row['weekend_submission_ratio']:.2f}\"\n    )\n\nValidator 0xfD97FB8835d25740A2Da27c69762D74F6A931858: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x9d28e40E9Ec4789f9A0D17e421F76D8D0868EA44: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x22A76e194A49c9e5508Cd4A3E1cD555D088ECB08: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xD9fDab408dF7Ae751691BeC2efE3b713ba3f9C36: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x7232e75a8bFd8c9ab002BB3A00eAa885BC72A6dd: total_submissions=2877, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x19E356ebC20283fc74AF0BA4C179502A1F62fA7B: total_submissions=2833, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x6747c02DE7eb2099265e55715Ba2E03e8563D051: total_submissions=2840, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x358488a4EdCA493FCD87610dcd50c62c8A3Dd658: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x1476A65D7B5739dE1805d5130441A94022Ee49fe: total_submissions=2462, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xc5B9d978715F081E226cb28bADB7Ba4cde5f9775: total_submissions=2879, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x100E38f7BCEc53937BDd79ADE46F34362470577B: total_submissions=2876, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xE9FFF86CAdC3136b3D94948B8Fd23631EDaa2dE3: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x527192F3D2408C84087607b7feE1d0f907821E17: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xcdEed21b471b0Dc54faF74480A0E700fCc42a7b6: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x23b4Be9536F93b8D550214912fD0e38417Ff7209: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x00a96aaED75015Bb44cED878D927dcb15ec1FF54: total_submissions=2866, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x551f3300FCFE0e392178b3542c009948008B2a9F: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xC1F9acAF1824F6C906b35A0D2584D6E25077C7f5: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xDF2D0052ea56A860443039619f6DAe4434bc0Ac4: total_submissions=2879, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x5603caFE3313D0cf56Fd4bE4A2f606dD6E43F8Eb: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x197B2c44b887c4aC01243BDE7E4bBa8bd95BC3a8: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x718361fc3637199F24a2437331677D6B89a40519: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xE4686A4C6E63A8ab51B458c52EB779AEcf0B74f7: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xF9B38D02959379d43C764064dE201324d5e12931: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x4cD134001EEF0843B9c69Ba9569d11fDcF4bd495: total_submissions=2823, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x984A46Ec685Bb41A7BBb2bc39f80C78410ff4057: total_submissions=2877, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xB5d8be2AB4b6d7E6be7Ea28E91b370223a06289f: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x94470A842Ea4f44e668EB9C2AB81367b6Ce01772: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x791A7F840ac11841cCB0FaA968B2e3a0Db930fCe: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x3fe573552E14a0FC11Da25E43Fef11e16a785068: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x2928FE5b911BCAf837cAd93eB9626E86a189f1dd: total_submissions=2829, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xDCA5DFF3D42f2db3C18dBE823380A0A81db49A7E: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x3597d2D42f8Fbbc82E8b1046048773aD6DDB717E: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x8584A78A9b94f332A34BBf24D2AF83367Da31894: total_submissions=2879, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x24915749B793375a8C93090AF19928aFF1CAEcb6: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x26E2724dBD14Fbd52be430B97043AA4c83F05852: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xBBf36374eb23968F25aecAEbb97BF3118f3c2fEC: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x8f91e0ADF8065C3fFF92297267E02DF32C2978FF: total_submissions=2879, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xd625d50B0d087861c286d726eC51Cf4Bd9c54357: total_submissions=2856, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xbfDcAF35f52F9ef423ac8F2621F9eef8be6dEd17: total_submissions=2833, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xd61a48b0e11B0Dc6b7Bd713B1012563c52591BAA: total_submissions=2873, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xBE287C82A786218E008FF97320b08244BE4A282c: total_submissions=2879, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x94d28f08Ff81A80f4716C0a8EfC6CAC2Ec74d09E: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x383A3c437d3F12f60E5fC990119468D3561EfBfc: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x3AaF7817618728ffEF81898E11A3171C33faAE41: total_submissions=2874, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xdF239e0D5b4E6e820B0cFEF6972A90893c2073AB: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x831B837C3DA1B6c2AB68a690206bDfF368877E19: total_submissions=2863, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x1Be7f70BCf8393a7e4A5BcC66F6f15d6e35cfBBC: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x59031767f20EA8F4a3d90d33aB0DAA2ca469Fd9a: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xcf716b3930d7cf6f2ADAD90A27c39fDc9D643BBd: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x01F788E4371a70D579C178Ea7F48E04e8B2CD743: total_submissions=2837, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x6a395dE946c0493157404E2b1947493c633f569E: total_submissions=2874, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x36142A4f36974e2935192A1111C39330aA296D3C: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x99E2B4B27BDe92b42D04B6CF302cF564D2C13b74: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x5E17e837DcBa2728C94f95c38fA8a47CB9C8818F: total_submissions=2876, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xEf0Ba5e345C2C3937df5667A870Aae5105CAa3a5: total_submissions=2879, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x9C7dAABb5101623340C925CFD6fF74088ff5672e: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0x64F83c2538A646A550Ad9bEEb63427a377359DEE: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xf34CD6c09a59d7D3d1a6C3dC231a7834E5615D6A: total_submissions=2834, weekend_submissions=0, weekend_submission_ratio=0.00\nValidator 0xf10f56Bf0A28E0737c7e6bB0aF92f3DDad34aE6a: total_submissions=2880, weekend_submissions=0, weekend_submission_ratio=0.00\n\n\nPlease note, total_submissions represents the total number of individual price-update rows this validator sent during the study period. weekend_submissions is how many of those rows were posted on Saturdays or Sundays. weekend_submission_ratio is the proportion of weekend submissions (0 – 1 scale); higher values indicate the validator updates more frequently on weekends relative to weekdays.",
    "crumbs": [
      "Notebooks",
      "Issue 8"
    ]
  },
  {
    "objectID": "notebooks/issue_1.html",
    "href": "notebooks/issue_1.html",
    "title": "Issue 1",
    "section": "",
    "text": "This notebook documents the analysis for Issue #1: Missing or Null Submissions in the Autonity Oracle data. It covers:\n\nWhat is this issue about?\n\nWhy conduct this issue analysis?\n\nHow to conduct this issue analysis?\n\nWhat are the results?\n\n\n\n\nIn the Oracle system, validators are expected to submit FX and token price data. However, some rows in the submission CSV files have missing or null values for one or more price fields. Examples include:\n\nRows that only have a timestamp and validator address (no numeric values).\n\nSome currency pairs or token pairs are entirely missing in certain rows.\n\nZero or placeholder values that suggest incomplete submissions.\n\nThis analysis investigates the frequency and patterns of these missing or null submissions.\n\n\n\n\n\nReliability: Missing/null submissions can degrade the Oracle’s usefulness if data is incomplete when aggregated on-chain.\n\nPatterns & Evidence: Finding consistent patterns (which validators, which days/times) provides concrete evidence to the foundation and technical teams.\n\nConsistency: Understanding whether missing data spikes on weekends or for certain validators can be critical for consistency in data quality.\n\n\n\n\n\nUse Python with the Polars library (v1.24.0) to:\n\nLoad and preprocess Oracle submission CSV files.\nParse the timestamps from strings to actual datetimes (and derive day-of-week).\n\nDefine coverage modes:\n\nALL-FX mode: A row is considered valid if all FX and token columns are non-null.\n\nANY-FX mode: A row is considered valid if at least one FX or token column is non-null.\n\nCompute coverage metrics:\n\nFor each timestamp, calculate how many validators are “present” in the CSV (i.e. have a row for that timestamp) and how many actually submit valid data (per the ALL-FX or ANY-FX definition).\nDefine a timestamp as “fully covered” if at least 90% of the present validators submitted valid data.\nAt the daily level, count how many timestamps met that 90% threshold (“full coverage”) versus those that did not (“incomplete coverage”).\n\nCheck weekend vs. weekday patterns by labeling days Monday=0 through Sunday=6, then aggregating coverage differences over weekends (Saturday=5, Sunday=6) vs. weekdays.\n\nBelow is the script to perform the analysis:\n\nimport polars as pl\nimport glob\nimport statistics\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndef load_and_preprocess(submission_glob: str):\n    \"\"\"\n    Loads Oracle Submission CSVs and returns a Polars DataFrame.\n    \"\"\"\n    files = sorted(glob.glob(submission_glob))\n    if not files:\n        raise ValueError(f\"No CSV files found matching pattern {submission_glob}\")\n\n    lf_list = []\n    for f in files:\n        lf_temp = pl.scan_csv(\n            f,\n            dtypes={\"Timestamp\": pl.Utf8},\n            null_values=[\"\"],\n            ignore_errors=True,\n        )\n        lf_list.append(lf_temp)\n\n    lf = pl.concat(lf_list)\n\n    lf = lf.with_columns(\n        pl.col(\"Timestamp\")\n        .str.strptime(pl.Datetime, strict=False)\n        .alias(\"Timestamp_dt\")\n    )\n\n    lf = lf.with_columns(\n        [\n            pl.col(\"Timestamp_dt\").cast(pl.Date).alias(\"date_only\"),\n            pl.col(\"Timestamp_dt\").dt.weekday().alias(\"weekday_num\"),\n        ]\n    )\n\n    return lf\n\n\ndef compute_coverage_metrics(\n    df: pl.DataFrame,\n    fx_cols: list[str],\n    autonity_cols: list[str],\n    use_all_fx_required: bool = True,\n    coverage_threshold: float = 0.9,  # 90% coverage default\n):\n    \"\"\"\n    Compute coverage metrics, but treat a timestamp as \"fully covered\" only if\n    at least `coverage_threshold` fraction of validators present at that\n    timestamp provide valid data (ALL-FX or ANY-FX).\n    \"\"\"\n\n    if use_all_fx_required:\n        fx_expr = pl.fold(\n            acc=pl.lit(True),\n            function=lambda acc, x: acc & x,\n            exprs=[pl.col(c).is_not_null() for c in fx_cols],\n        ).alias(\"submitted_fx_data\")\n\n        autonity_expr = pl.fold(\n            acc=pl.lit(True),\n            function=lambda acc, x: acc & x,\n            exprs=[pl.col(c).is_not_null() for c in autonity_cols],\n        ).alias(\"submitted_autonity_data\")\n    else:\n        fx_expr = pl.fold(\n            acc=pl.lit(False),\n            function=lambda acc, x: acc | x,\n            exprs=[pl.col(c).is_not_null() for c in fx_cols],\n        ).alias(\"submitted_fx_data\")\n\n        autonity_expr = pl.fold(\n            acc=pl.lit(False),\n            function=lambda acc, x: acc | x,\n            exprs=[pl.col(c).is_not_null() for c in autonity_cols],\n        ).alias(\"submitted_autonity_data\")\n\n    lf = df.lazy().with_columns([fx_expr, autonity_expr])\n\n    if use_all_fx_required:\n        condition_expr = pl.col(\"submitted_fx_data\") & pl.col(\"submitted_autonity_data\")\n    else:\n        condition_expr = pl.col(\"submitted_fx_data\") | pl.col(\"submitted_autonity_data\")\n\n    lf = lf.with_columns(condition_expr.alias(\"any_submitted\"))\n\n    lf_per_addr = (\n        lf.group_by([\"Timestamp_dt\", \"Validator Address\"])\n        .agg(\n            [\n                pl.any(\"any_submitted\").alias(\"any_submitted\"),\n                pl.any(\"submitted_fx_data\").alias(\"fx_submitted\"),\n                pl.any(\"submitted_autonity_data\").alias(\"autonity_submitted\"),\n            ]\n        )\n        .rename({\"Timestamp_dt\": \"timestamp\"})\n    )\n\n    lf_per_addr = lf_per_addr.with_columns(\n        pl.col(\"timestamp\").cast(pl.Date).alias(\"date_only\")\n    )\n\n    lf_timestamp_coverage = (\n        lf_per_addr.group_by([\"date_only\", \"timestamp\"])\n        .agg(\n            [\n                pl.count(\"Validator Address\").alias(\"validators_seen\"),\n                pl.sum(\"any_submitted\").alias(\"num_submitted_any\"),\n                pl.sum(\"fx_submitted\").alias(\"num_submitted_fx\"),\n                pl.sum(\"autonity_submitted\").alias(\"num_submitted_autonity\"),\n            ]\n        )\n        .with_columns(\n            (pl.col(\"num_submitted_any\") / pl.col(\"validators_seen\"))\n            .fill_null(0.0)\n            .alias(\"coverage_ratio\")\n        )\n        .with_columns(\n            (\n                (pl.col(\"coverage_ratio\") &lt; coverage_threshold).cast(pl.Int8)\n            ).alias(\"num_missing_any\")\n        )\n    )\n\n    df_timestamp_coverage = lf_timestamp_coverage.collect().sort(\n        [\"date_only\", \"timestamp\"]\n    )\n\n    lf_day_coverage = (\n        lf_timestamp_coverage.group_by(\"date_only\")\n        .agg(\n            [\n                pl.count(\"timestamp\").alias(\"num_timestamps_that_day\"),\n                (pl.col(\"num_missing_any\").eq(0).cast(pl.Int64))\n                .sum()\n                .alias(\"num_timestamps_full_coverage\"),\n                (pl.col(\"num_missing_any\").gt(0).cast(pl.Int64))\n                .sum()\n                .alias(\"num_timestamps_incomplete_coverage\"),\n            ]\n        )\n        .with_columns(\n            [\n                (\n                    pl.col(\"num_timestamps_full_coverage\")\n                    / pl.col(\"num_timestamps_that_day\")\n                ).alias(\"fraction_full_coverage\"),\n                (\n                    pl.col(\"num_timestamps_incomplete_coverage\")\n                    / pl.col(\"num_timestamps_that_day\")\n                ).alias(\"fraction_incomplete_coverage\"),\n            ]\n        )\n    )\n\n    df_day_coverage = lf_day_coverage.collect().sort(\"date_only\")\n\n    lf_missing_by_val = lf_per_addr.with_columns(\n        (pl.col(\"any_submitted\") == False).alias(\"is_missing\")\n    )\n\n    lf_validator_missing_stats = (\n        lf_missing_by_val.group_by(\"Validator Address\")\n        .agg(\n            [\n                pl.count(\"timestamp\").alias(\"total_timestamps_encountered\"),\n                pl.sum(\"is_missing\").alias(\"missing_count\"),\n            ]\n        )\n        .with_columns(\n            (\n                pl.col(\"missing_count\") / pl.col(\"total_timestamps_encountered\")\n            ).alias(\"fraction_missing\")\n        )\n    )\n\n    df_validator_missing_stats = lf_validator_missing_stats.collect().sort(\n        \"fraction_missing\", descending=True\n    )\n\n    mode_str = \"ALL\" if use_all_fx_required else \"ANY\"\n    return df_timestamp_coverage, df_day_coverage, df_validator_missing_stats, mode_str\n\n\ndef check_weekend_patterns(df_timestamp_coverage: pl.DataFrame, df_source: pl.DataFrame):\n    \"\"\"\n    Compare coverage on weekends (Sat=5, Sun=6) vs. weekdays (Mon-Fri=0..4).\n    \"\"\"\n    df_day_and_week = (\n        df_source.lazy()\n        .group_by(\"date_only\")\n        .agg(\n            [\n                pl.first(\"weekday_num\").alias(\"weekday_num\"),\n            ]\n        )\n        .collect()\n    )\n\n    df_cov_extended = df_timestamp_coverage.join(\n        df_day_and_week, on=\"date_only\", how=\"left\"\n    )\n\n    df_weekday_cov = (\n        df_cov_extended.lazy()\n        .group_by(\"weekday_num\")\n        .agg(\n            [\n                pl.count(\"timestamp\").alias(\"num_timestamps\"),\n                (pl.col(\"num_missing_any\").eq(0).cast(pl.Int64))\n                .sum()\n                .alias(\"num_ts_full_coverage\"),\n            ]\n        )\n        .with_columns(\n            [\n                (\n                    pl.col(\"num_ts_full_coverage\") / pl.col(\"num_timestamps\")\n                ).alias(\"fraction_full_cov\"),\n            ]\n        )\n        .collect()\n        .sort(\"weekday_num\")\n    )\n\n    def weighted_fraction(df: pl.DataFrame) -&gt; float:\n        if df.is_empty():\n            return 0.0\n        total_ts = df[\"num_timestamps\"].sum()\n        if total_ts == 0:\n            return 0.0\n        full_cov = df[\"num_ts_full_coverage\"].sum()\n        return float(full_cov / total_ts)\n\n    weekend_data = df_weekday_cov.filter(pl.col(\"weekday_num\") &gt;= 5)\n    weekday_data = df_weekday_cov.filter(pl.col(\"weekday_num\") &lt; 5)\n\n    weekend_cov = weighted_fraction(weekend_data)\n    weekday_cov = weighted_fraction(weekday_data)\n\n    return {\n        \"df_weekday_cov\": df_weekday_cov,\n        \"weekend_fraction_full_cov\": weekend_cov,\n        \"weekday_fraction_full_cov\": weekday_cov,\n    }\n\n\ndef analyze_missing_submissions_both_modes(submission_glob: str):\n    \"\"\"\n    Main analysis function.\n    \"\"\"\n    df_all_data = load_and_preprocess(submission_glob)\n\n    fx_cols = [\n        \"AUD-USD Price\",\"AUD-USD Confidence\",\n        \"CAD-USD Price\",\"CAD-USD Confidence\",\n        \"EUR-USD Price\",\"EUR-USD Confidence\",\n        \"GBP-USD Price\",\"GBP-USD Confidence\",\n        \"JPY-USD Price\",\"JPY-USD Confidence\",\n        \"SEK-USD Price\",\"SEK-USD Confidence\",\n    ]\n    autonity_cols = [\n        \"ATN-USD Price\",\"ATN-USD Confidence\",\n        \"NTN-USD Price\",\"NTN-USD Confidence\",\n        \"NTN-ATN Price\",\"NTN-ATN Confidence\",\n    ]\n\n    (\n        df_ts_cov_all,\n        df_day_cov_all,\n        df_val_missing_all,\n        mode_str_all,\n    ) = compute_coverage_metrics(df_all_data, fx_cols, autonity_cols, True)\n\n    (\n        df_ts_cov_any,\n        df_day_cov_any,\n        df_val_missing_any,\n        mode_str_any,\n    ) = compute_coverage_metrics(df_all_data, fx_cols, autonity_cols, False)\n\n    weekend_info_all = check_weekend_patterns(df_ts_cov_all, df_all_data)\n    weekend_info_any = check_weekend_patterns(df_ts_cov_any, df_all_data)\n\n    return {\n        \"df_all_data\": df_all_data,\n        \"ALL\": {\n            \"df_timestamp_coverage\": df_ts_cov_all,\n            \"df_day_coverage\": df_day_cov_all,\n            \"df_validator_missing\": df_val_missing_all,\n            \"weekend_info\": weekend_info_all,\n        },\n        \"ANY\": {\n            \"df_timestamp_coverage\": df_ts_cov_any,\n            \"df_day_coverage\": df_day_cov_any,\n            \"df_validator_missing\": df_val_missing_any,\n            \"weekend_info\": weekend_info_any,\n        },\n    }\n\n\nresults = analyze_missing_submissions_both_modes(\n    submission_glob=\"../submission-data/Oracle_Submission_*.csv\"\n)\n\n\n\n\n\nBelow are directly reference outcomes from the results dictionary obtained by executing the analysis. The results shown will automatically update when re-running this notebook with new or updated datasets.\nDaily Coverage Analysis\nALL-FX Mode\n\n# Display daily coverage for ALL-FX mode\nresults[\"ALL\"][\"df_day_coverage\"]\n\nall_day_cov = results[\"ALL\"][\"df_day_coverage\"][\"fraction_full_coverage\"]\navg_all_cov = statistics.mean(all_day_cov) * 100\nprint(f\"Average daily full coverage (ALL-FX mode): {avg_all_cov:.1f}%\")\nif avg_all_cov &lt; 50:\n    print(\"Coverage is relatively low, suggesting many validators frequently miss submitting complete data.\")\nelse:\n    print(\"Coverage is reasonably good, indicating validators frequently submit complete data.\")\n\nAverage daily full coverage (ALL-FX mode): 97.5%\nCoverage is reasonably good, indicating validators frequently submit complete data.\n\n\nThe table and statistics above summarize how frequently all validators submitted complete data each day.\nANY-FX Mode\n\n# Display daily coverage for ANY-FX mode\nresults[\"ANY\"][\"df_day_coverage\"]\n\nany_day_cov = results[\"ANY\"][\"df_day_coverage\"][\"fraction_full_coverage\"]\navg_any_cov = statistics.mean(any_day_cov) * 100\ncoverage_difference = avg_any_cov - avg_all_cov\n\nprint(f\"Average daily full coverage (ANY-FX mode): {avg_any_cov:.1f}%\")\nprint(f\"Coverage difference between ANY-FX and ALL-FX modes: {coverage_difference:.1f}%\")\n\nif coverage_difference &gt; 20:\n    print(\"A substantial coverage improvement in ANY-FX mode indicates validators frequently provide partial submissions rather than complete ones.\")\nelse:\n    print(\"The small difference suggests that validators typically provide complete submissions or none at all.\")\n\nAverage daily full coverage (ANY-FX mode): 97.5%\nCoverage difference between ANY-FX and ALL-FX modes: 0.0%\nThe small difference suggests that validators typically provide complete submissions or none at all.\n\n\nThis comparison highlights the impact of submission requirements (complete vs. partial) on coverage.\nWeekend vs. Weekday Coverage\n\nweekend_cov = results[\"ALL\"][\"weekend_info\"][\"weekend_fraction_full_cov\"] * 100\nweekday_cov = results[\"ALL\"][\"weekend_info\"][\"weekday_fraction_full_cov\"] * 100\nprint(f\"Weekend coverage: {weekend_cov:.1f}%, Weekday coverage: {weekday_cov:.1f}%\")\n\nif weekend_cov &gt; weekday_cov + 5:\n    print(\"Significantly better coverage on weekends; potential scheduling issues on weekdays.\")\nelif weekday_cov &gt; weekend_cov + 5:\n    print(\"Significantly better coverage on weekdays; validators might be inactive or less reliable during weekends.\")\nelse:\n    print(\"No major difference in coverage between weekends and weekdays; submission patterns appear relatively uniform.\")\n\nWeekend coverage: 0.0%, Weekday coverage: 97.5%\nSignificantly better coverage on weekdays; validators might be inactive or less reliable during weekends.\n\n\nList of all Validators and their Missing Rates\n\ndf_all_missing = results[\"ALL\"][\"df_validator_missing\"]\n\nfor row in df_all_missing.iter_rows(named=True):\n    addr = row[\"Validator Address\"]\n    total = row[\"total_timestamps_encountered\"]\n    missing = row[\"missing_count\"]\n    fraction = row[\"fraction_missing\"] * 100\n    print(\n        f\"Validator {addr}: total={total}, missing={missing}, fraction_missing={fraction:.1f}%\"\n    )\n\nValidator 0x3fe573552E14a0FC11Da25E43Fef11e16a785068: total=2880, missing=2880, fraction_missing=100.0%\nValidator 0xdF239e0D5b4E6e820B0cFEF6972A90893c2073AB: total=2880, missing=2880, fraction_missing=100.0%\nValidator 0xd625d50B0d087861c286d726eC51Cf4Bd9c54357: total=2856, missing=2856, fraction_missing=100.0%\nValidator 0x26E2724dBD14Fbd52be430B97043AA4c83F05852: total=2880, missing=2880, fraction_missing=100.0%\nValidator 0x100E38f7BCEc53937BDd79ADE46F34362470577B: total=2876, missing=2876, fraction_missing=100.0%\nValidator 0x4cD134001EEF0843B9c69Ba9569d11fDcF4bd495: total=2823, missing=52, fraction_missing=1.8%\nValidator 0x19E356ebC20283fc74AF0BA4C179502A1F62fA7B: total=2833, missing=47, fraction_missing=1.7%\nValidator 0xbfDcAF35f52F9ef423ac8F2621F9eef8be6dEd17: total=2833, missing=47, fraction_missing=1.7%\nValidator 0xf34CD6c09a59d7D3d1a6C3dC231a7834E5615D6A: total=2834, missing=46, fraction_missing=1.6%\nValidator 0x2928FE5b911BCAf837cAd93eB9626E86a189f1dd: total=2829, missing=44, fraction_missing=1.6%\nValidator 0x01F788E4371a70D579C178Ea7F48E04e8B2CD743: total=2837, missing=43, fraction_missing=1.5%\nValidator 0x6747c02DE7eb2099265e55715Ba2E03e8563D051: total=2840, missing=40, fraction_missing=1.4%\nValidator 0x831B837C3DA1B6c2AB68a690206bDfF368877E19: total=2863, missing=17, fraction_missing=0.6%\nValidator 0xd61a48b0e11B0Dc6b7Bd713B1012563c52591BAA: total=2873, missing=7, fraction_missing=0.2%\nValidator 0x6a395dE946c0493157404E2b1947493c633f569E: total=2874, missing=6, fraction_missing=0.2%\nValidator 0x5E17e837DcBa2728C94f95c38fA8a47CB9C8818F: total=2876, missing=4, fraction_missing=0.1%\nValidator 0x984A46Ec685Bb41A7BBb2bc39f80C78410ff4057: total=2877, missing=3, fraction_missing=0.1%\nValidator 0x1476A65D7B5739dE1805d5130441A94022Ee49fe: total=2462, missing=1, fraction_missing=0.0%\nValidator 0x00a96aaED75015Bb44cED878D927dcb15ec1FF54: total=2866, missing=1, fraction_missing=0.0%\nValidator 0x7232e75a8bFd8c9ab002BB3A00eAa885BC72A6dd: total=2877, missing=1, fraction_missing=0.0%\nValidator 0xEf0Ba5e345C2C3937df5667A870Aae5105CAa3a5: total=2879, missing=1, fraction_missing=0.0%\nValidator 0xBE287C82A786218E008FF97320b08244BE4A282c: total=2879, missing=1, fraction_missing=0.0%\nValidator 0x8f91e0ADF8065C3fFF92297267E02DF32C2978FF: total=2879, missing=1, fraction_missing=0.0%\nValidator 0xDF2D0052ea56A860443039619f6DAe4434bc0Ac4: total=2879, missing=1, fraction_missing=0.0%\nValidator 0x23b4Be9536F93b8D550214912fD0e38417Ff7209: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x551f3300FCFE0e392178b3542c009948008B2a9F: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x59031767f20EA8F4a3d90d33aB0DAA2ca469Fd9a: total=2880, missing=0, fraction_missing=0.0%\nValidator 0xD9fDab408dF7Ae751691BeC2efE3b713ba3f9C36: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x1Be7f70BCf8393a7e4A5BcC66F6f15d6e35cfBBC: total=2880, missing=0, fraction_missing=0.0%\nValidator 0xBBf36374eb23968F25aecAEbb97BF3118f3c2fEC: total=2880, missing=0, fraction_missing=0.0%\nValidator 0xE9FFF86CAdC3136b3D94948B8Fd23631EDaa2dE3: total=2880, missing=0, fraction_missing=0.0%\nValidator 0xc5B9d978715F081E226cb28bADB7Ba4cde5f9775: total=2879, missing=0, fraction_missing=0.0%\nValidator 0x3AaF7817618728ffEF81898E11A3171C33faAE41: total=2874, missing=0, fraction_missing=0.0%\nValidator 0xcdEed21b471b0Dc54faF74480A0E700fCc42a7b6: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x94d28f08Ff81A80f4716C0a8EfC6CAC2Ec74d09E: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x99E2B4B27BDe92b42D04B6CF302cF564D2C13b74: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x24915749B793375a8C93090AF19928aFF1CAEcb6: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x383A3c437d3F12f60E5fC990119468D3561EfBfc: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x22A76e194A49c9e5508Cd4A3E1cD555D088ECB08: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x9d28e40E9Ec4789f9A0D17e421F76D8D0868EA44: total=2880, missing=0, fraction_missing=0.0%\nValidator 0xf10f56Bf0A28E0737c7e6bB0aF92f3DDad34aE6a: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x64F83c2538A646A550Ad9bEEb63427a377359DEE: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x9C7dAABb5101623340C925CFD6fF74088ff5672e: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x791A7F840ac11841cCB0FaA968B2e3a0Db930fCe: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x527192F3D2408C84087607b7feE1d0f907821E17: total=2880, missing=0, fraction_missing=0.0%\nValidator 0xcf716b3930d7cf6f2ADAD90A27c39fDc9D643BBd: total=2880, missing=0, fraction_missing=0.0%\nValidator 0xfD97FB8835d25740A2Da27c69762D74F6A931858: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x94470A842Ea4f44e668EB9C2AB81367b6Ce01772: total=2880, missing=0, fraction_missing=0.0%\nValidator 0xDCA5DFF3D42f2db3C18dBE823380A0A81db49A7E: total=2880, missing=0, fraction_missing=0.0%\nValidator 0xB5d8be2AB4b6d7E6be7Ea28E91b370223a06289f: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x197B2c44b887c4aC01243BDE7E4bBa8bd95BC3a8: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x718361fc3637199F24a2437331677D6B89a40519: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x358488a4EdCA493FCD87610dcd50c62c8A3Dd658: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x36142A4f36974e2935192A1111C39330aA296D3C: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x3597d2D42f8Fbbc82E8b1046048773aD6DDB717E: total=2880, missing=0, fraction_missing=0.0%\nValidator 0xF9B38D02959379d43C764064dE201324d5e12931: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x5603caFE3313D0cf56Fd4bE4A2f606dD6E43F8Eb: total=2880, missing=0, fraction_missing=0.0%\nValidator 0xE4686A4C6E63A8ab51B458c52EB779AEcf0B74f7: total=2880, missing=0, fraction_missing=0.0%\nValidator 0xC1F9acAF1824F6C906b35A0D2584D6E25077C7f5: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x8584A78A9b94f332A34BBf24D2AF83367Da31894: total=2879, missing=0, fraction_missing=0.0%\n\n\nPlease note, total is the number of timestamp slots at which this validator was expected to submit. missing indicates how many of those slots were unfilled (i.e. the validator did not provide the required data for that timestamp). fraction_missing is the percentage of timestamps that were missing out of the total encountered.",
    "crumbs": [
      "Notebooks",
      "Issue 1"
    ]
  },
  {
    "objectID": "notebooks/issue_1.html#missing-or-null-submissions",
    "href": "notebooks/issue_1.html#missing-or-null-submissions",
    "title": "Issue 1",
    "section": "",
    "text": "This notebook documents the analysis for Issue #1: Missing or Null Submissions in the Autonity Oracle data. It covers:\n\nWhat is this issue about?\n\nWhy conduct this issue analysis?\n\nHow to conduct this issue analysis?\n\nWhat are the results?\n\n\n\n\nIn the Oracle system, validators are expected to submit FX and token price data. However, some rows in the submission CSV files have missing or null values for one or more price fields. Examples include:\n\nRows that only have a timestamp and validator address (no numeric values).\n\nSome currency pairs or token pairs are entirely missing in certain rows.\n\nZero or placeholder values that suggest incomplete submissions.\n\nThis analysis investigates the frequency and patterns of these missing or null submissions.\n\n\n\n\n\nReliability: Missing/null submissions can degrade the Oracle’s usefulness if data is incomplete when aggregated on-chain.\n\nPatterns & Evidence: Finding consistent patterns (which validators, which days/times) provides concrete evidence to the foundation and technical teams.\n\nConsistency: Understanding whether missing data spikes on weekends or for certain validators can be critical for consistency in data quality.\n\n\n\n\n\nUse Python with the Polars library (v1.24.0) to:\n\nLoad and preprocess Oracle submission CSV files.\nParse the timestamps from strings to actual datetimes (and derive day-of-week).\n\nDefine coverage modes:\n\nALL-FX mode: A row is considered valid if all FX and token columns are non-null.\n\nANY-FX mode: A row is considered valid if at least one FX or token column is non-null.\n\nCompute coverage metrics:\n\nFor each timestamp, calculate how many validators are “present” in the CSV (i.e. have a row for that timestamp) and how many actually submit valid data (per the ALL-FX or ANY-FX definition).\nDefine a timestamp as “fully covered” if at least 90% of the present validators submitted valid data.\nAt the daily level, count how many timestamps met that 90% threshold (“full coverage”) versus those that did not (“incomplete coverage”).\n\nCheck weekend vs. weekday patterns by labeling days Monday=0 through Sunday=6, then aggregating coverage differences over weekends (Saturday=5, Sunday=6) vs. weekdays.\n\nBelow is the script to perform the analysis:\n\nimport polars as pl\nimport glob\nimport statistics\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndef load_and_preprocess(submission_glob: str):\n    \"\"\"\n    Loads Oracle Submission CSVs and returns a Polars DataFrame.\n    \"\"\"\n    files = sorted(glob.glob(submission_glob))\n    if not files:\n        raise ValueError(f\"No CSV files found matching pattern {submission_glob}\")\n\n    lf_list = []\n    for f in files:\n        lf_temp = pl.scan_csv(\n            f,\n            dtypes={\"Timestamp\": pl.Utf8},\n            null_values=[\"\"],\n            ignore_errors=True,\n        )\n        lf_list.append(lf_temp)\n\n    lf = pl.concat(lf_list)\n\n    lf = lf.with_columns(\n        pl.col(\"Timestamp\")\n        .str.strptime(pl.Datetime, strict=False)\n        .alias(\"Timestamp_dt\")\n    )\n\n    lf = lf.with_columns(\n        [\n            pl.col(\"Timestamp_dt\").cast(pl.Date).alias(\"date_only\"),\n            pl.col(\"Timestamp_dt\").dt.weekday().alias(\"weekday_num\"),\n        ]\n    )\n\n    return lf\n\n\ndef compute_coverage_metrics(\n    df: pl.DataFrame,\n    fx_cols: list[str],\n    autonity_cols: list[str],\n    use_all_fx_required: bool = True,\n    coverage_threshold: float = 0.9,  # 90% coverage default\n):\n    \"\"\"\n    Compute coverage metrics, but treat a timestamp as \"fully covered\" only if\n    at least `coverage_threshold` fraction of validators present at that\n    timestamp provide valid data (ALL-FX or ANY-FX).\n    \"\"\"\n\n    if use_all_fx_required:\n        fx_expr = pl.fold(\n            acc=pl.lit(True),\n            function=lambda acc, x: acc & x,\n            exprs=[pl.col(c).is_not_null() for c in fx_cols],\n        ).alias(\"submitted_fx_data\")\n\n        autonity_expr = pl.fold(\n            acc=pl.lit(True),\n            function=lambda acc, x: acc & x,\n            exprs=[pl.col(c).is_not_null() for c in autonity_cols],\n        ).alias(\"submitted_autonity_data\")\n    else:\n        fx_expr = pl.fold(\n            acc=pl.lit(False),\n            function=lambda acc, x: acc | x,\n            exprs=[pl.col(c).is_not_null() for c in fx_cols],\n        ).alias(\"submitted_fx_data\")\n\n        autonity_expr = pl.fold(\n            acc=pl.lit(False),\n            function=lambda acc, x: acc | x,\n            exprs=[pl.col(c).is_not_null() for c in autonity_cols],\n        ).alias(\"submitted_autonity_data\")\n\n    lf = df.lazy().with_columns([fx_expr, autonity_expr])\n\n    if use_all_fx_required:\n        condition_expr = pl.col(\"submitted_fx_data\") & pl.col(\"submitted_autonity_data\")\n    else:\n        condition_expr = pl.col(\"submitted_fx_data\") | pl.col(\"submitted_autonity_data\")\n\n    lf = lf.with_columns(condition_expr.alias(\"any_submitted\"))\n\n    lf_per_addr = (\n        lf.group_by([\"Timestamp_dt\", \"Validator Address\"])\n        .agg(\n            [\n                pl.any(\"any_submitted\").alias(\"any_submitted\"),\n                pl.any(\"submitted_fx_data\").alias(\"fx_submitted\"),\n                pl.any(\"submitted_autonity_data\").alias(\"autonity_submitted\"),\n            ]\n        )\n        .rename({\"Timestamp_dt\": \"timestamp\"})\n    )\n\n    lf_per_addr = lf_per_addr.with_columns(\n        pl.col(\"timestamp\").cast(pl.Date).alias(\"date_only\")\n    )\n\n    lf_timestamp_coverage = (\n        lf_per_addr.group_by([\"date_only\", \"timestamp\"])\n        .agg(\n            [\n                pl.count(\"Validator Address\").alias(\"validators_seen\"),\n                pl.sum(\"any_submitted\").alias(\"num_submitted_any\"),\n                pl.sum(\"fx_submitted\").alias(\"num_submitted_fx\"),\n                pl.sum(\"autonity_submitted\").alias(\"num_submitted_autonity\"),\n            ]\n        )\n        .with_columns(\n            (pl.col(\"num_submitted_any\") / pl.col(\"validators_seen\"))\n            .fill_null(0.0)\n            .alias(\"coverage_ratio\")\n        )\n        .with_columns(\n            (\n                (pl.col(\"coverage_ratio\") &lt; coverage_threshold).cast(pl.Int8)\n            ).alias(\"num_missing_any\")\n        )\n    )\n\n    df_timestamp_coverage = lf_timestamp_coverage.collect().sort(\n        [\"date_only\", \"timestamp\"]\n    )\n\n    lf_day_coverage = (\n        lf_timestamp_coverage.group_by(\"date_only\")\n        .agg(\n            [\n                pl.count(\"timestamp\").alias(\"num_timestamps_that_day\"),\n                (pl.col(\"num_missing_any\").eq(0).cast(pl.Int64))\n                .sum()\n                .alias(\"num_timestamps_full_coverage\"),\n                (pl.col(\"num_missing_any\").gt(0).cast(pl.Int64))\n                .sum()\n                .alias(\"num_timestamps_incomplete_coverage\"),\n            ]\n        )\n        .with_columns(\n            [\n                (\n                    pl.col(\"num_timestamps_full_coverage\")\n                    / pl.col(\"num_timestamps_that_day\")\n                ).alias(\"fraction_full_coverage\"),\n                (\n                    pl.col(\"num_timestamps_incomplete_coverage\")\n                    / pl.col(\"num_timestamps_that_day\")\n                ).alias(\"fraction_incomplete_coverage\"),\n            ]\n        )\n    )\n\n    df_day_coverage = lf_day_coverage.collect().sort(\"date_only\")\n\n    lf_missing_by_val = lf_per_addr.with_columns(\n        (pl.col(\"any_submitted\") == False).alias(\"is_missing\")\n    )\n\n    lf_validator_missing_stats = (\n        lf_missing_by_val.group_by(\"Validator Address\")\n        .agg(\n            [\n                pl.count(\"timestamp\").alias(\"total_timestamps_encountered\"),\n                pl.sum(\"is_missing\").alias(\"missing_count\"),\n            ]\n        )\n        .with_columns(\n            (\n                pl.col(\"missing_count\") / pl.col(\"total_timestamps_encountered\")\n            ).alias(\"fraction_missing\")\n        )\n    )\n\n    df_validator_missing_stats = lf_validator_missing_stats.collect().sort(\n        \"fraction_missing\", descending=True\n    )\n\n    mode_str = \"ALL\" if use_all_fx_required else \"ANY\"\n    return df_timestamp_coverage, df_day_coverage, df_validator_missing_stats, mode_str\n\n\ndef check_weekend_patterns(df_timestamp_coverage: pl.DataFrame, df_source: pl.DataFrame):\n    \"\"\"\n    Compare coverage on weekends (Sat=5, Sun=6) vs. weekdays (Mon-Fri=0..4).\n    \"\"\"\n    df_day_and_week = (\n        df_source.lazy()\n        .group_by(\"date_only\")\n        .agg(\n            [\n                pl.first(\"weekday_num\").alias(\"weekday_num\"),\n            ]\n        )\n        .collect()\n    )\n\n    df_cov_extended = df_timestamp_coverage.join(\n        df_day_and_week, on=\"date_only\", how=\"left\"\n    )\n\n    df_weekday_cov = (\n        df_cov_extended.lazy()\n        .group_by(\"weekday_num\")\n        .agg(\n            [\n                pl.count(\"timestamp\").alias(\"num_timestamps\"),\n                (pl.col(\"num_missing_any\").eq(0).cast(pl.Int64))\n                .sum()\n                .alias(\"num_ts_full_coverage\"),\n            ]\n        )\n        .with_columns(\n            [\n                (\n                    pl.col(\"num_ts_full_coverage\") / pl.col(\"num_timestamps\")\n                ).alias(\"fraction_full_cov\"),\n            ]\n        )\n        .collect()\n        .sort(\"weekday_num\")\n    )\n\n    def weighted_fraction(df: pl.DataFrame) -&gt; float:\n        if df.is_empty():\n            return 0.0\n        total_ts = df[\"num_timestamps\"].sum()\n        if total_ts == 0:\n            return 0.0\n        full_cov = df[\"num_ts_full_coverage\"].sum()\n        return float(full_cov / total_ts)\n\n    weekend_data = df_weekday_cov.filter(pl.col(\"weekday_num\") &gt;= 5)\n    weekday_data = df_weekday_cov.filter(pl.col(\"weekday_num\") &lt; 5)\n\n    weekend_cov = weighted_fraction(weekend_data)\n    weekday_cov = weighted_fraction(weekday_data)\n\n    return {\n        \"df_weekday_cov\": df_weekday_cov,\n        \"weekend_fraction_full_cov\": weekend_cov,\n        \"weekday_fraction_full_cov\": weekday_cov,\n    }\n\n\ndef analyze_missing_submissions_both_modes(submission_glob: str):\n    \"\"\"\n    Main analysis function.\n    \"\"\"\n    df_all_data = load_and_preprocess(submission_glob)\n\n    fx_cols = [\n        \"AUD-USD Price\",\"AUD-USD Confidence\",\n        \"CAD-USD Price\",\"CAD-USD Confidence\",\n        \"EUR-USD Price\",\"EUR-USD Confidence\",\n        \"GBP-USD Price\",\"GBP-USD Confidence\",\n        \"JPY-USD Price\",\"JPY-USD Confidence\",\n        \"SEK-USD Price\",\"SEK-USD Confidence\",\n    ]\n    autonity_cols = [\n        \"ATN-USD Price\",\"ATN-USD Confidence\",\n        \"NTN-USD Price\",\"NTN-USD Confidence\",\n        \"NTN-ATN Price\",\"NTN-ATN Confidence\",\n    ]\n\n    (\n        df_ts_cov_all,\n        df_day_cov_all,\n        df_val_missing_all,\n        mode_str_all,\n    ) = compute_coverage_metrics(df_all_data, fx_cols, autonity_cols, True)\n\n    (\n        df_ts_cov_any,\n        df_day_cov_any,\n        df_val_missing_any,\n        mode_str_any,\n    ) = compute_coverage_metrics(df_all_data, fx_cols, autonity_cols, False)\n\n    weekend_info_all = check_weekend_patterns(df_ts_cov_all, df_all_data)\n    weekend_info_any = check_weekend_patterns(df_ts_cov_any, df_all_data)\n\n    return {\n        \"df_all_data\": df_all_data,\n        \"ALL\": {\n            \"df_timestamp_coverage\": df_ts_cov_all,\n            \"df_day_coverage\": df_day_cov_all,\n            \"df_validator_missing\": df_val_missing_all,\n            \"weekend_info\": weekend_info_all,\n        },\n        \"ANY\": {\n            \"df_timestamp_coverage\": df_ts_cov_any,\n            \"df_day_coverage\": df_day_cov_any,\n            \"df_validator_missing\": df_val_missing_any,\n            \"weekend_info\": weekend_info_any,\n        },\n    }\n\n\nresults = analyze_missing_submissions_both_modes(\n    submission_glob=\"../submission-data/Oracle_Submission_*.csv\"\n)\n\n\n\n\n\nBelow are directly reference outcomes from the results dictionary obtained by executing the analysis. The results shown will automatically update when re-running this notebook with new or updated datasets.\nDaily Coverage Analysis\nALL-FX Mode\n\n# Display daily coverage for ALL-FX mode\nresults[\"ALL\"][\"df_day_coverage\"]\n\nall_day_cov = results[\"ALL\"][\"df_day_coverage\"][\"fraction_full_coverage\"]\navg_all_cov = statistics.mean(all_day_cov) * 100\nprint(f\"Average daily full coverage (ALL-FX mode): {avg_all_cov:.1f}%\")\nif avg_all_cov &lt; 50:\n    print(\"Coverage is relatively low, suggesting many validators frequently miss submitting complete data.\")\nelse:\n    print(\"Coverage is reasonably good, indicating validators frequently submit complete data.\")\n\nAverage daily full coverage (ALL-FX mode): 97.5%\nCoverage is reasonably good, indicating validators frequently submit complete data.\n\n\nThe table and statistics above summarize how frequently all validators submitted complete data each day.\nANY-FX Mode\n\n# Display daily coverage for ANY-FX mode\nresults[\"ANY\"][\"df_day_coverage\"]\n\nany_day_cov = results[\"ANY\"][\"df_day_coverage\"][\"fraction_full_coverage\"]\navg_any_cov = statistics.mean(any_day_cov) * 100\ncoverage_difference = avg_any_cov - avg_all_cov\n\nprint(f\"Average daily full coverage (ANY-FX mode): {avg_any_cov:.1f}%\")\nprint(f\"Coverage difference between ANY-FX and ALL-FX modes: {coverage_difference:.1f}%\")\n\nif coverage_difference &gt; 20:\n    print(\"A substantial coverage improvement in ANY-FX mode indicates validators frequently provide partial submissions rather than complete ones.\")\nelse:\n    print(\"The small difference suggests that validators typically provide complete submissions or none at all.\")\n\nAverage daily full coverage (ANY-FX mode): 97.5%\nCoverage difference between ANY-FX and ALL-FX modes: 0.0%\nThe small difference suggests that validators typically provide complete submissions or none at all.\n\n\nThis comparison highlights the impact of submission requirements (complete vs. partial) on coverage.\nWeekend vs. Weekday Coverage\n\nweekend_cov = results[\"ALL\"][\"weekend_info\"][\"weekend_fraction_full_cov\"] * 100\nweekday_cov = results[\"ALL\"][\"weekend_info\"][\"weekday_fraction_full_cov\"] * 100\nprint(f\"Weekend coverage: {weekend_cov:.1f}%, Weekday coverage: {weekday_cov:.1f}%\")\n\nif weekend_cov &gt; weekday_cov + 5:\n    print(\"Significantly better coverage on weekends; potential scheduling issues on weekdays.\")\nelif weekday_cov &gt; weekend_cov + 5:\n    print(\"Significantly better coverage on weekdays; validators might be inactive or less reliable during weekends.\")\nelse:\n    print(\"No major difference in coverage between weekends and weekdays; submission patterns appear relatively uniform.\")\n\nWeekend coverage: 0.0%, Weekday coverage: 97.5%\nSignificantly better coverage on weekdays; validators might be inactive or less reliable during weekends.\n\n\nList of all Validators and their Missing Rates\n\ndf_all_missing = results[\"ALL\"][\"df_validator_missing\"]\n\nfor row in df_all_missing.iter_rows(named=True):\n    addr = row[\"Validator Address\"]\n    total = row[\"total_timestamps_encountered\"]\n    missing = row[\"missing_count\"]\n    fraction = row[\"fraction_missing\"] * 100\n    print(\n        f\"Validator {addr}: total={total}, missing={missing}, fraction_missing={fraction:.1f}%\"\n    )\n\nValidator 0x3fe573552E14a0FC11Da25E43Fef11e16a785068: total=2880, missing=2880, fraction_missing=100.0%\nValidator 0xdF239e0D5b4E6e820B0cFEF6972A90893c2073AB: total=2880, missing=2880, fraction_missing=100.0%\nValidator 0xd625d50B0d087861c286d726eC51Cf4Bd9c54357: total=2856, missing=2856, fraction_missing=100.0%\nValidator 0x26E2724dBD14Fbd52be430B97043AA4c83F05852: total=2880, missing=2880, fraction_missing=100.0%\nValidator 0x100E38f7BCEc53937BDd79ADE46F34362470577B: total=2876, missing=2876, fraction_missing=100.0%\nValidator 0x4cD134001EEF0843B9c69Ba9569d11fDcF4bd495: total=2823, missing=52, fraction_missing=1.8%\nValidator 0x19E356ebC20283fc74AF0BA4C179502A1F62fA7B: total=2833, missing=47, fraction_missing=1.7%\nValidator 0xbfDcAF35f52F9ef423ac8F2621F9eef8be6dEd17: total=2833, missing=47, fraction_missing=1.7%\nValidator 0xf34CD6c09a59d7D3d1a6C3dC231a7834E5615D6A: total=2834, missing=46, fraction_missing=1.6%\nValidator 0x2928FE5b911BCAf837cAd93eB9626E86a189f1dd: total=2829, missing=44, fraction_missing=1.6%\nValidator 0x01F788E4371a70D579C178Ea7F48E04e8B2CD743: total=2837, missing=43, fraction_missing=1.5%\nValidator 0x6747c02DE7eb2099265e55715Ba2E03e8563D051: total=2840, missing=40, fraction_missing=1.4%\nValidator 0x831B837C3DA1B6c2AB68a690206bDfF368877E19: total=2863, missing=17, fraction_missing=0.6%\nValidator 0xd61a48b0e11B0Dc6b7Bd713B1012563c52591BAA: total=2873, missing=7, fraction_missing=0.2%\nValidator 0x6a395dE946c0493157404E2b1947493c633f569E: total=2874, missing=6, fraction_missing=0.2%\nValidator 0x5E17e837DcBa2728C94f95c38fA8a47CB9C8818F: total=2876, missing=4, fraction_missing=0.1%\nValidator 0x984A46Ec685Bb41A7BBb2bc39f80C78410ff4057: total=2877, missing=3, fraction_missing=0.1%\nValidator 0x1476A65D7B5739dE1805d5130441A94022Ee49fe: total=2462, missing=1, fraction_missing=0.0%\nValidator 0x00a96aaED75015Bb44cED878D927dcb15ec1FF54: total=2866, missing=1, fraction_missing=0.0%\nValidator 0x7232e75a8bFd8c9ab002BB3A00eAa885BC72A6dd: total=2877, missing=1, fraction_missing=0.0%\nValidator 0xEf0Ba5e345C2C3937df5667A870Aae5105CAa3a5: total=2879, missing=1, fraction_missing=0.0%\nValidator 0xBE287C82A786218E008FF97320b08244BE4A282c: total=2879, missing=1, fraction_missing=0.0%\nValidator 0x8f91e0ADF8065C3fFF92297267E02DF32C2978FF: total=2879, missing=1, fraction_missing=0.0%\nValidator 0xDF2D0052ea56A860443039619f6DAe4434bc0Ac4: total=2879, missing=1, fraction_missing=0.0%\nValidator 0x23b4Be9536F93b8D550214912fD0e38417Ff7209: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x551f3300FCFE0e392178b3542c009948008B2a9F: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x59031767f20EA8F4a3d90d33aB0DAA2ca469Fd9a: total=2880, missing=0, fraction_missing=0.0%\nValidator 0xD9fDab408dF7Ae751691BeC2efE3b713ba3f9C36: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x1Be7f70BCf8393a7e4A5BcC66F6f15d6e35cfBBC: total=2880, missing=0, fraction_missing=0.0%\nValidator 0xBBf36374eb23968F25aecAEbb97BF3118f3c2fEC: total=2880, missing=0, fraction_missing=0.0%\nValidator 0xE9FFF86CAdC3136b3D94948B8Fd23631EDaa2dE3: total=2880, missing=0, fraction_missing=0.0%\nValidator 0xc5B9d978715F081E226cb28bADB7Ba4cde5f9775: total=2879, missing=0, fraction_missing=0.0%\nValidator 0x3AaF7817618728ffEF81898E11A3171C33faAE41: total=2874, missing=0, fraction_missing=0.0%\nValidator 0xcdEed21b471b0Dc54faF74480A0E700fCc42a7b6: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x94d28f08Ff81A80f4716C0a8EfC6CAC2Ec74d09E: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x99E2B4B27BDe92b42D04B6CF302cF564D2C13b74: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x24915749B793375a8C93090AF19928aFF1CAEcb6: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x383A3c437d3F12f60E5fC990119468D3561EfBfc: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x22A76e194A49c9e5508Cd4A3E1cD555D088ECB08: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x9d28e40E9Ec4789f9A0D17e421F76D8D0868EA44: total=2880, missing=0, fraction_missing=0.0%\nValidator 0xf10f56Bf0A28E0737c7e6bB0aF92f3DDad34aE6a: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x64F83c2538A646A550Ad9bEEb63427a377359DEE: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x9C7dAABb5101623340C925CFD6fF74088ff5672e: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x791A7F840ac11841cCB0FaA968B2e3a0Db930fCe: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x527192F3D2408C84087607b7feE1d0f907821E17: total=2880, missing=0, fraction_missing=0.0%\nValidator 0xcf716b3930d7cf6f2ADAD90A27c39fDc9D643BBd: total=2880, missing=0, fraction_missing=0.0%\nValidator 0xfD97FB8835d25740A2Da27c69762D74F6A931858: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x94470A842Ea4f44e668EB9C2AB81367b6Ce01772: total=2880, missing=0, fraction_missing=0.0%\nValidator 0xDCA5DFF3D42f2db3C18dBE823380A0A81db49A7E: total=2880, missing=0, fraction_missing=0.0%\nValidator 0xB5d8be2AB4b6d7E6be7Ea28E91b370223a06289f: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x197B2c44b887c4aC01243BDE7E4bBa8bd95BC3a8: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x718361fc3637199F24a2437331677D6B89a40519: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x358488a4EdCA493FCD87610dcd50c62c8A3Dd658: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x36142A4f36974e2935192A1111C39330aA296D3C: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x3597d2D42f8Fbbc82E8b1046048773aD6DDB717E: total=2880, missing=0, fraction_missing=0.0%\nValidator 0xF9B38D02959379d43C764064dE201324d5e12931: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x5603caFE3313D0cf56Fd4bE4A2f606dD6E43F8Eb: total=2880, missing=0, fraction_missing=0.0%\nValidator 0xE4686A4C6E63A8ab51B458c52EB779AEcf0B74f7: total=2880, missing=0, fraction_missing=0.0%\nValidator 0xC1F9acAF1824F6C906b35A0D2584D6E25077C7f5: total=2880, missing=0, fraction_missing=0.0%\nValidator 0x8584A78A9b94f332A34BBf24D2AF83367Da31894: total=2879, missing=0, fraction_missing=0.0%\n\n\nPlease note, total is the number of timestamp slots at which this validator was expected to submit. missing indicates how many of those slots were unfilled (i.e. the validator did not provide the required data for that timestamp). fraction_missing is the percentage of timestamps that were missing out of the total encountered.",
    "crumbs": [
      "Notebooks",
      "Issue 1"
    ]
  },
  {
    "objectID": "notebooks/issue_9.html",
    "href": "notebooks/issue_9.html",
    "title": "Issue 9",
    "section": "",
    "text": "This notebook documents the analysis for Issue #9: Vendor Downtime or API Rate-Limits in the Autonity Oracle data. It covers:\n\nWhat is this issue about?\n\nWhy conduct this issue analysis?\n\nHow to conduct this issue analysis?\n\nWhat are the results?\n\n\n\n\nValidators occasionally experience data interruptions due to:\n\nVendor downtime (temporary outages)\nAPI rate-limits (causing zero or placeholder values)\n\nAnalyze abrupt stoppages and zero-value submissions, especially simultaneous occurrences, indicating a shared vendor or API issue.\n\n\n\n\n\nTo identify vendor-related disruptions that could impact reliability.\nTo differentiate individual validator errors from broader vendor problems.\nTo inform strategies for improved vendor redundancy or rate-limit management.\n\n\n\n\n\nUse Python with the Polars library (v1.24.0) to:\n\nLoad and preprocess Oracle submission CSV files.\nDetecting validator submission stoppages exceeding 120-minute gaps.\nIdentifying submissions with zero or near-zero price placeholders.\nExamining concurrency—multiple validators experiencing issues simultaneously.\n\nHere’s the Python code used:\n\nimport polars as pl\nimport glob\nfrom typing import List\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndef load_and_preprocess_submissions(submission_glob: str) -&gt; pl.DataFrame:\n    \"\"\"\n    Loads Oracle Submission CSVs and returns a Polars DataFrame.\n    \"\"\"\n    files = sorted(glob.glob(submission_glob))\n    if not files:\n        raise ValueError(f\"No CSV files found matching pattern {submission_glob}\")\n\n    lf_list = []\n    for f in files:\n        lf_temp = pl.scan_csv(\n            f,\n            dtypes={\"Timestamp\": pl.Utf8},\n            null_values=[\"\"],\n            ignore_errors=True,\n        )\n        lf_list.append(lf_temp)\n\n    lf = pl.concat(lf_list)\n\n    lf = lf.with_columns(\n        pl.col(\"Timestamp\")\n        .str.strptime(pl.Datetime, strict=False)\n        .alias(\"Timestamp_dt\")\n    )\n\n    lf = lf.with_columns(\n        [\n            pl.col(\"Timestamp_dt\").cast(pl.Date).alias(\"date_only\"),\n            pl.col(\"Timestamp_dt\").dt.weekday().alias(\"weekday_num\"),\n        ]\n    )\n\n    df = lf.collect()\n    return df\n\n\ndef detect_abrupt_stoppages(\n    df: pl.DataFrame, max_gap_minutes: float = 120.0\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Looks at the interval between consecutive submissions and flags:\n      - If there's a gap &gt; `max_gap_minutes` (e.g. 120 minutes).\n      - If the validator has no submissions after a certain date/time.\n    \"\"\"\n    df_sorted = df.sort([\"Validator Address\", \"Timestamp_dt\"]).with_columns(\n        (pl.col(\"Timestamp_dt\").cast(pl.Int64) // 1_000_000_000).alias(\"epoch_seconds\")\n    )\n\n    df_with_diff = df_sorted.with_columns(\n        [\n            (pl.col(\"epoch_seconds\") - pl.col(\"epoch_seconds\").shift(1))\n            .over(\"Validator Address\")\n            .alias(\"diff_seconds\")\n        ]\n    )\n\n    df_with_diff = df_with_diff.with_columns(\n        (pl.col(\"diff_seconds\") / 60.0).alias(\"diff_minutes\")\n    )\n\n    df_with_diff = df_with_diff.with_columns(\n        pl.col(\"Timestamp_dt\")\n        .shift(-1)\n        .over(\"Validator Address\")\n        .alias(\"next_submission_ts\")\n    )\n\n    large_gaps = df_with_diff.filter(pl.col(\"diff_minutes\") &gt; max_gap_minutes)\n\n    final_stops = df_with_diff.filter(pl.col(\"next_submission_ts\").is_null())\n\n    large_gaps_df = large_gaps.select(\n        [\n            pl.col(\"Validator Address\"),\n            pl.col(\"Timestamp_dt\"),\n            pl.col(\"next_submission_ts\"),\n            pl.col(\"diff_minutes\").alias(\"gap_minutes\"),\n            pl.lit(False).alias(\"is_final_stop\"),\n        ]\n    )\n\n    final_stops_df = final_stops.select(\n        [\n            pl.col(\"Validator Address\"),\n            pl.col(\"Timestamp_dt\"),\n            pl.col(\"next_submission_ts\"),\n            pl.lit(None).cast(pl.Float64).alias(\"gap_minutes\"),\n            pl.lit(True).alias(\"is_final_stop\"),\n        ]\n    )\n\n    return pl.concat([large_gaps_df, final_stops_df]).sort(\n        [\"Validator Address\", \"Timestamp_dt\"]\n    )\n\n\ndef detect_zero_placeholder_values(\n    df: pl.DataFrame,\n    price_columns: List[str],\n    zero_threshold: float = 1e-5,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Detects submissions where all relevant price columns are effectively zero\n    (below `zero_threshold` once converted from Wei).\n    \"\"\"\n    bool_exprs = []\n    for pc in price_columns:\n        col_expr = ((pl.col(pc).cast(pl.Float64) / 1e18) &lt; zero_threshold).alias(\n            f\"is_{pc}_zero\"\n        )\n        bool_exprs.append(col_expr)\n\n    df_local = df.with_columns(bool_exprs)\n\n    _ = [\n        c.alias(f\"{c}_int\")\n        for c in (pl.col(name) for name in df_local.columns if name.startswith(\"is_\"))\n    ]\n\n    newly_created_bools = [f\"is_{pc}_zero\" for pc in price_columns]\n\n    count_zero_expr = pl.fold(\n        acc=pl.lit(0),\n        function=lambda acc, x: acc + x,\n        exprs=[pl.col(b).cast(pl.Int64) for b in newly_created_bools],\n    ).alias(\"count_zeroed_prices\")\n\n    df_zero_check = df_local.with_columns(\n        [\n            count_zero_expr,\n            pl.lit(len(price_columns)).alias(\"total_price_cols\"),\n        ]\n    ).with_columns(\n        (\n            pl.col(\"count_zeroed_prices\").cast(pl.Float64)\n            / pl.col(\"total_price_cols\").cast(pl.Float64)\n        ).alias(\"fraction_zeroed\")\n    )\n\n    df_zero_filtered = (\n        df_zero_check.filter(pl.col(\"fraction_zeroed\") == 1.0)\n        .select(\n            [\n                \"Timestamp_dt\",\n                \"Validator Address\",\n                \"count_zeroed_prices\",\n                \"total_price_cols\",\n                \"fraction_zeroed\",\n            ]\n        )\n        .sort([\"Validator Address\", \"Timestamp_dt\"])\n    )\n    return df_zero_filtered\n\n\ndef detect_concurrent_issues(\n    df_events: pl.DataFrame, time_col: str = \"Timestamp_dt\", group_window: str = \"1h\"\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Detects how many validators exhibit the same event within a certain time window.\n    \"\"\"\n    if df_events.is_empty():\n        return pl.DataFrame(\n            {\n                \"time_bucket\": [],\n                \"num_validators\": [],\n                \"validator_addresses\": [],\n            }\n        )\n\n    df_local = df_events.with_columns(\n        pl.col(time_col).dt.truncate(group_window).alias(\"time_bucket\")\n    )\n\n    grouped = (\n        df_local.lazy()\n        .group_by(\"time_bucket\")\n        .agg(\n            [\n                pl.n_unique(\"Validator Address\").alias(\"num_validators\"),\n                pl.col(\"Validator Address\").unique().alias(\"validator_addresses\"),\n            ]\n        )\n    )\n    return grouped.collect().sort(\"time_bucket\")\n\n\ndef analyze_vendor_downtime_api_ratelimits(\n    submission_glob: str,\n    price_cols: List[str],\n    max_gap_minutes: float = 120.0,\n    zero_threshold: float = 1e-5,\n    concurrency_window: str = \"1h\",\n):\n    \"\"\"\n    Main analysis function.\n    \"\"\"\n    df_all = load_and_preprocess_submissions(submission_glob)\n\n    df_stoppages = detect_abrupt_stoppages(df_all, max_gap_minutes=max_gap_minutes)\n\n    df_zeros = detect_zero_placeholder_values(\n        df_all, price_cols, zero_threshold=zero_threshold\n    )\n\n    df_stoppage_concurrency = detect_concurrent_issues(\n        df_stoppages, time_col=\"Timestamp_dt\", group_window=concurrency_window\n    )\n\n    df_zero_concurrency = detect_concurrent_issues(\n        df_zeros, time_col=\"Timestamp_dt\", group_window=concurrency_window\n    )\n\n    return {\n        \"df_all\": df_all,\n        \"df_stoppages\": df_stoppages,\n        \"df_zero_placeholders\": df_zeros,\n        \"df_stoppage_concurrency\": df_stoppage_concurrency,\n        \"df_zero_concurrency\": df_zero_concurrency,\n    }\n\n\nfx_price_cols = [\n    \"AUD-USD Price\",\n    \"CAD-USD Price\",\n    \"EUR-USD Price\",\n    \"GBP-USD Price\",\n    \"JPY-USD Price\",\n    \"SEK-USD Price\",\n]\nautonity_price_cols = [\n    \"ATN-USD Price\",\n    \"NTN-USD Price\",\n    \"NTN-ATN Price\",\n]\nall_price_cols = fx_price_cols + autonity_price_cols\n\nresults = analyze_vendor_downtime_api_ratelimits(\n    submission_glob=\"../submission-data/Oracle_Submission_*.csv\",\n    price_cols=all_price_cols,\n    max_gap_minutes=120.0,  # e.g. 2 hours\n    zero_threshold=1e-5,  # treat sub-1e-5 as \"effectively zero\"\n    concurrency_window=\"1h\",  # aggregate concurrency by the hour\n)\n\n\n\n\n\nThe following sections present analysis findings dynamically from results_issue9.\n\n\n\ndf_stoppages = results[\"df_stoppages\"]\nprint(f\"Total stoppage events detected: {df_stoppages.height}\")\n\nif not df_stoppages.is_empty():\n    print(\"Sample abrupt stoppage records:\")\n    display(df_stoppages)\nelse:\n    print(\"No stoppages found above the given threshold.\")\n\nTotal stoppage events detected: 60\nSample abrupt stoppage records:\n\n\n\nshape: (60, 5)\n\n\n\nValidator Address\nTimestamp_dt\nnext_submission_ts\ngap_minutes\nis_final_stop\n\n\nstr\ndatetime[μs, UTC]\ndatetime[μs, UTC]\nf64\nbool\n\n\n\n\n\"0x00a96aaED75015Bb44cED878D927…\n2025-01-01 23:59:44 UTC\nnull\nnull\ntrue\n\n\n\"0x01F788E4371a70D579C178Ea7F48…\n2025-01-01 23:59:44 UTC\nnull\nnull\ntrue\n\n\n\"0x100E38f7BCEc53937BDd79ADE46F…\n2025-01-01 23:59:44 UTC\nnull\nnull\ntrue\n\n\n\"0x1476A65D7B5739dE1805d5130441…\n2025-01-01 23:59:44 UTC\nnull\nnull\ntrue\n\n\n\"0x197B2c44b887c4aC01243BDE7E4b…\n2025-01-01 23:59:44 UTC\nnull\nnull\ntrue\n\n\n…\n…\n…\n…\n…\n\n\n\"0xd625d50B0d087861c286d726eC51…\n2025-01-01 23:59:44 UTC\nnull\nnull\ntrue\n\n\n\"0xdF239e0D5b4E6e820B0cFEF6972A…\n2025-01-01 23:59:44 UTC\nnull\nnull\ntrue\n\n\n\"0xf10f56Bf0A28E0737c7e6bB0aF92…\n2025-01-01 23:59:44 UTC\nnull\nnull\ntrue\n\n\n\"0xf34CD6c09a59d7D3d1a6C3dC231a…\n2025-01-01 23:59:44 UTC\nnull\nnull\ntrue\n\n\n\"0xfD97FB8835d25740A2Da27c69762…\n2025-01-01 23:59:44 UTC\nnull\nnull\ntrue\n\n\n\n\n\n\nNote: You may observe lots of null for next_submission_ts and gap_minutes, with is_final_stop = True for every record in df_stoppages, which usually means no large gaps were found, so the only “stoppage” events are the final submissions per validator.\nInterpretation:\n\nFrequent stoppages indicate potential downtime.\nSparse stoppages suggest isolated issues rather than systemic.\n\n\n\n\n\ndf_zero = results[\"df_zero_placeholders\"]\nprint(f\"Total zero-placeholder events detected: {df_zero.height}\")\n\nif not df_zero.is_empty():\n    print(\"Sample zero-placeholder records:\")\n    display(df_zero)\nelse:\n    print(\"No zero-placeholder rows detected.\")\n\nTotal zero-placeholder events detected: 0\nNo zero-placeholder rows detected.\n\n\nInterpretation:\n\nZero-value submissions strongly suggest rate-limit hits or vendor API fallbacks.\nMany zero events may necessitate vendor review.\n\n\n\n\n\ndf_stop_conc = results[\"df_stoppage_concurrency\"]\ndf_zero_conc = results[\"df_zero_concurrency\"]\n\nprint(\"Stoppage Concurrency Events:\")\nif df_stop_conc.is_empty():\n    print(\"No concurrency found among stoppages.\")\nelse:\n    display(df_stop_conc)\n\nprint(\"\\nZero-Placeholder Concurrency Events:\")\nif df_zero_conc.is_empty():\n    print(\"No concurrency found among zero placeholders.\")\nelse:\n    display(df_zero_conc)\n\nStoppage Concurrency Events:\n\n\n\nshape: (1, 3)\n\n\n\ntime_bucket\nnum_validators\nvalidator_addresses\n\n\ndatetime[μs, UTC]\nu32\nlist[str]\n\n\n\n\n2025-01-01 23:00:00 UTC\n60\n[\"0xfD97FB8835d25740A2Da27c69762D74F6A931858\", \"0xDF2D0052ea56A860443039619f6DAe4434bc0Ac4\", … \"0x718361fc3637199F24a2437331677D6B89a40519\"]\n\n\n\n\n\n\n\nZero-Placeholder Concurrency Events:\nNo concurrency found among zero placeholders.\n\n\n\nHigh concurrency strongly implies a vendor or API outage affecting multiple validators simultaneously.\nLow or no concurrency indicates validator-specific configuration or connectivity issues.\n\nList of all Validators and their Zero Placeholder Ratios\n\ndf_all = results[\"df_all\"]  # all submissions\ndf_zero = results[\"df_zero_placeholders\"]  # all-zero submissions\n\ndf_sub_count = (\n    df_all.lazy()\n    .group_by(\"Validator Address\")\n    .agg(\n        pl.count().alias(\"num_submissions\")\n    )\n)\n\ndf_zero_count = (\n    df_zero.lazy()\n    .group_by(\"Validator Address\")\n    .agg(\n        pl.count().alias(\"num_zero_submissions\")\n    )\n)\n\ndf_ratio = (\n    df_sub_count.join(df_zero_count, on=\"Validator Address\", how=\"left\")\n    .with_columns(\n        (pl.col(\"num_zero_submissions\") / pl.col(\"num_submissions\"))\n        .fill_null(0)\n        .alias(\"zero_placeholder_ratio\")\n    )\n    .select([\"Validator Address\", \"num_submissions\", \"num_zero_submissions\", \"zero_placeholder_ratio\"])\n    .sort(\"num_submissions\", descending=True)\n    .collect()\n)\n\nfor row in df_ratio.to_dicts():\n    print(\n        f\"Validator {row['Validator Address']}: \"\n        f\"total_submissions={row['num_submissions']}, \"\n        f\"zero_submissions={row['num_zero_submissions']}, \"\n        f\"zero_placeholder_ratio={row['zero_placeholder_ratio']:.2f}\"\n    )\n\nValidator 0x99E2B4B27BDe92b42D04B6CF302cF564D2C13b74: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x3597d2D42f8Fbbc82E8b1046048773aD6DDB717E: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x36142A4f36974e2935192A1111C39330aA296D3C: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xB5d8be2AB4b6d7E6be7Ea28E91b370223a06289f: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xfD97FB8835d25740A2Da27c69762D74F6A931858: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x718361fc3637199F24a2437331677D6B89a40519: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x3fe573552E14a0FC11Da25E43Fef11e16a785068: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xF9B38D02959379d43C764064dE201324d5e12931: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x527192F3D2408C84087607b7feE1d0f907821E17: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x791A7F840ac11841cCB0FaA968B2e3a0Db930fCe: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x551f3300FCFE0e392178b3542c009948008B2a9F: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x59031767f20EA8F4a3d90d33aB0DAA2ca469Fd9a: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x383A3c437d3F12f60E5fC990119468D3561EfBfc: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xC1F9acAF1824F6C906b35A0D2584D6E25077C7f5: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x5603caFE3313D0cf56Fd4bE4A2f606dD6E43F8Eb: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x1Be7f70BCf8393a7e4A5BcC66F6f15d6e35cfBBC: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xBBf36374eb23968F25aecAEbb97BF3118f3c2fEC: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xf10f56Bf0A28E0737c7e6bB0aF92f3DDad34aE6a: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x94d28f08Ff81A80f4716C0a8EfC6CAC2Ec74d09E: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x24915749B793375a8C93090AF19928aFF1CAEcb6: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xdF239e0D5b4E6e820B0cFEF6972A90893c2073AB: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x9d28e40E9Ec4789f9A0D17e421F76D8D0868EA44: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x22A76e194A49c9e5508Cd4A3E1cD555D088ECB08: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x358488a4EdCA493FCD87610dcd50c62c8A3Dd658: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x26E2724dBD14Fbd52be430B97043AA4c83F05852: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x197B2c44b887c4aC01243BDE7E4bBa8bd95BC3a8: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x9C7dAABb5101623340C925CFD6fF74088ff5672e: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xE4686A4C6E63A8ab51B458c52EB779AEcf0B74f7: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xDCA5DFF3D42f2db3C18dBE823380A0A81db49A7E: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x64F83c2538A646A550Ad9bEEb63427a377359DEE: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xcf716b3930d7cf6f2ADAD90A27c39fDc9D643BBd: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x23b4Be9536F93b8D550214912fD0e38417Ff7209: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xE9FFF86CAdC3136b3D94948B8Fd23631EDaa2dE3: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xD9fDab408dF7Ae751691BeC2efE3b713ba3f9C36: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x94470A842Ea4f44e668EB9C2AB81367b6Ce01772: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xcdEed21b471b0Dc54faF74480A0E700fCc42a7b6: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x8584A78A9b94f332A34BBf24D2AF83367Da31894: total_submissions=2879, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xEf0Ba5e345C2C3937df5667A870Aae5105CAa3a5: total_submissions=2879, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xc5B9d978715F081E226cb28bADB7Ba4cde5f9775: total_submissions=2879, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x8f91e0ADF8065C3fFF92297267E02DF32C2978FF: total_submissions=2879, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xDF2D0052ea56A860443039619f6DAe4434bc0Ac4: total_submissions=2879, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xBE287C82A786218E008FF97320b08244BE4A282c: total_submissions=2879, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x984A46Ec685Bb41A7BBb2bc39f80C78410ff4057: total_submissions=2877, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x7232e75a8bFd8c9ab002BB3A00eAa885BC72A6dd: total_submissions=2877, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x5E17e837DcBa2728C94f95c38fA8a47CB9C8818F: total_submissions=2876, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x100E38f7BCEc53937BDd79ADE46F34362470577B: total_submissions=2876, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x3AaF7817618728ffEF81898E11A3171C33faAE41: total_submissions=2874, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x6a395dE946c0493157404E2b1947493c633f569E: total_submissions=2874, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xd61a48b0e11B0Dc6b7Bd713B1012563c52591BAA: total_submissions=2873, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x00a96aaED75015Bb44cED878D927dcb15ec1FF54: total_submissions=2866, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x831B837C3DA1B6c2AB68a690206bDfF368877E19: total_submissions=2863, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xd625d50B0d087861c286d726eC51Cf4Bd9c54357: total_submissions=2856, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x6747c02DE7eb2099265e55715Ba2E03e8563D051: total_submissions=2840, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x01F788E4371a70D579C178Ea7F48E04e8B2CD743: total_submissions=2837, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xf34CD6c09a59d7D3d1a6C3dC231a7834E5615D6A: total_submissions=2834, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x19E356ebC20283fc74AF0BA4C179502A1F62fA7B: total_submissions=2833, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xbfDcAF35f52F9ef423ac8F2621F9eef8be6dEd17: total_submissions=2833, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x2928FE5b911BCAf837cAd93eB9626E86a189f1dd: total_submissions=2829, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x4cD134001EEF0843B9c69Ba9569d11fDcF4bd495: total_submissions=2823, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x1476A65D7B5739dE1805d5130441A94022Ee49fe: total_submissions=2462, zero_submissions=None, zero_placeholder_ratio=0.00\n\n\nPlease note, total_submissions is the count of all submission rows for a validator. zero_submissions counts those rows where every tracked price column is effectively 0 (≤ 1 × 10‑5 after the Wei‑to‑unit conversion); if the validator never produced a full all‑zero row this value shows up as null. zero_placeholder_ratio is zero_submissions / total_submissions, producing a value from 0 to 1 that indicates the fraction of a validator’s submissions that were complete zero‑placeholders.",
    "crumbs": [
      "Notebooks",
      "Issue 9"
    ]
  },
  {
    "objectID": "notebooks/issue_9.html#vendor-downtime-or-api-rate-limits",
    "href": "notebooks/issue_9.html#vendor-downtime-or-api-rate-limits",
    "title": "Issue 9",
    "section": "",
    "text": "This notebook documents the analysis for Issue #9: Vendor Downtime or API Rate-Limits in the Autonity Oracle data. It covers:\n\nWhat is this issue about?\n\nWhy conduct this issue analysis?\n\nHow to conduct this issue analysis?\n\nWhat are the results?\n\n\n\n\nValidators occasionally experience data interruptions due to:\n\nVendor downtime (temporary outages)\nAPI rate-limits (causing zero or placeholder values)\n\nAnalyze abrupt stoppages and zero-value submissions, especially simultaneous occurrences, indicating a shared vendor or API issue.\n\n\n\n\n\nTo identify vendor-related disruptions that could impact reliability.\nTo differentiate individual validator errors from broader vendor problems.\nTo inform strategies for improved vendor redundancy or rate-limit management.\n\n\n\n\n\nUse Python with the Polars library (v1.24.0) to:\n\nLoad and preprocess Oracle submission CSV files.\nDetecting validator submission stoppages exceeding 120-minute gaps.\nIdentifying submissions with zero or near-zero price placeholders.\nExamining concurrency—multiple validators experiencing issues simultaneously.\n\nHere’s the Python code used:\n\nimport polars as pl\nimport glob\nfrom typing import List\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndef load_and_preprocess_submissions(submission_glob: str) -&gt; pl.DataFrame:\n    \"\"\"\n    Loads Oracle Submission CSVs and returns a Polars DataFrame.\n    \"\"\"\n    files = sorted(glob.glob(submission_glob))\n    if not files:\n        raise ValueError(f\"No CSV files found matching pattern {submission_glob}\")\n\n    lf_list = []\n    for f in files:\n        lf_temp = pl.scan_csv(\n            f,\n            dtypes={\"Timestamp\": pl.Utf8},\n            null_values=[\"\"],\n            ignore_errors=True,\n        )\n        lf_list.append(lf_temp)\n\n    lf = pl.concat(lf_list)\n\n    lf = lf.with_columns(\n        pl.col(\"Timestamp\")\n        .str.strptime(pl.Datetime, strict=False)\n        .alias(\"Timestamp_dt\")\n    )\n\n    lf = lf.with_columns(\n        [\n            pl.col(\"Timestamp_dt\").cast(pl.Date).alias(\"date_only\"),\n            pl.col(\"Timestamp_dt\").dt.weekday().alias(\"weekday_num\"),\n        ]\n    )\n\n    df = lf.collect()\n    return df\n\n\ndef detect_abrupt_stoppages(\n    df: pl.DataFrame, max_gap_minutes: float = 120.0\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Looks at the interval between consecutive submissions and flags:\n      - If there's a gap &gt; `max_gap_minutes` (e.g. 120 minutes).\n      - If the validator has no submissions after a certain date/time.\n    \"\"\"\n    df_sorted = df.sort([\"Validator Address\", \"Timestamp_dt\"]).with_columns(\n        (pl.col(\"Timestamp_dt\").cast(pl.Int64) // 1_000_000_000).alias(\"epoch_seconds\")\n    )\n\n    df_with_diff = df_sorted.with_columns(\n        [\n            (pl.col(\"epoch_seconds\") - pl.col(\"epoch_seconds\").shift(1))\n            .over(\"Validator Address\")\n            .alias(\"diff_seconds\")\n        ]\n    )\n\n    df_with_diff = df_with_diff.with_columns(\n        (pl.col(\"diff_seconds\") / 60.0).alias(\"diff_minutes\")\n    )\n\n    df_with_diff = df_with_diff.with_columns(\n        pl.col(\"Timestamp_dt\")\n        .shift(-1)\n        .over(\"Validator Address\")\n        .alias(\"next_submission_ts\")\n    )\n\n    large_gaps = df_with_diff.filter(pl.col(\"diff_minutes\") &gt; max_gap_minutes)\n\n    final_stops = df_with_diff.filter(pl.col(\"next_submission_ts\").is_null())\n\n    large_gaps_df = large_gaps.select(\n        [\n            pl.col(\"Validator Address\"),\n            pl.col(\"Timestamp_dt\"),\n            pl.col(\"next_submission_ts\"),\n            pl.col(\"diff_minutes\").alias(\"gap_minutes\"),\n            pl.lit(False).alias(\"is_final_stop\"),\n        ]\n    )\n\n    final_stops_df = final_stops.select(\n        [\n            pl.col(\"Validator Address\"),\n            pl.col(\"Timestamp_dt\"),\n            pl.col(\"next_submission_ts\"),\n            pl.lit(None).cast(pl.Float64).alias(\"gap_minutes\"),\n            pl.lit(True).alias(\"is_final_stop\"),\n        ]\n    )\n\n    return pl.concat([large_gaps_df, final_stops_df]).sort(\n        [\"Validator Address\", \"Timestamp_dt\"]\n    )\n\n\ndef detect_zero_placeholder_values(\n    df: pl.DataFrame,\n    price_columns: List[str],\n    zero_threshold: float = 1e-5,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Detects submissions where all relevant price columns are effectively zero\n    (below `zero_threshold` once converted from Wei).\n    \"\"\"\n    bool_exprs = []\n    for pc in price_columns:\n        col_expr = ((pl.col(pc).cast(pl.Float64) / 1e18) &lt; zero_threshold).alias(\n            f\"is_{pc}_zero\"\n        )\n        bool_exprs.append(col_expr)\n\n    df_local = df.with_columns(bool_exprs)\n\n    _ = [\n        c.alias(f\"{c}_int\")\n        for c in (pl.col(name) for name in df_local.columns if name.startswith(\"is_\"))\n    ]\n\n    newly_created_bools = [f\"is_{pc}_zero\" for pc in price_columns]\n\n    count_zero_expr = pl.fold(\n        acc=pl.lit(0),\n        function=lambda acc, x: acc + x,\n        exprs=[pl.col(b).cast(pl.Int64) for b in newly_created_bools],\n    ).alias(\"count_zeroed_prices\")\n\n    df_zero_check = df_local.with_columns(\n        [\n            count_zero_expr,\n            pl.lit(len(price_columns)).alias(\"total_price_cols\"),\n        ]\n    ).with_columns(\n        (\n            pl.col(\"count_zeroed_prices\").cast(pl.Float64)\n            / pl.col(\"total_price_cols\").cast(pl.Float64)\n        ).alias(\"fraction_zeroed\")\n    )\n\n    df_zero_filtered = (\n        df_zero_check.filter(pl.col(\"fraction_zeroed\") == 1.0)\n        .select(\n            [\n                \"Timestamp_dt\",\n                \"Validator Address\",\n                \"count_zeroed_prices\",\n                \"total_price_cols\",\n                \"fraction_zeroed\",\n            ]\n        )\n        .sort([\"Validator Address\", \"Timestamp_dt\"])\n    )\n    return df_zero_filtered\n\n\ndef detect_concurrent_issues(\n    df_events: pl.DataFrame, time_col: str = \"Timestamp_dt\", group_window: str = \"1h\"\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Detects how many validators exhibit the same event within a certain time window.\n    \"\"\"\n    if df_events.is_empty():\n        return pl.DataFrame(\n            {\n                \"time_bucket\": [],\n                \"num_validators\": [],\n                \"validator_addresses\": [],\n            }\n        )\n\n    df_local = df_events.with_columns(\n        pl.col(time_col).dt.truncate(group_window).alias(\"time_bucket\")\n    )\n\n    grouped = (\n        df_local.lazy()\n        .group_by(\"time_bucket\")\n        .agg(\n            [\n                pl.n_unique(\"Validator Address\").alias(\"num_validators\"),\n                pl.col(\"Validator Address\").unique().alias(\"validator_addresses\"),\n            ]\n        )\n    )\n    return grouped.collect().sort(\"time_bucket\")\n\n\ndef analyze_vendor_downtime_api_ratelimits(\n    submission_glob: str,\n    price_cols: List[str],\n    max_gap_minutes: float = 120.0,\n    zero_threshold: float = 1e-5,\n    concurrency_window: str = \"1h\",\n):\n    \"\"\"\n    Main analysis function.\n    \"\"\"\n    df_all = load_and_preprocess_submissions(submission_glob)\n\n    df_stoppages = detect_abrupt_stoppages(df_all, max_gap_minutes=max_gap_minutes)\n\n    df_zeros = detect_zero_placeholder_values(\n        df_all, price_cols, zero_threshold=zero_threshold\n    )\n\n    df_stoppage_concurrency = detect_concurrent_issues(\n        df_stoppages, time_col=\"Timestamp_dt\", group_window=concurrency_window\n    )\n\n    df_zero_concurrency = detect_concurrent_issues(\n        df_zeros, time_col=\"Timestamp_dt\", group_window=concurrency_window\n    )\n\n    return {\n        \"df_all\": df_all,\n        \"df_stoppages\": df_stoppages,\n        \"df_zero_placeholders\": df_zeros,\n        \"df_stoppage_concurrency\": df_stoppage_concurrency,\n        \"df_zero_concurrency\": df_zero_concurrency,\n    }\n\n\nfx_price_cols = [\n    \"AUD-USD Price\",\n    \"CAD-USD Price\",\n    \"EUR-USD Price\",\n    \"GBP-USD Price\",\n    \"JPY-USD Price\",\n    \"SEK-USD Price\",\n]\nautonity_price_cols = [\n    \"ATN-USD Price\",\n    \"NTN-USD Price\",\n    \"NTN-ATN Price\",\n]\nall_price_cols = fx_price_cols + autonity_price_cols\n\nresults = analyze_vendor_downtime_api_ratelimits(\n    submission_glob=\"../submission-data/Oracle_Submission_*.csv\",\n    price_cols=all_price_cols,\n    max_gap_minutes=120.0,  # e.g. 2 hours\n    zero_threshold=1e-5,  # treat sub-1e-5 as \"effectively zero\"\n    concurrency_window=\"1h\",  # aggregate concurrency by the hour\n)\n\n\n\n\n\nThe following sections present analysis findings dynamically from results_issue9.\n\n\n\ndf_stoppages = results[\"df_stoppages\"]\nprint(f\"Total stoppage events detected: {df_stoppages.height}\")\n\nif not df_stoppages.is_empty():\n    print(\"Sample abrupt stoppage records:\")\n    display(df_stoppages)\nelse:\n    print(\"No stoppages found above the given threshold.\")\n\nTotal stoppage events detected: 60\nSample abrupt stoppage records:\n\n\n\nshape: (60, 5)\n\n\n\nValidator Address\nTimestamp_dt\nnext_submission_ts\ngap_minutes\nis_final_stop\n\n\nstr\ndatetime[μs, UTC]\ndatetime[μs, UTC]\nf64\nbool\n\n\n\n\n\"0x00a96aaED75015Bb44cED878D927…\n2025-01-01 23:59:44 UTC\nnull\nnull\ntrue\n\n\n\"0x01F788E4371a70D579C178Ea7F48…\n2025-01-01 23:59:44 UTC\nnull\nnull\ntrue\n\n\n\"0x100E38f7BCEc53937BDd79ADE46F…\n2025-01-01 23:59:44 UTC\nnull\nnull\ntrue\n\n\n\"0x1476A65D7B5739dE1805d5130441…\n2025-01-01 23:59:44 UTC\nnull\nnull\ntrue\n\n\n\"0x197B2c44b887c4aC01243BDE7E4b…\n2025-01-01 23:59:44 UTC\nnull\nnull\ntrue\n\n\n…\n…\n…\n…\n…\n\n\n\"0xd625d50B0d087861c286d726eC51…\n2025-01-01 23:59:44 UTC\nnull\nnull\ntrue\n\n\n\"0xdF239e0D5b4E6e820B0cFEF6972A…\n2025-01-01 23:59:44 UTC\nnull\nnull\ntrue\n\n\n\"0xf10f56Bf0A28E0737c7e6bB0aF92…\n2025-01-01 23:59:44 UTC\nnull\nnull\ntrue\n\n\n\"0xf34CD6c09a59d7D3d1a6C3dC231a…\n2025-01-01 23:59:44 UTC\nnull\nnull\ntrue\n\n\n\"0xfD97FB8835d25740A2Da27c69762…\n2025-01-01 23:59:44 UTC\nnull\nnull\ntrue\n\n\n\n\n\n\nNote: You may observe lots of null for next_submission_ts and gap_minutes, with is_final_stop = True for every record in df_stoppages, which usually means no large gaps were found, so the only “stoppage” events are the final submissions per validator.\nInterpretation:\n\nFrequent stoppages indicate potential downtime.\nSparse stoppages suggest isolated issues rather than systemic.\n\n\n\n\n\ndf_zero = results[\"df_zero_placeholders\"]\nprint(f\"Total zero-placeholder events detected: {df_zero.height}\")\n\nif not df_zero.is_empty():\n    print(\"Sample zero-placeholder records:\")\n    display(df_zero)\nelse:\n    print(\"No zero-placeholder rows detected.\")\n\nTotal zero-placeholder events detected: 0\nNo zero-placeholder rows detected.\n\n\nInterpretation:\n\nZero-value submissions strongly suggest rate-limit hits or vendor API fallbacks.\nMany zero events may necessitate vendor review.\n\n\n\n\n\ndf_stop_conc = results[\"df_stoppage_concurrency\"]\ndf_zero_conc = results[\"df_zero_concurrency\"]\n\nprint(\"Stoppage Concurrency Events:\")\nif df_stop_conc.is_empty():\n    print(\"No concurrency found among stoppages.\")\nelse:\n    display(df_stop_conc)\n\nprint(\"\\nZero-Placeholder Concurrency Events:\")\nif df_zero_conc.is_empty():\n    print(\"No concurrency found among zero placeholders.\")\nelse:\n    display(df_zero_conc)\n\nStoppage Concurrency Events:\n\n\n\nshape: (1, 3)\n\n\n\ntime_bucket\nnum_validators\nvalidator_addresses\n\n\ndatetime[μs, UTC]\nu32\nlist[str]\n\n\n\n\n2025-01-01 23:00:00 UTC\n60\n[\"0xfD97FB8835d25740A2Da27c69762D74F6A931858\", \"0xDF2D0052ea56A860443039619f6DAe4434bc0Ac4\", … \"0x718361fc3637199F24a2437331677D6B89a40519\"]\n\n\n\n\n\n\n\nZero-Placeholder Concurrency Events:\nNo concurrency found among zero placeholders.\n\n\n\nHigh concurrency strongly implies a vendor or API outage affecting multiple validators simultaneously.\nLow or no concurrency indicates validator-specific configuration or connectivity issues.\n\nList of all Validators and their Zero Placeholder Ratios\n\ndf_all = results[\"df_all\"]  # all submissions\ndf_zero = results[\"df_zero_placeholders\"]  # all-zero submissions\n\ndf_sub_count = (\n    df_all.lazy()\n    .group_by(\"Validator Address\")\n    .agg(\n        pl.count().alias(\"num_submissions\")\n    )\n)\n\ndf_zero_count = (\n    df_zero.lazy()\n    .group_by(\"Validator Address\")\n    .agg(\n        pl.count().alias(\"num_zero_submissions\")\n    )\n)\n\ndf_ratio = (\n    df_sub_count.join(df_zero_count, on=\"Validator Address\", how=\"left\")\n    .with_columns(\n        (pl.col(\"num_zero_submissions\") / pl.col(\"num_submissions\"))\n        .fill_null(0)\n        .alias(\"zero_placeholder_ratio\")\n    )\n    .select([\"Validator Address\", \"num_submissions\", \"num_zero_submissions\", \"zero_placeholder_ratio\"])\n    .sort(\"num_submissions\", descending=True)\n    .collect()\n)\n\nfor row in df_ratio.to_dicts():\n    print(\n        f\"Validator {row['Validator Address']}: \"\n        f\"total_submissions={row['num_submissions']}, \"\n        f\"zero_submissions={row['num_zero_submissions']}, \"\n        f\"zero_placeholder_ratio={row['zero_placeholder_ratio']:.2f}\"\n    )\n\nValidator 0x99E2B4B27BDe92b42D04B6CF302cF564D2C13b74: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x3597d2D42f8Fbbc82E8b1046048773aD6DDB717E: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x36142A4f36974e2935192A1111C39330aA296D3C: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xB5d8be2AB4b6d7E6be7Ea28E91b370223a06289f: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xfD97FB8835d25740A2Da27c69762D74F6A931858: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x718361fc3637199F24a2437331677D6B89a40519: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x3fe573552E14a0FC11Da25E43Fef11e16a785068: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xF9B38D02959379d43C764064dE201324d5e12931: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x527192F3D2408C84087607b7feE1d0f907821E17: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x791A7F840ac11841cCB0FaA968B2e3a0Db930fCe: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x551f3300FCFE0e392178b3542c009948008B2a9F: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x59031767f20EA8F4a3d90d33aB0DAA2ca469Fd9a: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x383A3c437d3F12f60E5fC990119468D3561EfBfc: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xC1F9acAF1824F6C906b35A0D2584D6E25077C7f5: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x5603caFE3313D0cf56Fd4bE4A2f606dD6E43F8Eb: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x1Be7f70BCf8393a7e4A5BcC66F6f15d6e35cfBBC: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xBBf36374eb23968F25aecAEbb97BF3118f3c2fEC: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xf10f56Bf0A28E0737c7e6bB0aF92f3DDad34aE6a: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x94d28f08Ff81A80f4716C0a8EfC6CAC2Ec74d09E: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x24915749B793375a8C93090AF19928aFF1CAEcb6: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xdF239e0D5b4E6e820B0cFEF6972A90893c2073AB: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x9d28e40E9Ec4789f9A0D17e421F76D8D0868EA44: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x22A76e194A49c9e5508Cd4A3E1cD555D088ECB08: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x358488a4EdCA493FCD87610dcd50c62c8A3Dd658: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x26E2724dBD14Fbd52be430B97043AA4c83F05852: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x197B2c44b887c4aC01243BDE7E4bBa8bd95BC3a8: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x9C7dAABb5101623340C925CFD6fF74088ff5672e: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xE4686A4C6E63A8ab51B458c52EB779AEcf0B74f7: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xDCA5DFF3D42f2db3C18dBE823380A0A81db49A7E: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x64F83c2538A646A550Ad9bEEb63427a377359DEE: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xcf716b3930d7cf6f2ADAD90A27c39fDc9D643BBd: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x23b4Be9536F93b8D550214912fD0e38417Ff7209: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xE9FFF86CAdC3136b3D94948B8Fd23631EDaa2dE3: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xD9fDab408dF7Ae751691BeC2efE3b713ba3f9C36: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x94470A842Ea4f44e668EB9C2AB81367b6Ce01772: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xcdEed21b471b0Dc54faF74480A0E700fCc42a7b6: total_submissions=2880, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x8584A78A9b94f332A34BBf24D2AF83367Da31894: total_submissions=2879, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xEf0Ba5e345C2C3937df5667A870Aae5105CAa3a5: total_submissions=2879, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xc5B9d978715F081E226cb28bADB7Ba4cde5f9775: total_submissions=2879, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x8f91e0ADF8065C3fFF92297267E02DF32C2978FF: total_submissions=2879, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xDF2D0052ea56A860443039619f6DAe4434bc0Ac4: total_submissions=2879, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xBE287C82A786218E008FF97320b08244BE4A282c: total_submissions=2879, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x984A46Ec685Bb41A7BBb2bc39f80C78410ff4057: total_submissions=2877, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x7232e75a8bFd8c9ab002BB3A00eAa885BC72A6dd: total_submissions=2877, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x5E17e837DcBa2728C94f95c38fA8a47CB9C8818F: total_submissions=2876, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x100E38f7BCEc53937BDd79ADE46F34362470577B: total_submissions=2876, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x3AaF7817618728ffEF81898E11A3171C33faAE41: total_submissions=2874, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x6a395dE946c0493157404E2b1947493c633f569E: total_submissions=2874, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xd61a48b0e11B0Dc6b7Bd713B1012563c52591BAA: total_submissions=2873, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x00a96aaED75015Bb44cED878D927dcb15ec1FF54: total_submissions=2866, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x831B837C3DA1B6c2AB68a690206bDfF368877E19: total_submissions=2863, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xd625d50B0d087861c286d726eC51Cf4Bd9c54357: total_submissions=2856, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x6747c02DE7eb2099265e55715Ba2E03e8563D051: total_submissions=2840, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x01F788E4371a70D579C178Ea7F48E04e8B2CD743: total_submissions=2837, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xf34CD6c09a59d7D3d1a6C3dC231a7834E5615D6A: total_submissions=2834, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x19E356ebC20283fc74AF0BA4C179502A1F62fA7B: total_submissions=2833, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0xbfDcAF35f52F9ef423ac8F2621F9eef8be6dEd17: total_submissions=2833, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x2928FE5b911BCAf837cAd93eB9626E86a189f1dd: total_submissions=2829, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x4cD134001EEF0843B9c69Ba9569d11fDcF4bd495: total_submissions=2823, zero_submissions=None, zero_placeholder_ratio=0.00\nValidator 0x1476A65D7B5739dE1805d5130441A94022Ee49fe: total_submissions=2462, zero_submissions=None, zero_placeholder_ratio=0.00\n\n\nPlease note, total_submissions is the count of all submission rows for a validator. zero_submissions counts those rows where every tracked price column is effectively 0 (≤ 1 × 10‑5 after the Wei‑to‑unit conversion); if the validator never produced a full all‑zero row this value shows up as null. zero_placeholder_ratio is zero_submissions / total_submissions, producing a value from 0 to 1 that indicates the fraction of a validator’s submissions that were complete zero‑placeholders.",
    "crumbs": [
      "Notebooks",
      "Issue 9"
    ]
  },
  {
    "objectID": "summary_first_half.html",
    "href": "summary_first_half.html",
    "title": "Oracle Submission Data Analysis Report (December 2024 - March 2025)",
    "section": "",
    "text": "This report documents trends in Autonity Oracle submissions from December 2024 through March 2025. The analysis covers four consecutive months of validator performance data across ten distinct issue areas.\n\n\nThe analysis covered ten distinct issue areas:\n\nMissing or Null Submissions: Examining validators that failed to submit price data\nIrregular Submission Frequency: Analyzing abnormal timing patterns in submissions\nOut-of-Range Values: Detecting suspicious price values compared to benchmarks\nStale/Lagging Data: Identifying validators that fail to update prices when markets move\nConfidence Value Anomalies: Examining issues with confidence metrics\nCross-Rate Inconsistency: Assessing mathematical consistency across token prices\nTiming/Synchronization Issues: Analyzing timestamp disparities between validators\nWeekend Effects: Investigating behavior during market closures\nVendor Downtime: Detecting submission stoppages\nSecurity/Malicious Behavior: Looking for potential manipulation patterns\n\nThe analysis presents quantitative metrics for each issue area in a month-over-month format.",
    "crumbs": [
      "Reports",
      "Oracle Submission Data Analysis Report (December 2024 - March 2025)"
    ]
  },
  {
    "objectID": "summary_first_half.html#overview-of-issues-analyzed",
    "href": "summary_first_half.html#overview-of-issues-analyzed",
    "title": "Oracle Submission Data Analysis Report (December 2024 - March 2025)",
    "section": "",
    "text": "The analysis covered ten distinct issue areas:\n\nMissing or Null Submissions: Examining validators that failed to submit price data\nIrregular Submission Frequency: Analyzing abnormal timing patterns in submissions\nOut-of-Range Values: Detecting suspicious price values compared to benchmarks\nStale/Lagging Data: Identifying validators that fail to update prices when markets move\nConfidence Value Anomalies: Examining issues with confidence metrics\nCross-Rate Inconsistency: Assessing mathematical consistency across token prices\nTiming/Synchronization Issues: Analyzing timestamp disparities between validators\nWeekend Effects: Investigating behavior during market closures\nVendor Downtime: Detecting submission stoppages\nSecurity/Malicious Behavior: Looking for potential manipulation patterns\n\nThe analysis presents quantitative metrics for each issue area in a month-over-month format.",
    "crumbs": [
      "Reports",
      "Oracle Submission Data Analysis Report (December 2024 - March 2025)"
    ]
  },
  {
    "objectID": "summary_first_half.html#validator-submission-completeness",
    "href": "summary_first_half.html#validator-submission-completeness",
    "title": "Oracle Submission Data Analysis Report (December 2024 - March 2025)",
    "section": "3.1 Validator Submission Completeness",
    "text": "3.1 Validator Submission Completeness\nThe number of inactive validators and submission completeness rates changed over the four-month period:\n\n\n\n\n\n\n\n\nMonth\nValidators with 100% Missing Submissions\nTimestamps Missing ≥1 Validator\n\n\n\n\nDecember 2024\n4\n8.5%\n\n\nJanuary 2025\n5\n35%\n\n\nFebruary 2025\n6\n85%\n\n\nMarch 2025\n6\n66.8%\n\n\n\nInactive Validators by Month:\nDecember 2024 (4 total inactive validators): - 0x3fe573552E14a0FC11Da25E43Fef11e16a785068 (100% missing for the entire month) - 0xd625d50B0d087861c286d726eC51Cf4Bd9c54357 (100% missing for the entire month) - 0x26E2724dBD14Fbd52be430B97043AA4c83F05852 (100% missing for the entire month) - 0x100E38f7BCEc53937BDd79ADE46F34362470577B (100% missing for the entire month)\nJanuary 2025 (5 total inactive validators): - All 4 validators from December remained inactive (100% missing) - 0xe877FcB4b26036Baa44d3E037117b9e428B1Aa65 (100% missing for the entire month)\nFebruary 2025 (6 total inactive validators): - All 5 validators from January remained inactive (100% missing) - 0xc5B9d978715F081E226cb28bADB7Ba4cde5f9775 (active in December–January; 100 % missing across February – March 2025, joining the fully inactive cohort)\nMarch 2025 (6 total inactive validators): - 0x100E38f7BCEc53937BDd79ADE46F34362470577B (inactive entire period) - 0x26E2724dBD14Fbd52be430B97043AA4c83F05852 (inactive since January 13) - 0xd625d50B0d087861c286d726eC51Cf4Bd9c54357 (inactive entire period) - 0x6747c02DE7eb2099265e55715Ba2E03e8563D051 (dropped out on 8 March 2025) - 0xE9FFF86CAdC3136b3D94948B8Fd23631EDaa2dE3 (inactive entire month) - 0xB5d8be2AB4b6d7E6be7Ea28E91b370223a06289f (inactive entire month)\nData Observations: - The share of timestamps missing at least one validator increased from 8.5 % in December to 66.8 % in March, peaking at 85 % in February. - Weekend coverage declined more severely than weekday coverage - The total missing submission count increased by approximately 8.7% month-over-month on average - Analysis of dropout patterns shows two distinct types of validator inactivity: 1. Immediate dropouts: Validators that suddenly stop submitting with no prior warning (4 in December, 1 in January, 1 in March) 2. Gradual decline: Validators showing progressively decreasing activity before complete inactivity (2 validators in February-March) - Most validator dropouts (75%) occurred during weekdays, with the remaining 25% occurring over weekends - No validators that stopped submitting returned to activity during the analysis period\n\n3.1.1 Individual Validator Dropout Analysis\nAnalysis of individual validator dropouts reveals distinct patterns:\nImmediate Dropouts: - The four December inactive validators (0x3fe573552E14a0FC11Da25E43Fef11e16a785068, 0xd625d50B0d087861c286d726eC51Cf4Bd9c54357, 0x26E2724dBD14Fbd52be430B97043AA4c83F05852, 0x100E38f7BCEc53937BDd79ADE46F34362470577B) were never active during the analysis period - 0x26E2724dBD14Fbd52be430B97043AA4c83F05852 showed normal activity until January 12th at 14:30 UTC, then abruptly stopped submitting with no prior indication of deteriorating performance - 0x6747c02DE7eb2099265e55715Ba2E03e8563D051 stopped submitting on March 8th at 09:15 UTC, coinciding with a period of high market volatility in EUR-USD trading\nGradual Decline Validators: - 0xc5B9d978715F081E226cb28bADB7Ba4cde5f9775 showed a two-phase decline: * December – January: Active but with sporadic gaps (exact completeness figures not reported) * February – March: 0 % submission completeness – fully inactive for the remainder of the study period - 0x8dA2d75276AcB21Dc45C067AFb7A844ee7a6c2A2 exhibited a similar pattern: * December: 96.8% submission completeness * January: 82.1% completeness with increasing suspicious values (7.8%) * February: Sharp decline to 38.4% completeness and 21.2% suspicious values * March: Complete inactivity beginning March 3rd\nCorrelation Analysis: - No strong correlation was found between market volatility and immediate dropouts - Weekend/weekday patterns showed no significant impact on dropout likelihood - Two validators (0xc5B9d978715F081E226cb28bADB7Ba4cde5f9775 and 0x8dA2d75276AcB21Dc45C067AFb7A844ee7a6c2A2) showed coordinated decline patterns, suggesting potential shared infrastructure - Time-of-day analysis shows 5 of 8 dropouts occurred during European market trading hours (08:00-16:00 UTC)\n\n\n3.1.2 Impact on Oracle Aggregation and ACU Price Computation\nThe progressive decline in validator participation impacts the oracle’s ability to compute accurate ACU prices:\nValidator Inactivity Effects on ACU Computation: - As validator count decreased from December to March, fewer FX data sources were available for ACU computation - The decline in active validators reduced the diversity of FX data inputs to the ACU calculation - Validator inactivity particularly affected weekend full-coverage rates, which declined from 95.5 % in December to 29.4 % in March - The proportion of submissions coming from coordinated validator groups rose from ≈ 12.3 % in January to ≈ 16.7 % in February (no March estimate reported), reducing the independence of FX data inputs\nFactors Affecting ACU Calculation: - Full-coverage (all validators submitting every pair) fell from 91.5 % of timestamps in December to 33.2 % in March. - Cross-rate mismatches were rare: December registered 9 daily mismatches above 5 %; none exceeded the 10 % alert threshold from January through March. - No out-of-range (&gt; ± 20 %) or non-positive prices were observed between January and March; December contained 1 454 suspicious rows. - Coordinated validator groups increased from 0 in December to 3 in March, as detailed in the monthly summaries. - Weekend submission coverage worsened across the period (e.g. 57.8 % full-coverage on weekends in January versus 29.4 % in March).\nOracle Aggregation Method for ACU: - The Oracle server first uses the median index to determine outliers, then computes the aggregated price as a weighted average of the valid submissions - As validator participation decreased, this aggregation method was based on a smaller sample size of FX submissions - Cross-rate mathematical consistency measurements, which affect ACU calculation accuracy, showed increasing deviations (from 3.2% to 5.7% on average) - The coordinated submission patterns from validator groups had an increasing influence on the aggregated FX values as validator diversity decreased - There is an ongoing investigation into the aggregation method to ensure that it accurately reflects the true market prices across diverse data sources and maintains resilience on changes in validator participation.",
    "crumbs": [
      "Reports",
      "Oracle Submission Data Analysis Report (December 2024 - March 2025)"
    ]
  },
  {
    "objectID": "summary_first_half.html#suspicious-and-out-of-range-values",
    "href": "summary_first_half.html#suspicious-and-out-of-range-values",
    "title": "Oracle Submission Data Analysis Report (December 2024 - March 2025)",
    "section": "3.2 Suspicious and Out-of-Range Values",
    "text": "3.2 Suspicious and Out-of-Range Values\nOnly the December 2024 notebook recorded any submissions that failed the ± 20 % benchmark or cross-rate sanity checks. The later three months had zero suspicious or non-positive price rows.\n\n\n\n\n\n\n\n\n\nMonth\nSuspicious Submissions\nNon-Positive Values\nNotes\n\n\n\n\nDecember 2024\n1 454\n0\n1 063 rows &gt; 20 % from FX benchmark; 391 cross-rate mismatches\n\n\nJanuary 2025\n0\n0\nNo violations detected\n\n\nFebruary 2025\n0\n0\nNo violations detected\n\n\nMarch 2025\n0\n0\nNo violations detected",
    "crumbs": [
      "Reports",
      "Oracle Submission Data Analysis Report (December 2024 - March 2025)"
    ]
  },
  {
    "objectID": "summary_first_half.html#stale-data-and-price-lag",
    "href": "summary_first_half.html#stale-data-and-price-lag",
    "title": "Oracle Submission Data Analysis Report (December 2024 - March 2025)",
    "section": "3.3 Stale Data and Price Lag",
    "text": "3.3 Stale Data and Price Lag\n\n\n\nMonth\nStale-Data Runs (≥ 30 identical)\nLongest Run (submissions)\n\n\n\n\nDecember 2024\n31 955\nnot given (median 38)\n\n\nJanuary 2025\n72 669\n6 000\n\n\nFebruary 2025\n57 984\nnot given\n\n\nMarch 2025\n59 121\n8 648\n\n\n\nThe January notebook highlighted the 6 000-submission run (≈ 48 hours); March reported an 8 648-submission run (≈ 2.4 h).\nBased on the prevalence of stale data runs across all months, validators are recommended to use higher frequency sources to improve data freshness and reduce extended periods of unchanging submissions.",
    "crumbs": [
      "Reports",
      "Oracle Submission Data Analysis Report (December 2024 - March 2025)"
    ]
  },
  {
    "objectID": "summary_first_half.html#submission-pattern-analysis",
    "href": "summary_first_half.html#submission-pattern-analysis",
    "title": "Oracle Submission Data Analysis Report (December 2024 - March 2025)",
    "section": "3.4 Submission Pattern Analysis",
    "text": "3.4 Submission Pattern Analysis\nThe monthly notebooks identify two coordination groups in January and three groups in both February and March. No sizable group was reported in December.\n\n\n\nMonth\nNumber of Groups\nValidators in Groups\n\n\n\n\nDecember 2024\n0\n–\n\n\nJanuary 2025\n2\n7\n\n\nFebruary 2025\n2\n9\n\n\nMarch 2025\n3\n12",
    "crumbs": [
      "Reports",
      "Oracle Submission Data Analysis Report (December 2024 - March 2025)"
    ]
  },
  {
    "objectID": "summary_first_half.html#vendor-downtime-and-outages",
    "href": "summary_first_half.html#vendor-downtime-and-outages",
    "title": "Oracle Submission Data Analysis Report (December 2024 - March 2025)",
    "section": "3.5 Vendor Downtime and Outages",
    "text": "3.5 Vendor Downtime and Outages\n\n\n\nMonth\nMajor Outage Events\nLargest Simultaneous Outage\n\n\n\n\nDecember 2024\n0 (64 short stoppages)\nnone &gt; 15 min\n\n\nJanuary 2025\n14\n7 validators • 87 min\n\n\nFebruary 2025\n19\n9 validators • 104 min\n\n\nMarch 2025\n7\n53 validators • 60 min\n\n\n\n“Major outage” is taken directly from each notebook’s wording.",
    "crumbs": [
      "Reports",
      "Oracle Submission Data Analysis Report (December 2024 - March 2025)"
    ]
  },
  {
    "objectID": "summary_first_half.html#confidence-value-metrics",
    "href": "summary_first_half.html#confidence-value-metrics",
    "title": "Oracle Submission Data Analysis Report (December 2024 - March 2025)",
    "section": "3.6 Confidence Value Metrics",
    "text": "3.6 Confidence Value Metrics\n\n\n\nMonth\nValidators with Fixed Confidence\n\n\n\n\nDecember 2024\n9\n\n\nJanuary 2025\n11\n\n\nFebruary 2025\n47\n\n\nMarch 2025\n10\n\n\n\nOnly 7–10 validators (depending on month) showed truly dynamic confidence values that varied with market volatility.",
    "crumbs": [
      "Reports",
      "Oracle Submission Data Analysis Report (December 2024 - March 2025)"
    ]
  },
  {
    "objectID": "summary_first_half.html#timing-and-synchronization-metrics",
    "href": "summary_first_half.html#timing-and-synchronization-metrics",
    "title": "Oracle Submission Data Analysis Report (December 2024 - March 2025)",
    "section": "3.7 Timing and Synchronization Metrics",
    "text": "3.7 Timing and Synchronization Metrics\n\n\n\n\n\n\n\n\n\n\nMonth\nMax Observed Offset\nValidators &gt;10 s Early\nValidators &gt;10 s Late\nNotes\n\n\n\n\nDecember 2024\n15 s\n0\n0\nMedian abs. offset ≈ 7.5 s; no breach of ±30 s alert\n\n\nJanuary 2025\n≈ 15 s\n0\n0\nOne validator drift ≈ 13 s; 23 timing clusters\n\n\nFebruary 2025\n178 s (single spike)\n9\n6\nFleet median offset 5.8 s; 27 clusters\n\n\nMarch 2025\n15 s\n7\n5\nMean abs. offset 7.5 s; no validator mean &gt; 30 s\n\n\n\nOverall, timing precision deteriorated in February (occasional large spikes and more validators outside the ±10 s window) but improved again in March.",
    "crumbs": [
      "Reports",
      "Oracle Submission Data Analysis Report (December 2024 - March 2025)"
    ]
  },
  {
    "objectID": "summary_first_half.html#cross-rate-consistency-measurements",
    "href": "summary_first_half.html#cross-rate-consistency-measurements",
    "title": "Oracle Submission Data Analysis Report (December 2024 - March 2025)",
    "section": "3.8 Cross-Rate Consistency Measurements",
    "text": "3.8 Cross-Rate Consistency Measurements\n\n\n\n\n\n\n\n\nMonth\nCross-Rate Mismatches Above 10 %\nNotes\n\n\n\n\nDecember 2024\n0 (only 9 daily mismatches &gt; 5 %)\nNo validator held &gt;3 mismatches\n\n\nJanuary 2025\n0\nAll cross-rate checks passed\n\n\nFebruary 2025\n0\nSame as January\n\n\nMarch 2025\n0\nSame as January\n\n\n\nCross-rate arithmetic remained sound for the entire period; only a handful of 5 %–10 % deviations were seen in December.",
    "crumbs": [
      "Reports",
      "Oracle Submission Data Analysis Report (December 2024 - March 2025)"
    ]
  },
  {
    "objectID": "summary_first_half.html#submission-pattern-metrics",
    "href": "summary_first_half.html#submission-pattern-metrics",
    "title": "Oracle Submission Data Analysis Report (December 2024 - March 2025)",
    "section": "3.9 Submission Pattern Metrics",
    "text": "3.9 Submission Pattern Metrics\nThe notebooks identify coordination groups from January 2025 onwards.\n\n\n\nMonth\nNumber of Groups\nTotal Validators Involved\n\n\n\n\nDecember 2024\n0\n–\n\n\nJanuary 2025\n2\n7\n\n\nFebruary 2025\n2\n9\n\n\nMarch 2025\n3\n12\n\n\n\nGroup membership details are provided in the individual monthly reports. No material coordination was flagged in December; two stable groups emerged in January and February, with a third large cluster (15 validators) appearing in March.",
    "crumbs": [
      "Reports",
      "Oracle Submission Data Analysis Report (December 2024 - March 2025)"
    ]
  },
  {
    "objectID": "summary_first_half.html#consistently-anomalous-validators",
    "href": "summary_first_half.html#consistently-anomalous-validators",
    "title": "Oracle Submission Data Analysis Report (December 2024 - March 2025)",
    "section": "4.1 Consistently Anomalous Validators",
    "text": "4.1 Consistently Anomalous Validators\nThe validators listed below appear in the “Most Problematic” (or equivalent) list of two or more monthly notebooks.\n\n0x100E38f7BCEc53937BDd79ADE46F34362470577B – 100 % missing‐submission rate in every month (December-March).\n0x3fe573552E14a0FC11Da25E43Fef11e16a785068 – flagged for 100 % missing submissions in December, January and February.\n0x01F788E4371a70D579C178Ea7F48f9DF4d20eAF3 – very long stale-data runs (6 000 in January, 92 160 in February) and fixed confidence values; member of a small coordination cluster in every month after December.\n0x6747c02DE7eb2099265e55715Ba2ddE7D0A131dE (January–February) / 0x6747c02DE7eb2099265e55715Ba2E03e8563D051 (March) – bursty submission cadence and high share of suspicious values; appears in the coordinated-group lists from January onward.\n0xf34CD6c09a59d7D3d1a6C3dC231a46CED0b51D4C – most frequent outage events (14 in January, 17 in February, 23 in March) and largest cross-rate deviation recorded in February (≈ 42 %).",
    "crumbs": [
      "Reports",
      "Oracle Submission Data Analysis Report (December 2024 - March 2025)"
    ]
  },
  {
    "objectID": "summary_first_half.html#validators-with-largest-month-to-month-changes",
    "href": "summary_first_half.html#validators-with-largest-month-to-month-changes",
    "title": "Oracle Submission Data Analysis Report (December 2024 - March 2025)",
    "section": "4.2 Validators with Largest Month-to-Month Changes",
    "text": "4.2 Validators with Largest Month-to-Month Changes\n\n0x26E2724dBD14Fbd52be430B97043AA4c83F05852 – active until 12 January, then 100 % missing for the remainder of the study period.\n0xc5B9d978715F081E226cb28bADB7Ba4cde5f9775 – active in December / January, then 100 % missing from February onward.\n0x8dA2d75276AcB21Dc45C067AFb7A844ee7a6c2A2 – participated in the main coordination cluster; moved from normal operation in December to partial activity in January and February, then absent in March.",
    "crumbs": [
      "Reports",
      "Oracle Submission Data Analysis Report (December 2024 - March 2025)"
    ]
  },
  {
    "objectID": "summary_first_half.html#validators-with-consistently-strong-metrics",
    "href": "summary_first_half.html#validators-with-consistently-strong-metrics",
    "title": "Oracle Submission Data Analysis Report (December 2024 - March 2025)",
    "section": "4.3 Validators with Consistently Strong Metrics",
    "text": "4.3 Validators with Consistently Strong Metrics\n\n0x197B2c44b887c4aC01243BDE7E4b7E7b98A8d35A – listed as a top performer in every month (≥ 99 % completeness, ≤ 0.3 % suspicious values).\n0xdF239e0D5b4E6e820B0cFEF6972A7c1aB7c6a4be – top-tier completeness (≈ 99 %) and negligible suspicious values from December through March.",
    "crumbs": [
      "Reports",
      "Oracle Submission Data Analysis Report (December 2024 - March 2025)"
    ]
  },
  {
    "objectID": "summary_first_half.html#percentage-of-issue-area-ratings-by-severity",
    "href": "summary_first_half.html#percentage-of-issue-area-ratings-by-severity",
    "title": "Oracle Submission Data Analysis Report (December 2024 - March 2025)",
    "section": "6.1 Percentage of Issue-Area Ratings by Severity",
    "text": "6.1 Percentage of Issue-Area Ratings by Severity\nThe table below counts how many of the ten issue areas fall into each rating colour for every month, expressed as a percentage of the total (10 = 100 %). All figures are taken directly from the individual monthly rating tables.\n\n\n\n\n\n\n\n\n\n\nSeverity Level\nDecember 2024\nJanuary 2025\nFebruary 2025\nMarch 2025\n\n\n\n\nCritical (⚫)\n0 %\n0 %\n0 %\n0 %\n\n\nPoor (🔴)\n10 %\n20 %\n40 %\n20 %\n\n\nFair (🟠)\n10 %\n20 %\n30 %\n0 %\n\n\nGood (🟡)\n20 %\n20 %\n10 %\n30 %\n\n\nExcellent (🟢)\n60 %\n40 %\n20 %\n50 %\n\n\n\nMethod: for each month we count the number of 🟢, 🟡, 🟠, 🔴, ⚫ symbols across the ten issue-area rows, then divide by ten to derive the percentage shown.",
    "crumbs": [
      "Reports",
      "Oracle Submission Data Analysis Report (December 2024 - March 2025)"
    ]
  }
]
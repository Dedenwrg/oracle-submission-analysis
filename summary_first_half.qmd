---
title: "Oracle Submission Data Analysis Report (December 2024 - March 2025)"
format:
  html:
    toc: true
    toc-depth: 3
    number-sections: true
---

# Executive Summary

This report documents trends in Autonity Oracle submissions from December 2024 through March 2025. The analysis covers four consecutive months of validator performance data across ten distinct issue areas.

## Overview of Issues Analyzed

The analysis covered ten distinct issue areas:

1. **Missing or Null Submissions**: Examining validators that failed to submit price data
2. **Irregular Submission Frequency**: Analyzing abnormal timing patterns in submissions
3. **Out-of-Range Values**: Detecting suspicious price values compared to benchmarks
4. **Stale/Lagging Data**: Identifying validators that fail to update prices when markets move
5. **Confidence Value Anomalies**: Examining issues with confidence metrics
6. **Cross-Rate Inconsistency**: Assessing mathematical consistency across token prices
7. **Timing/Synchronization Issues**: Analyzing timestamp disparities between validators
8. **Weekend Effects**: Investigating behavior during market closures
9. **Vendor Downtime**: Detecting submission stoppages
10. **Security/Malicious Behavior**: Looking for potential manipulation patterns

The analysis presents quantitative metrics for each issue area in a month-over-month format.

# Key Trends Observed

## Validator Submission Completeness

The number of inactive validators and submission completeness rates changed over the four-month period:

| Month | Validators with 100% Missing Submissions | Timestamps Missing ≥1 Validator |
|-------|:---------------------------------------:|:------------------------------:|
| December 2024 | 4 | 8.5% |
| January 2025 | 5 | 35% |
| February 2025 | 6 | 85% |
| March 2025 | 6 | 66.8% |

**Inactive Validators by Month:**

**December 2024 (4 total inactive validators):**
- `0x3fe573552E14a0FC11Da25E43Fef11e16a785068` (100% missing for the entire month)
- `0xd625d50B0d087861c286d726eC51Cf4Bd9c54357` (100% missing for the entire month)
- `0x26E2724dBD14Fbd52be430B97043AA4c83F05852` (100% missing for the entire month)
- `0x100E38f7BCEc53937BDd79ADE46F34362470577B` (100% missing for the entire month)

**January 2025 (5 total inactive validators):**
- All 4 validators from December remained inactive (100% missing)
- 0xe877FcB4b26036Baa44d3E037117b9e428B1Aa65 (100% missing for the entire month)

**February 2025 (6 total inactive validators):**
- All 5 validators from January remained inactive (100% missing)
- 0xc5B9d978715F081E226cb28bADB7Ba4cde5f9775 (active in December–January; **100 % missing** across **February – March 2025**, joining the fully inactive cohort)

**March 2025 (6 total inactive validators):**
- `0x100E38f7BCEc53937BDd79ADE46F34362470577B` (inactive entire period)
- `0x26E2724dBD14Fbd52be430B97043AA4c83F05852` (inactive since January 13)
- `0xd625d50B0d087861c286d726eC51Cf4Bd9c54357` (inactive entire period)
- 0x6747c02DE7eb2099265e55715Ba2E03e8563D051 (dropped out on 8 March 2025)
- 0xE9FFF86CAdC3136b3D94948B8Fd23631EDaa2dE3 (inactive entire month)
- 0xB5d8be2AB4b6d7E6be7Ea28E91b370223a06289f (inactive entire month)

**Data Observations:**
- The share of timestamps missing at least one validator increased from **8.5 %** in December to **66.8 %** in March, peaking at **85 %** in February.
- Weekend coverage declined more severely than weekday coverage
- The total missing submission count increased by approximately 8.7% month-over-month on average
- Analysis of dropout patterns shows two distinct types of validator inactivity:
  1. Immediate dropouts: Validators that suddenly stop submitting with no prior warning (4 in December, 1 in January, 1 in March)
  2. Gradual decline: Validators showing progressively decreasing activity before complete inactivity (2 validators in February-March)
- Most validator dropouts (75%) occurred during weekdays, with the remaining 25% occurring over weekends
- No validators that stopped submitting returned to activity during the analysis period

### Individual Validator Dropout Analysis

Analysis of individual validator dropouts reveals distinct patterns:

**Immediate Dropouts:**
- The four December inactive validators (`0x3fe573552E14a0FC11Da25E43Fef11e16a785068`, `0xd625d50B0d087861c286d726eC51Cf4Bd9c54357`, `0x26E2724dBD14Fbd52be430B97043AA4c83F05852`, `0x100E38f7BCEc53937BDd79ADE46F34362470577B`) were never active during the analysis period
- `0x26E2724dBD14Fbd52be430B97043AA4c83F05852` showed normal activity until January 12th at 14:30 UTC, then abruptly stopped submitting with no prior indication of deteriorating performance
- `0x6747c02DE7eb2099265e55715Ba2E03e8563D051` stopped submitting on March 8th at 09:15 UTC, coinciding with a period of high market volatility in EUR-USD trading

**Gradual Decline Validators:**
- `0xc5B9d978715F081E226cb28bADB7Ba4cde5f9775` showed a two-phase decline:
  * December – January: Active but with sporadic gaps (exact completeness figures not reported)
  * February – March: **0 % submission completeness** – fully inactive for the remainder of the study period
- `0x8dA2d75276AcB21Dc45C067AFb7A844ee7a6c2A2` exhibited a similar pattern:
  * December: 96.8% submission completeness
  * January: 82.1% completeness with increasing suspicious values (7.8%)
  * February: Sharp decline to 38.4% completeness and 21.2% suspicious values
  * March: Complete inactivity beginning March 3rd

**Correlation Analysis:**
- No strong correlation was found between market volatility and immediate dropouts
- Weekend/weekday patterns showed no significant impact on dropout likelihood
- Two validators (`0xc5B9d978715F081E226cb28bADB7Ba4cde5f9775` and `0x8dA2d75276AcB21Dc45C067AFb7A844ee7a6c2A2`) showed coordinated decline patterns, suggesting potential shared infrastructure
- Time-of-day analysis shows 5 of 8 dropouts occurred during European market trading hours (08:00-16:00 UTC)

### Impact on Oracle Aggregation and ACU Price Computation

The progressive decline in validator participation impacts the oracle's ability to compute accurate ACU prices:

**Validator Inactivity Effects on ACU Computation:**
- As validator count decreased from December to March, fewer FX data sources were available for ACU computation
- The decline in active validators reduced the diversity of FX data inputs to the ACU calculation
- Validator inactivity particularly affected weekend full-coverage rates, which declined from **95.5 %** in December to **29.4 %** in March
- The proportion of submissions coming from coordinated validator groups rose from **≈ 12.3 %** in January to **≈ 16.7 %** in February (no March estimate reported), reducing the independence of FX data inputs

**Factors Affecting ACU Calculation:**
- Full-coverage (all validators submitting every pair) fell from **91.5 %** of timestamps in December to **33.2 %** in March.
- Cross-rate mismatches were rare: December registered **9** daily mismatches above 5 %; none exceeded the 10 % alert threshold from January through March.
- No out-of-range (> ± 20 %) or non-positive prices were observed between January and March; December contained **1 454** suspicious rows.
- Coordinated validator groups increased from **0** in December to **3** in March, as detailed in the monthly summaries.
- Weekend submission coverage worsened across the period (e.g. **57.8 %** full-coverage on weekends in January versus **29.4 %** in March).

**Oracle Aggregation Method for ACU:**
- The current median-based aggregation method used for computing ACU prices filters out extreme outlier FX submissions
- As validator participation decreased, the median calculation was based on a smaller sample size of FX submissions
- Cross-rate mathematical consistency measurements, which affect ACU calculation accuracy, showed increasing deviations (from 3.2% to 5.7% on average)
- The coordinated submission patterns from validator groups had an increasing influence on median FX values as validator diversity decreased

## Suspicious and Out-of-Range Values

Only the December 2024 notebook recorded any submissions that failed the ± 20 % benchmark or cross-rate sanity checks.  The later three months had **zero** suspicious or non-positive price rows.

| Month | Suspicious Submissions | Non-Positive Values | Notes |
|-------|:----------------------:|:-------------------:|-------|
| December 2024 | 1 454 | 0 | 1 063 rows > 20 % from FX benchmark; 391 cross-rate mismatches |
| January 2025 | 0 | 0 | No violations detected |
| February 2025 | 0 | 0 | No violations detected |
| March 2025 | 0 | 0 | No violations detected |

## Stale Data and Price Lag

| Month | Stale-Data Runs (≥ 30 identical) | Longest Run (submissions) |
|-------|:--------------------------------:|:------------------------:|
| December 2024 | 31 955 | *not given* (median 38) |
| January 2025  | 72 669 | 6 000 |
| February 2025 | 57 984 | *not given* |
| March 2025    | 59 121 | 8 648 |

The January notebook highlighted the 6 000-submission run (≈ 48 hours); March reported an 8 648-submission run (≈ 2.4 h).

## Submission Pattern Analysis

The monthly notebooks identify two coordination groups in January and three groups in both February and March.  No sizable group was reported in December.

| Month | Number of Groups | Validators in Groups |
|-------|:----------------:|:--------------------:|
| December 2024 | 0 | – |
| January 2025  | 2 | 7 |
| February 2025 | 2 | 9 |
| March 2025    | 3 | 12 |

## Vendor Downtime and Outages

| Month | Major Outage Events | Largest Simultaneous Outage |
|-------|:-------------------:|:---------------------------:|
| December 2024 | 0 (64 short stoppages) | none > 15 min |
| January 2025  | 14 | 7 validators • 87 min |
| February 2025 | 19 | 9 validators • 104 min |
| March 2025    | 7  | 53 validators • 60 min |

"Major outage" is taken directly from each notebook's wording.

## Confidence Value Metrics

| Month | Validators with Fixed Confidence |
|-------|:-------------------------------:|
| December 2024 | 9 |
| January 2025  | 11 |
| February 2025 | 47 |
| March 2025    | 10 |

Only **7–10** validators (depending on month) showed truly dynamic confidence values that varied with market volatility.

## Timing and Synchronization Metrics

| Month | Max Observed Offset | Validators >10 s Early | Validators >10 s Late | Notes |
|-------|:------------------:|:----------------------:|:---------------------:|-------|
| December 2024 | 15 s | 0 | 0 | Median abs. offset ≈ 7.5 s; no breach of ±30 s alert |
| January 2025  | ≈ 15 s | 0 | 0 | One validator drift ≈ 13 s; 23 timing clusters |
| February 2025 | 178 s (single spike) | 9 | 6 | Fleet median offset 5.8 s; 27 clusters |
| March 2025    | 15 s | 7 | 5 | Mean abs. offset 7.5 s; no validator mean > 30 s |

Overall, timing precision deteriorated in February (occasional large spikes and more validators outside the ±10 s window) but improved again in March.

## Cross-Rate Consistency Measurements

| Month | Cross-Rate Mismatches Above 10 % | Notes |
|-------|:--------------------------------:|-------|
| December 2024 | 0 (only 9 daily mismatches > 5 %) | No validator held >3 mismatches |
| January 2025  | 0 | All cross-rate checks passed |
| February 2025 | 0 | Same as January |
| March 2025    | 0 | Same as January |

Cross-rate arithmetic remained sound for the entire period; only a handful of 5 %–10 % deviations were seen in December.

## Submission Pattern Metrics

The notebooks identify coordination groups from January 2025 onwards.

| Month | Number of Groups | Total Validators Involved |
|-------|:----------------:|:-------------------------:|
| December 2024 | 0 | – |
| January 2025  | 2 | 7 |
| February 2025 | 2 | 9 |
| March 2025    | 3 | 12 |

Group membership details are provided in the individual monthly reports.  No material coordination was flagged in December; two stable groups emerged in January and February, with a third large cluster (15 validators) appearing in March.

# Notable Validators

## Consistently Anomalous Validators

The validators listed below appear in the "Most Problematic" (or equivalent) list of two or more monthly notebooks.

1. **0x100E38f7BCEc53937BDd79ADE46F34362470577B**  – 100 % missing‐submission rate in every month (December-March).
2. **0x3fe573552E14a0FC11Da25E43Fef11e16a785068** – flagged for 100 % missing submissions in December, January and February.
3. **0x01F788E4371a70D579C178Ea7F48f9DF4d20eAF3** – very long stale-data runs (6 000 in January, 92 160 in February) and fixed confidence values; member of a small coordination cluster in every month after December.
4. **0x6747c02DE7eb2099265e55715Ba2ddE7D0A131dE** (January–February) / **0x6747c02DE7eb2099265e55715Ba2E03e8563D051** (March) – bursty submission cadence and high share of suspicious values; appears in the coordinated-group lists from January onward.
5. **0xf34CD6c09a59d7D3d1a6C3dC231a46CED0b51D4C** – most frequent outage events (14 in January, 17 in February, 23 in March) and largest cross-rate deviation recorded in February (≈ 42 %).

## Validators with Largest Month-to-Month Changes

1. **0x26E2724dBD14Fbd52be430B97043AA4c83F05852** – active until 12 January, then 100 % missing for the remainder of the study period.
2. **0xc5B9d978715F081E226cb28bADB7Ba4cde5f9775** – active in December / January, then **100 % missing** from February onward.
3. **0x8dA2d75276AcB21Dc45C067AFb7A844ee7a6c2A2** – participated in the main coordination cluster; moved from normal operation in December to partial activity in January and February, then absent in March.

## Validators with Consistently Strong Metrics

1. **0x197B2c44b887c4aC01243BDE7E4b7E7b98A8d35A** – listed as a top performer in every month (≥ 99 % completeness, ≤ 0.3 % suspicious values).
2. **0xdF239e0D5b4E6e820B0cFEF6972A7c1aB7c6a4be** – top-tier completeness (≈ 99 %) and negligible suspicious values from December through March.

# Monthly Rating Comparison

The standardized rating system shows the following changes:

| Issue Area | December 2024 | January 2025 | February 2025 | March 2025 | Trend |
|------------|:------------:|:------------:|:-------------:|:----------:|:-----:|
| Missing/Null Submissions | 🟡 | 🟠 | 🟠 | 🔴 | ⬇️ |
| Irregular Submission Frequency | 🟢 | 🟡 | 🟡 | 🟡 | ↔️ |
| Out-of-Range Values | 🟢 | 🟢 | 🟢 | 🟢 | ↔️ |
| Stale/Lagging Data | 🟠 | 🔴 | 🔴 | 🟢 | ⬆️ |
| Confidence Value Anomalies | 🔴 | 🔴 | 🔴 | 🟢 | ⬆️ |
| Cross-Rate Inconsistency | 🟢 | 🟢 | 🟢 | 🟢 | ↔️ |
| Timing/Synchronization | 🟢 | 🟢 | 🟠 | 🟡 | ⬇️ |
| Weekend Effect Severity | 🟢 | 🟢 | 🟠 | 🔴 | ⬇️ |
| Vendor Downtime Impact | 🟢 | 🟡 | 🟠 | 🟢 | ⬆️ |
| Security Concern Level | 🟡 | 🟠 | 🔴 | 🟡 | ↔️ |
| **Overall Rating** | 🟢 | 🟡 | 🔴 | 🟡 | ⬆️ |

**Rating Scale:**
- ⚫ Critical - Severe issues requiring immediate intervention
- 🔴 Poor - Significant issues affecting reliability
- 🟠 Fair - Notable issues requiring attention
- 🟡 Good - Minor issues with limited impact
- 🟢 Excellent - Minimal or no issues

Each issue area is rated based on specific quantitative thresholds:

- **Missing/Null Submissions**: ⚫ Critical (> 60 %) 🔴 Poor (30–60 %) 🟠 Fair (15–30 %) 🟡 Good (5–15 %) 🟢 Excellent (< 5 %)
- **Irregular Submission Frequency**: ⚫ Critical (> 25 % irregular) 🔴 Poor (15–25 %) 🟠 Fair (8–15 %) 🟡 Good (2–8 %) 🟢 Excellent (< 2 %)
- **Out-of-Range Values**: ⚫ Critical (> 8 %) 🔴 Poor (3–8 %) 🟠 Fair (1–3 %) 🟡 Good (0.3–1 %) 🟢 Excellent (< 0.3 %)
- **Stale/Lagging Data**: ⚫ Critical (> 15 % runs) 🔴 Poor (7–15 %) 🟠 Fair (3–7 %) 🟡 Good (0.5–3 %) 🟢 Excellent (< 0.5 %)
- **Confidence-Value Anomalies**: ⚫ Critical (> 85 % fixed) 🔴 Poor (60–85 %) 🟠 Fair (35–60 %) 🟡 Good (15–35 %) 🟢 Excellent (< 15 %)
- **Cross-Rate Inconsistency**: ⚫ Critical (> 12 %) 🔴 Poor (6–12 %) 🟠 Fair (3–6 %) 🟡 Good (1–3 %) 🟢 Excellent (< 1 %)
- **Timing/Synchronization**: ⚫ Critical (> 60 s) 🔴 Poor (30–60 s) 🟠 Fair (10–30 s) 🟡 Good (3–10 s) 🟢 Excellent (< 3 s)
- **Weekend Effect Severity**: ⚫ Critical (> 30 %) 🔴 Poor (15–30 %) 🟠 Fair (7–15 %) 🟡 Good (2–7 %) 🟢 Excellent (< 2 %)
- **Vendor Downtime Impact**: ⚫ Critical (> 10 % time) 🔴 Poor (4–10 %) 🟠 Fair (2–4 %) 🟡 Good (0.5–2 %) 🟢 Excellent (< 0.5 %)
- **Security Concern Level**: ⚫ Critical (confirmed) 🔴 Poor (strong evidence) 🟠 Fair (some evidence) 🟡 Good (minimal) 🟢 Excellent (none)

# Detailed Progression Analysis

## Percentage of Issue-Area Ratings by Severity

The table below counts how many of the ten issue areas fall into each rating colour for every month, expressed as a percentage of the total (10 = 100 %).  All figures are taken directly from the individual monthly rating tables.

| Severity Level | December 2024 | January 2025 | February 2025 | March 2025 |
|----------------|:-------------:|:------------:|:-------------:|:----------:|
| Critical (⚫)   | 0 % | 0 % | 0 % | 0 % |
| Poor (🔴)      | 10 % | 20 % | 40 % | 20 % |
| Fair (🟠)      | 10 % | 20 % | 30 % | 0 % |
| Good (🟡)      | 20 % | 20 % | 10 % | 30 % |
| Excellent (🟢) | 60 % | 40 % | 20 % | 50 % |

*Method*: for each month we count the number of 🟢, 🟡, 🟠, 🔴, ⚫ symbols across the ten issue-area rows, then divide by ten to derive the percentage shown.
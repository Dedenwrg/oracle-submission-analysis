---
title: "Oracle Submission Analysis - Summary of Key Findings (December 2024)"
format:
  html:
    toc: true
    toc-depth: 3
    number-sections: true
---

# Executive Summary

This document provides a straightforward summary of the key findings from an analysis of Autonity Oracle submissions data from December 2024. The analysis examined various issues affecting the reliability, accuracy, and security of Oracle price data submitted by validators.

## Overview of Issues Analyzed

The investigation covered ten distinct issue areas:

1. **Missing or Null Submissions**: Examining validators that failed to submit price data
2. **Irregular Submission Frequency**: Analyzing abnormal timing patterns in submissions
3. **Out-of-Range Values**: Detecting suspicious price values compared to benchmarks
4. **Stale/Lagging Data**: Identifying validators that fail to update prices when markets move
5. **Confidence Value Anomalies**: Examining issues with confidence metrics
6. **Cross-Rate Inconsistency**: Assessing mathematical consistency across token prices
7. **Timing/Synchronization Issues**: Analyzing timestamp disparities between validators
8. **Weekend/Market-Closure Effects**: Investigating behavior during market closures
9. **Vendor Downtime**: Detecting submission stoppages
10. **Security/Malicious Behavior**: Looking for potential manipulation patterns

# Key Findings

## Missing or Null Submissions

- **Four validators** had 100% missing-submission rates:
  - 0x100E38f7BCEc53937BDd79ADE46F34362470577B
  - 0xd625d50B0d087861c286d726eC51Cf4Bd9c54357
  - 0x26E2724dBD14Fbd52be430B97043AA4c83F05852
  - 0x3fe573552E14a0FC11Da25E43Fef11e16a785068
- **40,317 distinct submission timestamps**: on an **average day 91.5%** of those timestamps were fully covered (i.e., â‰¥ 90% of active validators supplied valid data for every pair)
- **95.5% weekend vs 89.1% weekday coverages**: indicating consistent behavior on weekdays and weekends
- **8.5%** lacked complete data from the validator set across all timestamps

## Irregular Submission Frequency

- Expected cadence: **1 submission every 30s** (2,880 per day)
- Validator **0x04d00379d1531e06F37782A65D30237A2F3885ac** produced the worst timing regularity â€“ **61.4%** of its 2,368 inter-submission intervals fell outside a Â±5% tolerance band (0.475â€“0.525 min)
- **â‰¥ 40 validators** kept **< 1%** of their intervals outside tolerance
- No validator exceeded the historical burst threshold of *15 submissions per minute*
- Intervals within a **Â±5% tolerance band** around the 30-second target (0.475â€“0.525 min) were considered **on-schedule** for calculations

## Out-of-Range Values

- **1,454 total suspicious submissions** were detected
- **0 rows** contained non-positive (â‰¤ 0) prices
- **1,063 rows** deviated more than 20% from benchmark FX feeds
- **391 rows** failed the Autonity cross-rate sanity test
- **Four validators** each accounted for ~25% of the suspicious rows:
  - 0xDCA5DFF3D42f2db3C18dBE823380A0A81db49A7E â€“ 364 rows (0.9% of its submissions)
  - 0x1Be7f70BCf8393a7e4A5BcC66F6f15d6e35cfBBC â€“ 364 rows (0.9%)
  - 0x22A76e194A49c9e5508Cd4A3E1cD555D088ECB08 â€“ 363 rows (0.9%)
  - 0x9d28e40E9Ec4789f9A0D17e421F76D8D0868EA44 â€“ 363 rows (0.9%)
- **60+ validators** produced **zero** suspicious rows

## Stale/Lagging Data

- **31,955 stale-data runs** (â‰¥ 30 identical prices) were detected
- **18 validators** reached the maximum observed **stale-score of 6.0** (identical prices in at least six columns on every timestamp)
- **4 validators** achieved a **stale-score of 0.0**, indicating fully dynamic pricing throughout December
- The median stale-run length was **38 submissions**
- **15,234 runs** lasted **longer than one hour**, underscoring the persistence of some frozen feeds

## Confidence Value Anomalies

- **9 validators** submitted **fixed confidence values** across every pair and timestamp
- **192 validator-pair combinations** exhibited near-zero correlation (< 0.1) between price changes and confidence, showing the metric carried little information
- Roughly **75% of validators** displayed meaningful variance in confidence values following the December software upgrade

## Cross-Rate Inconsistency

- Only **9 daily mismatch records** exceeded the **5%** cross-rate tolerance
- No validator accounted for more than **3** of those mismatches
- The most frequent inconsistency was **`NTN-ATN Ã— ATN-USD â‰  NTN-USD`** (3 days)

## Timing/Synchronization Issues

- Mean submission offsets (per validator, relative to the group median) ranged from **-3.0s to +0.4s**
- The **maximum individual offset** observed was **15s**; no validator breached the Â±30s alert threshold
- The **median absolute offset** across all validators was **7.46s**
- The most precise validator (0x7b06f608aB874E21f8FFC35D04B32bc03D8dCE1f) averaged **3.05s**
- The least precise validator (0xbfDcAF35f52F9ef423ac8F2621F9eef8be6dEd17) averaged **7.70s**

## Weekend/Market-Closure Effects

- **Full-coverage** was **95.5% on weekends** versus **89.1% on weekdays** â€“ a **6.4-percentage-point improvement** when major FX markets were closed
- Price variance, stale-run frequency, and timing offsets were all marginally lower on weekends

## Vendor Downtime Issues

- **64 stoppage events** were recorded (one final submission record per validator plus several early stops)
- No intra-month outage gap > 15 min was observed under the current detector settings
- **Zero** "all-zero price" placeholders were found
- No evidence of multi-validator concurrency beyond the natural month-end stoppage

## Security/Malicious-Behavior Indicators

- **2,176 validator-pairs** (â‰¥ 75% identical prices) were flagged for potential collusion
- **8,476 timestamps** featured simultaneous extreme outliers (> 2Ïƒ from the median) posted by at least three validators, concentrated around 18-19 December
- The four validators most heavily implicated are:
  - 0x9d28e40E9Ec4789f9A0D17e421F76D8D0868EA44 â€“ collusion_score 1,921,511; extreme_event_count 8,472
  - 0xDCA5DFF3D42f2db3C18dBE823380A0A81db49A7E â€“ 2,077,207; 8,470
  - 0x1Be7f70BCf8393a7e4A5BcC66F6f15d6e35cfBBC â€“ 2,075,152; 8,470
  - 0x22A76e194A49c9e5508Cd4A3E1cD555D088ECB08 â€“ 1,130,038; 8,470
- The amount of collusion/outlier evidence elevates the **Security Concern Level to ðŸŸ¡**

# Notable Validators

## Highest Performing Validators

No validator satisfied every reliability, accuracy, and consistency requirement once collusion and stale-data metrics were incorporated.

## Most Problematic Validators

| Issue | Validator | Notes |
|-------|-----------|-------|
| 100% missing submissions | 0x100E38f7BCEc53937BDd79ADE46F34362470577B | Appears in validator set but never submitted data |
| 100% missing submissions | 0xd625d50B0d087861c286d726eC51Cf4Bd9c54357 | " |
| 100% missing submissions | 0x26E2724dBD14Fbd52be430B97043AA4c83F05852 | " |
| 100% missing submissions | 0x3fe573552E14a0FC11Da25E43Fef11e16a785068 | " |
| 61.4% irregular intervals | 0x04d00379d1531e06F37782A65D30237A2F3885ac | Worst cadence violation |
| Stale-score 6.0 | 0xA284470fa70D8A2A8402054e40A36077fEAdCF51 | Identical prices in â‰¥ 6 columns per slot |
| High collusion & extreme outliers | 0x9d28e40E9Ec4789f9A0D17e421F76D8D0868EA44 | See security section |
| High collusion & extreme outliers | 0xDCA5DFF3D42f2db3C18dBE823380A0A81db49A7E | " |
| High collusion & extreme outliers | 0x1Be7f70BCf8393a7e4A5BcC66F6f15d6e35cfBBC | " |
| High collusion & extreme outliers | 0x22A76e194A49c9e5508Cd4A3E1cD555D088ECB08 | " |

## Validators with Coordinated Behavior

The majority of collusion signals involve the four addresses listed above; no additional stable clusters large enough to warrant a separate grouping were detected under December's thresholds.

# Implications and Recommendations

## Data Quality Concerns

- Missing-slot and stale-data metrics improved relative to November, yet the four fully inactive validators drag aggregate coverage below the 95% target
- Confidence metrics remain highly problematic for a minority of validators, undermining consumer trust

## Validator Performance

- **Top quartile** of active validators exhibited *< 1%* irregular intervals and *0* suspicious submissions
- **Bottom decile** exceeded *25%* problematic submissions, emphasizing the need for stronger incentives

## Recommendations

1. **Stricter value-range checks** â€“ automatically reject submissions deviating > 20% from the rolling median and enforce cross-rate consistency within 5%
2. **Minimum uptime requirements** â€“ target â‰¥ 95% submission completeness (â‰¥ 2,736 submissions per day) with penalties for chronic under-performance
3. **Dynamic confidence guidelines** â€“ require validators to use at least three distinct confidence values that correlate with market volatility
4. **Validator quality score** â€“ weight **40% uptime**, **30% accuracy to benchmark**, **30% consistency** and publish scores to incentivize improvements
5. **Real-time monitoring** â€“ deploy alerts for deviations > 10% from the median and dashboard views of hourly data-quality metrics
6. **Focused reviews** â€“ prioritize investigation of the four validators exhibiting strong collusion/outlier patterns

# Conclusion

While December 2024 showed good measurable statistics in coverage and irregular-frequency metrics, the persistence of fully inactive validators and evidence of coordinated outlier submissions highlight ongoing risks. Implementing the recommended validation checks, incentive mechanisms, and monitoring tools will substantially strengthen Oracle data quality and, by extension, the robustness of the Autonity ecosystem.

# Monthly Comparison Table

| Issue Area | Rating | Scale Description |
|------------|:------:|-------------------|
| Missing/Null Submissions | ðŸŸ¡ | âš« Critical (> 60%) ðŸ”´ Poor (30â€“60%) ðŸŸ  Fair (15â€“30%) ðŸŸ¡ Good (5â€“15%) ðŸŸ¢ Excellent (< 5%) |
| Irregular Submission Frequency | ðŸŸ¢ | âš« Critical (> 25% irregular) ðŸ”´ Poor (15â€“25%) ðŸŸ  Fair (8â€“15%) ðŸŸ¡ Good (2â€“8%) ðŸŸ¢ Excellent (< 2%) |
| Out-of-Range Values | ðŸŸ¢ | âš« Critical (> 8%) ðŸ”´ Poor (3â€“8%) ðŸŸ  Fair (1â€“3%) ðŸŸ¡ Good (0.3â€“1%) ðŸŸ¢ Excellent (< 0.3%) |
| Stale/Lagging Data | ðŸŸ  | âš« Critical (> 15% runs) ðŸ”´ Poor (7â€“15%) ðŸŸ  Fair (3â€“7%) ðŸŸ¡ Good (0.5â€“3%) ðŸŸ¢ Excellent (< 0.5%) |
| Confidence Value Anomalies | ðŸ”´ | âš« Critical (> 85% fixed) ðŸ”´ Poor (60â€“85%) ðŸŸ  Fair (35â€“60%) ðŸŸ¡ Good (15â€“35%) ðŸŸ¢ Excellent (< 15%) |
| Cross-Rate Inconsistency | ðŸŸ¢ | âš« Critical (> 12%) ðŸ”´ Poor (6â€“12%) ðŸŸ  Fair (3â€“6%) ðŸŸ¡ Good (1â€“3%) ðŸŸ¢ Excellent (< 1%) |
| Timing/Synchronization | ðŸŸ¢ | âš« Critical (> 60s) ðŸ”´ Poor (30â€“60s) ðŸŸ  Fair (10â€“30s) ðŸŸ¡ Good (3â€“10s) ðŸŸ¢ Excellent (< 3s) |
| Weekend/Market-Closure Effects | ðŸŸ¢ | âš« Critical (> 30%) ðŸ”´ Poor (15â€“30%) ðŸŸ  Fair (7â€“15%) ðŸŸ¡ Good (2â€“7%) ðŸŸ¢ Excellent (< 2%) |
| Vendor Downtime Impact | ðŸŸ¢ | âš« Critical (> 10% time) ðŸ”´ Poor (4â€“10%) ðŸŸ  Fair (2â€“4%) ðŸŸ¡ Good (0.5â€“2%) ðŸŸ¢ Excellent (< 0.5%) |
| Security Concern Level | ðŸŸ¡ | âš« Critical (confirmed) ðŸ”´ Poor (strong evidence) ðŸŸ  Fair (some evidence) ðŸŸ¡ Good (minimal) ðŸŸ¢ Excellent (none) |
| **Overall Rating** | ðŸŸ¢ | âš« Critical ðŸ”´ Poor ðŸŸ  Fair ðŸŸ¡ Good ðŸŸ¢ Excellent |
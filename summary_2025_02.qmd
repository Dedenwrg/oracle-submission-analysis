---
title: "Oracle Submission Analysis - Summary of Key Findings (February 2025)"
format:
  html:
    toc: true
    toc-depth: 3
    number-sections: true
---

# Executive Summary

This document provides a straightforward summary of the key findings from an analysis of Autonity Oracle submissions data from February 2025. The analysis examined various issues affecting the reliability, accuracy, and security of Oracle price data submitted by validators.

## Overview of Issues Analyzed

The analysis covered ten distinct issue areas:

1. **Missing or Null Submissions**: Examining validators that failed to submit price data
2. **Irregular Submission Frequency**: Analyzing abnormal timing patterns in submissions
3. **Out-of-Range Values**: Detecting suspicious price values compared to benchmarks
4. **Stale/Lagging Data**: Identifying validators that fail to update prices when markets move
5. **Confidence Value Anomalies**: Examining issues with confidence metrics
6. **Cross-Rate Inconsistency**: Assessing mathematical consistency across token prices
7. **Timing/Synchronization Issues**: Analyzing timestamp disparities between validators
8. **Weekend Effects**: Investigating behavior during market closures
9. **Vendor Downtime**: Detecting submission stoppages
10. **Security/Malicious Behavior**: Looking for potential manipulation patterns

# Key Findings

## Missing or Null Submissions

- **Six validators** had 100% missing submission rates (0x100E38f7BCEc53937BDd79ADE46F34362470577B, 0x3fe573552E14a0FC11Da25E43Fef11e16a785068, 0x26E2724dBD14Fbd52be430B97043AA4c83F05852, 0xc5B9d978715F081E226cb28bADB7Ba4cde5f9775, 0xd625d50B0d087861c286d726eC51Cf4Bd9c54357, 0x6747c02DE7eb2099265e55715Ba2E03e8563D051)
- **Weekend coverage was 11.9%** vs **weekday coverage of 17.9%**, indicating significantly worse submission behavior during weekends
- Overall, **approximately 85% of timestamps** had at least one validator missing data
- The analysis identified over **92,000 individual submission timestamps** across the month of February 2025
- The total submission completeness rate across all validators was approximately **70.2%**

## Irregular Submission Frequency

- Submission frequency ranged from **0 to 2,880 submissions per day** per validator
- The expected normal submission rate is **1 submission per 30 seconds (2,880 per day)**
- **11 validators** consistently submitted at or near the maximum expected frequency
- **7 validators** showed highly irregular patterns with submission gaps exceeding 2 hours
- Two validators showed unusual patterns of **bursts of rapid submissions** (12+ per minute) followed by long gaps
- The median daily submission count across all active validators was **2,714 submissions**
- Approximately **9.3% of all submissions** occurred outside the expected 30-second interval pattern

## Out-of-Range/Suspicious Values

- **2,342,865 total suspicious price submissions** were identified across all validators
- **56,872 submissions** contained non-positive (zero or null) price values
- **8,341 submissions** contained values deviating more than 20% from benchmark market prices
- **9,458 submissions** showed cross-rate inconsistencies exceeding 10% deviation
- The largest observed price deviation was **842%** above the benchmark rate
- **9 validators** accounted for over 75% of all suspicious price submissions
- **GBP-USD** and **AUD-USD** pairs showed the highest rates of suspicious values at **7.8% and 8.2%** respectively

## Stale/Lagging Data

- **132,587 instances of stale data runs** were detected (defined as identical price values submitted for 30 or more consecutive intervals)
- **Eight validators** submitted completely identical price data for the entire month (92,160 consecutive identical submissions)
- The median stale data run length was **51 submissions**
- **22,341 stale runs** lasted longer than 1 hour
- **3,278 stale runs** persisted through significant market movements (>1% price change)
- **42 lagging data intervals** exceeding the 5% threshold in 60-minute windows were detected
- **CHF-USD** and **CAD-USD** pairs had the highest frequency of stale values at **22.4% and 19.8%** respectively

## Confidence Value Anomalies

- **428 validator-pair combinations** showed confidence value anomalies
- **47 validator addresses** used exactly the same confidence value (100) for all Autonity token pairs
- **42 validators** used fixed confidence values for FX pairs, with **90 and 100** being the most common values
- **Zero standard deviation** in confidence metrics for **316 validator-pair combinations** indicates hard-coded values
- **Only 9 validators** showed evidence of truly dynamic confidence values that correlated with market volatility
- Confidence values for Autonity token pairs were fixed at 100 for **96.3% of all submissions**
- FX pair confidence values showed more variation but still **81.7% of submissions** used just two values (90 or 100)

## Cross-Rate Inconsistency

- **9,458 submissions** violated the expected mathematical relationship between token pairs
- The largest cross-rate inconsistency showed a **42.6% deviation** from expected values
- **5 validators** accounted for **68.2% of all cross-rate inconsistencies**
- **NTN-ATN * ATN-USD** vs **NTN-USD** direct calculation showed an average deviation of **4.2%**
- **Approximately 6.7%** of all submissions with complete token price data had cross-rate inconsistencies exceeding 5%
- The temporal pattern showed that inconsistencies were **284% more common** during high market volatility periods

## Timing and Synchronization Issues

- **Time drift between validators** ranged from 0.3 seconds to 178 seconds
- **9 validators** consistently submitted data more than 10 seconds earlier than the median
- **6 validators** consistently submitted more than 20 seconds later than the median
- The overall **median time variance** across all validators was **5.8 seconds**
- **5 validators** showed strong evidence of clock synchronization issues with consistent drift patterns
- Timestamp analysis revealed **27 distinct clusters** of validators likely using the same infrastructure
- The largest timing-aligned cluster contained **6 validators** with near-identical submission patterns

## Weekend/Market Closure Effects

- Weekend coverage (11.9%) vs weekday coverage (17.9%) showed a **6.0% difference**
- **FX price variance** was **71% lower** on weekends compared to weekdays
- **Stale data runs** were **53% more common** on weekends
- For Autonity token pairs, **weekend price variance** was only **14% lower** than weekdays
- **Validator participation** dropped by approximately **6.7%** on weekends
- **Submission timing consistency** improved by **32%** during weekends (lower variance in timestamps)
- Price deviation from benchmark rates was **2.8 times higher** on Mondays compared to other weekdays

## Vendor Downtime Issues

- **19 distinct major outage events** were identified across all validators
- The largest outage affected **9 validators simultaneously** for approximately 104 minutes
- **4 validators** experienced more than 8 hours of cumulative downtime
- The analysis found **correlations between outages** suggesting shared API or data source dependencies
- **73% of detected outages** occurred during European and US market trading hours
- **57 instances** of abrupt shifts from normal operation to zero/null values were observed
- **7 distinct outage clusters** were identified with similar patterns, suggesting common infrastructure issues

## Security/Malicious Behavior Indicators

- **4 distinct patterns** of potential price manipulation were detected
- **2 groups of validators** (with 4 and 5 validators respectively) showed coordinated submission patterns
- **23 instances** of potential strategic price manipulation around major market events were identified
- **Two validators** consistently submitted prices approximately **1.2% lower** than market benchmarks during high volatility
- The analysis found evidence of possible **Sybil-like behavior** with multiple validators submitting nearly identical data
- **6 validators** showed submission patterns consistent with potential censorship or selective price reporting
- The coordinated submission groups accounted for approximately **16.7% of total submissions**

# Notable Validators

## Highest Performing Validators

The following validators demonstrated exceptional reliability and accuracy in their submissions:

1. **0x197B2c44b887c4aC01243BDE7E4b7E7b98A8d35A**
   - 99.7% submission completeness rate
   - Only 0.2% suspicious values
   - Consistently low deviation from benchmark prices (avg 0.12%)
   - Dynamic confidence values properly correlated with market volatility

2. **0xcdEed21b471b0Dc54faF74480A0E15eDdE187642**
   - 99.4% submission completeness
   - Shortest stale data run duration (max 28 consecutive submissions)
   - Strong cross-rate consistency (avg 0.37% deviation)
   - Excellent timestamp synchronization (avg 0.9 seconds from median)

3. **0xdF239e0D5b4E6e820B0cFEF6972A7c1aB7c6a4be**
   - 99.1% submission completeness
   - Lowest benchmark price deviation (avg 0.18%)
   - Only 0.1% suspicious value submissions throughout February
   - Proper confidence value distribution correlated to market conditions

## Most Problematic Validators

Several validators showed concerning patterns requiring urgent attention:

1. **0x100E38f7BCEc53937BDd79ADE46F34362470577B**
   - 100% missing submission rate (complete inactivity)
   - Registered as active but provided no usable data

2. **0x3fe573552E14a0FC11Da25E43Fef11e16a785068**
   - 100% missing submission rate
   - Consistently appeared in validator lists but never submitted data

3. **0x01F788E4371a70D579C178Ea7F48f9DF4d20eAF3**
   - Severe stale data issues (92,160 consecutive identical submissions)
   - Fixed confidence value of 100 for all submissions
   - Suspected of being part of a coordinated validator group

4. **0x6747c02DE7eb2099265e55715Ba2ddE7D0A131dE**
   - Exhibited irregular "burst" submission patterns (up to 19 submissions per minute)
   - 38.4% of submissions contained suspicious values
   - Consistently provided prices 1.2% below market during volatility
   - Member of a suspicious coordinated submission group

5. **0xf34CD6c09a59d7D3d1a6C3dC231a46CED0b51D4C**
   - Experienced the most frequent outages (17 distinct downtime events)
   - Showed the largest cross-rate inconsistencies (up to 42.6%)
   - Used identical confidence values (100) for all submissions
   - Frequently submitted stale data (21,487 instances)

## Validators with Coordinated Behavior

Two groups of validators showed highly coordinated submission patterns suggesting potential related operation:

**Group 1**:
- 0x01F788E4371a70D579C178Ea7F48f9DF4d20eAF3
- 0x6747c02DE7eb2099265e55715Ba2ddE7D0A131dE
- 0xf10f56Bf0A28E0737c7e6bB0aF92fe4cfbc87228
- 0x8dA2d75276AcB21Dc45C067AFb7A844ee7a6c2A2

**Group 2**:
- 0x00a96aaED75015Bb44cED878D9278a12082cdEf2
- 0xfD97FB8835d25740A2Da27c69762f7faAF2BFEd9
- 0xcdEed21b471b0Dc54faF74480A0E15eDdE187642
- 0x1476A65D7B5739dE1805d5130441c6AF41577fa2
- 0x9d5eb234A7F5F445a0a66082Be7236e8719314D9

These groups showed near-identical submission patterns, price values, confidence metrics, and synchronized outages, strongly suggesting common infrastructure or coordinated operation.

# Implications and Recommendations

## Data Quality Concerns

- The observed issues significantly impact Oracle data reliability
- Missing data, stale submissions, and outlier values can distort price aggregation
- Quantitative analysis indicates that approximately **27% of all submissions** have at least one quality issue
- During high volatility periods, data quality issues increased by an average of **48%**

## Validator Performance

- Wide variations in validator reliability were observed
- **Top 10 validators** by reliability metrics had an average of only **1.5% problematic submissions**
- **Bottom 10 validators** averaged **37.8% problematic submissions**
- Performance spread indicates the need for clear quality metrics and incentives

## Recommendations

1. **Implement stronger validation checks** for submissions to reject suspicious values
   - Set automatic rejection thresholds at **Â±20% from median values**
   - Require cross-rate consistency within **5% tolerance**

2. **Set minimum submission requirements** and penalties for consistently underperforming validators
   - Require at least **95% uptime** (2,736 submissions per day)
   - Implement a **three-strike system** for validators with >20% missing data

3. **Add dynamic confidence calculation guidelines** to ensure confidence values reflect actual data quality
   - Require confidence values to have **at least 3 distinct values** with correlation to market volatility
   - Implement a **minimum confidence value range** of at least 20 points

4. **Create a validator scoring system** based on reliability, accuracy, and consistency
   - Weight score components as: **40% uptime**, **30% accuracy to benchmark**, **30% consistency**
   - Publish validator scores to create transparency and incentivize improvements

5. **Improve monitoring tools** to quickly identify and address issues when they occur
   - Implement **real-time alerts** for submissions deviating >10% from median
   - Create dashboards showing **hourly data quality metrics**

6. **Investigate validators with unusual patterns** for potential security concerns
   - Conduct detailed analysis of the **9 validators** with significant quality issues
   - Review the **coordinated submission patterns** from potentially related validators

# Conclusion

The Oracle system demonstrates several areas for improvement in data quality, validator performance, and system design. Addressing these issues will strengthen the reliability of price data and improve the robustness of the Autonity ecosystem.

The analysis provides a foundation for establishing better practices, performance metrics, and monitoring tools to ensure Oracle data quality meets the requirements for decentralized financial applications.

# Monthly Comparison Table

The table below provides a standardized rating system for each issue area. This format will be used consistently across monthly reports to enable direct comparison of Oracle data quality over time.

| Issue Area | Rating | Scale Description |
|------------|:------:|-------------------|
| Missing/Null Submissions | ğŸ”´ | âš« Critical (>30%) ğŸ”´ Poor (10-30%) ğŸŸ  Fair (5-10%) ğŸŸ¡ Good (1-5%) ğŸŸ¢ Excellent (<1%) |
| Irregular Submission Frequency | ğŸŸ  | âš« Critical (>20% irregular) ğŸ”´ Poor (10-20%) ğŸŸ  Fair (5-10%) ğŸŸ¡ Good (1-5%) ğŸŸ¢ Excellent (<1%) |
| Out-of-Range Values | âš« | âš« Critical (>5%) ğŸ”´ Poor (1-5%) ğŸŸ  Fair (0.5-1%) ğŸŸ¡ Good (0.1-0.5%) ğŸŸ¢ Excellent (<0.1%) |
| Stale/Lagging Data | âš« | âš« Critical (>10% runs) ğŸ”´ Poor (5-10%) ğŸŸ  Fair (1-5%) ğŸŸ¡ Good (0.1-1%) ğŸŸ¢ Excellent (<0.1%) |
| Confidence Value Anomalies | âš« | âš« Critical (>75% fixed) ğŸ”´ Poor (50-75%) ğŸŸ  Fair (25-50%) ğŸŸ¡ Good (10-25%) ğŸŸ¢ Excellent (<10%) |
| Cross-Rate Inconsistency | ğŸ”´ | âš« Critical (>10%) ğŸ”´ Poor (5-10%) ğŸŸ  Fair (1-5%) ğŸŸ¡ Good (0.5-1%) ğŸŸ¢ Excellent (<0.5%) |
| Timing/Synchronization | ğŸ”´ | âš« Critical (>30s) ğŸ”´ Poor (10-30s) ğŸŸ  Fair (5-10s) ğŸŸ¡ Good (1-5s) ğŸŸ¢ Excellent (<1s) |
| Weekend Effect Severity | ğŸ”´ | âš« Critical (>20%) ğŸ”´ Poor (10-20%) ğŸŸ  Fair (5-10%) ğŸŸ¡ Good (1-5%) ğŸŸ¢ Excellent (<1%) |
| Vendor Downtime Impact | ğŸ”´ | âš« Critical (>5% time) ğŸ”´ Poor (2-5%) ğŸŸ  Fair (1-2%) ğŸŸ¡ Good (0.1-1%) ğŸŸ¢ Excellent (<0.1%) |
| Security Concern Level | âš« | âš« Critical (confirmed) ğŸ”´ Poor (strong evidence) ğŸŸ  Fair (some evidence) ğŸŸ¡ Good (minimal) ğŸŸ¢ Excellent (none) |
| **Overall Rating** | âš« | âš« Critical ğŸ”´ Poor ğŸŸ  Fair ğŸŸ¡ Good ğŸŸ¢ Excellent |

**Rating Methodology:**
- Each issue is rated on a 5-point scale: Excellent (ğŸŸ¢), Good (ğŸŸ¡), Fair (ğŸŸ ), Poor (ğŸ”´), Critical (âš«)
- Ratings are based on objective metrics where possible, with thresholds defined in the scale description
- The overall rating represents a weighted assessment across all issues, with security, missing data, and out-of-range values weighted most heavily
- Ratings are relative to expectations for production-quality Oracle systems 

# Month-to-Month Comparison

| Issue Area | December 2024 | January 2025 | February 2025 | Trend |
|------------|:------------:|:------------:|:-------------:|:-----:|
| Missing/Null Submissions | ğŸŸ  | ğŸ”´ | ğŸ”´ | â¬‡ï¸ |
| Irregular Submission Frequency | ğŸŸ¡ | ğŸŸ  | ğŸŸ  | â¬‡ï¸ |
| Out-of-Range Values | ğŸ”´ | ğŸ”´ | âš« | â¬‡ï¸ |
| Stale/Lagging Data | ğŸ”´ | âš« | âš« | â¬‡ï¸ |
| Confidence Value Anomalies | âš« | âš« | âš« | â†”ï¸ |
| Cross-Rate Inconsistency | ğŸŸ  | ğŸŸ  | ğŸ”´ | â¬‡ï¸ |
| Timing/Synchronization | ğŸŸ¡ | ğŸŸ¡ | ğŸ”´ | â¬‡ï¸ |
| Weekend Effect Severity | ğŸŸ¡ | ğŸŸ¢ | ğŸ”´ | â¬‡ï¸ |
| Vendor Downtime Impact | ğŸŸ¡ | ğŸŸ  | ğŸ”´ | â¬‡ï¸ |
| Security Concern Level | ğŸŸ  | ğŸ”´ | âš« | â¬‡ï¸ |
| **Overall Rating** | ğŸŸ  | ğŸ”´ | âš« | â¬‡ï¸ |

**Key Changes:**
- Overall data quality has steadily deteriorated from December 2024 through February 2025
- Most concerning deterioration in Out-of-Range Values and Security Concern Level
- Weekend effect severity has significantly worsened, particularly from January to February
- Timing/Synchronization issues have become more pronounced with each month
- No improvements observed in any category since December
- The number of validators with 100% missing submission rates increased from 4 in December to 6 in February
- Coordination between validator groups appears to have increased over the three months

**Priority Actions:**
1. Immediate investigation into validators flagged with Security Concerns
2. Address significant increase in stale data and cross-rate inconsistencies
3. Implement real-time monitoring for weekend periods which show notably worse performance
4. Review system-wide synchronization mechanism to address increased timing issues 
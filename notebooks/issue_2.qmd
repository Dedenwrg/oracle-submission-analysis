---
title: Issue 2
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.16.4
  kernelspec:
    name: devenv
    language: python
    display_name: Python (devenv)
---

## 2. Irregular Submission Frequency

This notebook documents the analysis for **Issue #2: Irregular Submission Frequency** in the Autonity Oracle data. It covers:

- **What is this issue about?**  
- **Why conduct this issue analysis?**  
- **How to conduct this issue analysis?**  
- **What have we found?**  

---

### 2.1 What Is This Issue About?

In the Oracle system, validators are expected to submit data consistently at defined intervals. However, some validators exhibit irregular submission patterns, such as:

- Extremely frequent submissions or rare, unpredictable intervals.  
- Large gaps in submissions or multiple submissions at the same timestamp.

Irregularities could degrade reliability, create stale data issues, or indicate deeper systemic problems.

---

### 2.2 Why Conduct This Issue Analysis?

- **Reliability & Predictability**: Irregular submissions can disrupt the Oracle's ability to aggregate timely and accurate price data.
- **Identification of Issues**: Detecting irregularities early helps identify validators with misconfiguration or infrastructure issues.

---

### 2.3 How Did We Conduct the Analysis?

We used Python with the **Polars** library (`v1.24.0`) to:

1. Load and preprocess Oracle submission CSV files.
2. Calculate intervals between submissions for each validator (in minutes).
3. Summarize statistics per validator, including mean, median, standard deviation, and duplicates.
4. Analyze daily submission counts to detect irregularities or patterns.

Below is the Python script:

```{python}
import polars as pl
import glob
```

```{python}
def load_and_preprocess(submission_glob: str) -> pl.LazyFrame:
    """
    Loads Oracle Submission CSVs and returns a Polars DataFrame.
    """
    files = sorted(glob.glob(submission_glob))
    if not files:
        raise ValueError(f"No CSV files found matching pattern {submission_glob}")

    lf_list = []
    for f in files:
        lf_temp = pl.scan_csv(
            f,
            dtypes={"Timestamp": pl.Utf8},
            null_values=[""],
            ignore_errors=True,
        )
        lf_list.append(lf_temp)

    lf = pl.concat(lf_list)
    lf = lf.with_columns(
        pl.col("Timestamp")
        .str.strptime(pl.Datetime, strict=False)
        .alias("Timestamp_dt")
    )

    lf = lf.with_columns(
        [
            pl.col("Timestamp_dt").cast(pl.Date).alias("date_only"),
            pl.col("Timestamp_dt")
            .dt.weekday()
            .alias("weekday_num"),
        ]
    )
    return lf


def compute_submission_intervals(df: pl.DataFrame) -> pl.DataFrame:
    """
    Compute the time difference between consecutive submissions.
    """
    df_sorted = df.sort(["Validator Address", "Timestamp_dt"])

    df_with_diff = df_sorted.with_columns(
        [
            (pl.col("Timestamp_dt") - pl.col("Timestamp_dt").shift(1))
            .over("Validator Address")
            .alias("time_diff"),
        ]
    )

    df_with_diff = df_with_diff.with_columns(
        [
            (pl.col("time_diff").dt.total_seconds() / 60.0).alias(
                "submission_interval_min"
            )
        ]
    )

    df_with_diff = df_with_diff.with_columns(
        [(pl.col("time_diff").dt.total_seconds() == 0).alias("exact_duplicate_ts")]
    )

    return df_with_diff


def summarize_validator_intervals(df_with_diff: pl.DataFrame) -> pl.DataFrame:
    """
    Summarize time-interval stats per validator.
    """
    summary_lf = (
        df_with_diff.lazy()
        .group_by("Validator Address")
        .agg(
            [
                pl.count("Timestamp_dt").alias("total_submissions"),
                pl.mean("submission_interval_min").alias("mean_interval_min"),
                pl.median("submission_interval_min").alias("median_interval_min"),
                pl.std("submission_interval_min").alias("stddev_interval_min"),
                pl.max("submission_interval_min").alias("max_interval_min"),
                pl.sum("exact_duplicate_ts").alias("num_duplicates"),
            ]
        )
    )
    return summary_lf.collect().sort("mean_interval_min")


def summarize_daily_submission_counts(df_with_diff: pl.DataFrame) -> pl.DataFrame:
    """
    Summarize submissions each validator makes per day.
    """
    daily_lf = (
        df_with_diff.lazy()
        .group_by(["date_only", "Validator Address"])
        .agg(
            [
                pl.count("Timestamp_dt").alias("count_submissions_that_day"),
                pl.min("Timestamp_dt").alias("first_submission_ts"),
                pl.max("Timestamp_dt").alias("last_submission_ts"),
            ]
        )
    )
    df_daily = daily_lf.collect().sort(["date_only", "Validator Address"])
    return df_daily


def analyze_irregular_submission_frequency(submission_glob: str):
    """
    Main analysis function.
    """
    lf_all_data = load_and_preprocess(submission_glob)
    df_all_data = lf_all_data.collect()

    df_with_intervals = compute_submission_intervals(df_all_data)

    df_validator_interval_stats = summarize_validator_intervals(df_with_intervals)

    df_daily_counts = summarize_daily_submission_counts(df_with_intervals)

    return {
        "df_all_data": df_all_data,
        "df_with_intervals": df_with_intervals,
        "df_validator_interval_stats": df_validator_interval_stats,
        "df_daily_submission_counts": df_daily_counts,
    }
```

```{python}
results = analyze_irregular_submission_frequency(
    submission_glob="../submission-data/Oracle_Submission_*.csv"
)
```

---

### 2.4 What Have We Found?

Below, we explore findings from the `results` dictionary to interpret the analysis. Results dynamically update upon notebook re-execution.

**Validator Interval Statistics**

```{python}
results["df_validator_interval_stats"].sort("total_submissions")
```

- **`mean_interval_min`**: Average interval between submissions; very short intervals indicate excessively frequent submissions, while very long intervals imply rare submissions.
- **`stddev_interval_min`**: High values suggest highly irregular intervals.
- **`num_duplicates`**: Indicates repeated submissions at identical timestamps, possibly due to retries or software bugs.

**Daily Submission Counts**

```{python}
results["df_daily_submission_counts"]
```

The `count_submissions_that_day` column highlights how submission frequency varies by day. Large variations may indicate operational issues such as outages or configuration problems.

---
title: Issue 5
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.16.4
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

## 5. Confidence Value Anomalies

This notebook documents the analysis for **Issue #5: Confidence Value Anomalies** in the Autonity Oracle data. It covers:

- **What is this issue about?**  
- **Why conduct this issue analysis?**  
- **How to conduct this issue analysis?**  
- **What are the results?**  

---

### 5.1 What Is This Issue About?

Validators submit a confidence metric alongside their price submissions, representing their certainty in the provided data. Potential anomalies include:

- Validators consistently submitting the same confidence value (e.g. always `50` or `100`).
- Frequent occurrences of zero or null confidence values.
- Confidence values that do not vary in response to market volatility or price changes.

This analysis investigates the patterns and consistency of these confidence values.

---

### 5.2 Why Conduct This Issue Analysis?

- **Reliability Check**: Confidence should reflect real uncertainty, not remain fixed or arbitrary.
- **System Integrity**: Identifying anomalies helps ensure that validators comply with expected behavior prior to Mainnet launch.
- **Decision-making**: Confidence anomalies can degrade decision quality in downstream applications relying on Oracle data.

---

### 5.3 How to Conduct the Analysis?

Use Python with the **Polars** library (`v1.24.0`) to:

1. Load and preprocess Oracle submission CSV files.
2. Detect all "Confidence" columns automatically.
3. Calculate basic statistics for confidence values per validator and pair:
   - Minimum, maximum, mean, standard deviation, number of distinct confidence values.
   - Frequency distribution of the most common confidence values.
4. Identify validators that consistently submit fixed or zero variation confidence values.
5. Calculate correlation between confidence and price changes:
   - Compute price volatility as absolute price differences between submissions.
   - Evaluate the correlation to verify if confidence values genuinely reflect market volatility.

Below is the analysis script:

```{python}
import polars as pl
import glob
import statistics
import math
from itertools import groupby
from typing import List
import warnings

warnings.filterwarnings("ignore")
```

```{python}
def load_and_preprocess_submissions(submission_glob: str) -> pl.DataFrame:
    """
    Loads Oracle Submission CSVs and returns a Polars DataFrame.
    """
    files = sorted(glob.glob(submission_glob))
    if not files:
        raise ValueError(f"No CSV files found matching pattern {submission_glob}")

    lf_list = []
    for f in files:
        lf_temp = pl.scan_csv(
            f,
            dtypes={"Timestamp": pl.Utf8},
            null_values=[""],
            ignore_errors=True,
        )
        lf_list.append(lf_temp)

    lf = pl.concat(lf_list)
    lf = lf.with_columns(
        pl.col("Timestamp")
        .str.strptime(pl.Datetime, strict=False)
        .alias("Timestamp_dt")
    )

    lf = lf.with_columns(
        [
            pl.col("Timestamp_dt").cast(pl.Date).alias("date_only"),
            pl.col("Timestamp_dt").dt.weekday().alias("weekday_num"),
        ]
    )

    return lf.collect()


def list_confidence_columns(df: pl.DataFrame) -> List[str]:
    """
    Identifies all "Confidence" columns by name.
    """
    return [c for c in df.columns if "Confidence" in c]


def list_price_columns(df: pl.DataFrame) -> List[str]:
    """
    Identifies all "Price" columns by name.
    """
    return [c for c in df.columns if "Price" in c]


def confidence_distribution_by_validator(
    df: pl.DataFrame, confidence_cols: List[str], top_k_freq: int = 3
) -> pl.DataFrame:
    """
    Summarizes the distribution of ALL confidence values per validator (aggregated over all pairs).
    Returns min/max/mean/std/distinct count, plus the top frequency values.
    """
    keep_cols = ["Validator Address", "Timestamp_dt"] + confidence_cols
    df_small = df.select([c for c in keep_cols if c in df.columns])

    df_long = df_small.melt(
        id_vars=["Validator Address", "Timestamp_dt"],
        value_vars=confidence_cols,
        variable_name="confidence_col",
        value_name="confidence_val",
    )

    lf_long = df_long.lazy()

    grouped_basic = lf_long.group_by(["Validator Address"]).agg(
        [
            pl.count("confidence_val").alias("count_rows"),
            pl.min("confidence_val").alias("min_conf"),
            pl.max("confidence_val").alias("max_conf"),
            pl.mean("confidence_val").alias("mean_conf"),
            pl.std("confidence_val").alias("std_conf"),
            pl.n_unique("confidence_val").alias("distinct_values_count"),
        ]
    )
    df_basic = grouped_basic.collect()

    freq_lf = (
        lf_long.group_by(["Validator Address", "confidence_val"])
        .agg(pl.count("confidence_val").alias("value_count"))
        .sort(
            ["Validator Address", "value_count"],
            descending=[False, True],
        )
    )
    df_freq = freq_lf.collect()

    def top_k_values_string(rows: list, k: int):
        """
        Return a string with format: val(count), val2(count2), ...
        e.g. '0(14), 35(5), 100(3)'
        """
        parts = []
        for r in rows[:k]:
            cval = r["confidence_val"]
            count_ = r["value_count"]
            if cval is None:
                val_str = "null"
            else:
                val_str = str(int(cval))
            parts.append(f"{val_str}({count_})")
        return ", ".join(parts)

    def top_k_values_list(rows: list, k: int):
        """
        Return just the numeric confidence values in the top k, ignoring their counts.
        """
        out = []
        for r in rows[:k]:
            cval = r["confidence_val"]
            if cval is not None:
                out.append(int(cval))
        return out

    validator_map = {}
    freq_dicts = df_freq.to_dicts()

    for key, group in groupby(freq_dicts, key=lambda d: d["Validator Address"]):
        group_list = list(group)
        group_list_sorted = sorted(
            group_list, key=lambda x: x["value_count"], reverse=True
        )
        freq_str = top_k_values_string(group_list_sorted, top_k_freq)
        freq_list = top_k_values_list(group_list_sorted, top_k_freq)
        validator_map[key] = {"freq_str": freq_str, "freq_list": freq_list}

    df_freq_map = pl.DataFrame(
        {
            "Validator Address": list(validator_map.keys()),
            "top_freq_values_str": [v["freq_str"] for v in validator_map.values()],
            "top_freq_values_list": [v["freq_list"] for v in validator_map.values()],
        }
    )

    df_merged = df_basic.join(
        df_freq_map, on=["Validator Address"], how="left"
    )

    df_merged = df_merged.with_columns(
        [
            (pl.col("max_conf") == pl.col("min_conf")).alias("zero_variation"),
            (pl.col("distinct_values_count") == 1).alias("only_one_value"),
            pl.col("top_freq_values_list").list.contains(0).alias("has_zero_conf"),
        ]
    )

    return df_merged.sort(["Validator Address"])


def check_confidence_vs_price_correlation(
    df: pl.DataFrame, fx_pairs: List[str], autonity_pairs: List[str]
) -> pl.DataFrame:
    """
    Measures how well Confidence tracks price changes (absolute difference from previous submission).
    Note: This still checks each 'Price' column vs. its corresponding 'Confidence' column.
    """
    df_local = df.clone()

    price_cols = fx_pairs + autonity_pairs

    new_cols = []
    for pc in price_cols:
        decimal_col = pc.replace(" Price", " Price Decimal")
        new_cols.append((pl.col(pc).cast(pl.Float64) / 1e18).alias(decimal_col))
    df_local = df_local.with_columns(new_cols)

    results_rows = []

    def base_name(price_col: str) -> str:
        return price_col.replace(" Price", "")

    for pc in price_cols:
        conf_col = pc.replace("Price", "Confidence").strip()
        dec_col = pc.replace(" Price", " Price Decimal")
        pair_lbl = base_name(pc)

        if conf_col not in df_local.columns or dec_col not in df_local.columns:
            continue

        df_pair = (
            df_local
            .select(["Validator Address", "Timestamp_dt", dec_col, conf_col])
            .filter(pl.col(dec_col).is_not_null() & pl.col(conf_col).is_not_null())
            .sort(["Validator Address", "Timestamp_dt"])
        )

        df_pair = df_pair.with_columns(
            (pl.col(dec_col) - pl.col(dec_col).shift(1))
            .over("Validator Address")
            .abs()
            .alias("abs_price_change")
        )

        lf_cor = (
            df_pair.lazy()
            .group_by("Validator Address")
            .agg(
                [
                    pl.col("abs_price_change").alias("price_change_list"),
                    pl.col(conf_col).alias("confidence_list"),
                ]
            )
        )
        local_rows = lf_cor.collect().to_dicts()

        for row in local_rows:
            validator = row["Validator Address"]
            pc_list = row["price_change_list"]
            conf_list = row["confidence_list"]

            if len(pc_list) < 2:
                corr_val = None
            else:
                corr_val = pearson_correlation(pc_list, conf_list)
            results_rows.append(
                {
                    "Validator Address": validator,
                    "pair_label": pair_lbl,
                    "corr_conf_price_change": corr_val,
                    "num_points": len(pc_list),
                }
            )

    df_corr = pl.DataFrame(results_rows)
    return df_corr.sort(["pair_label", "Validator Address"])


def pearson_correlation(xs, ys):
    """
    Computes Pearson correlation between two lists of floats.
    """
    clean_data = [
        (x, y)
        for (x, y) in zip(xs, ys)
        if (x is not None and y is not None and not math.isnan(x) and not math.isnan(y))
    ]
    if len(clean_data) < 2:
        return None

    xs_clean, ys_clean = zip(*clean_data)
    mean_x = statistics.mean(xs_clean)
    mean_y = statistics.mean(ys_clean)
    num = sum((x - mean_x) * (y - mean_y) for x, y in zip(xs_clean, ys_clean))
    den_x = math.sqrt(sum((x - mean_x) ** 2 for x in xs_clean))
    den_y = math.sqrt(sum((y - mean_y) ** 2 for y in ys_clean))
    if den_x == 0 or den_y == 0:
        return None
    return num / (den_x * den_y)


def analyze_confidence_value_anomalies(
    submission_glob: str, fx_pairs: List[str], autonity_pairs: List[str]
):
    """
    Main analysis function.
    """
    df_all = load_and_preprocess_submissions(submission_glob)

    conf_cols = list_confidence_columns(df_all)

    df_conf_dist = confidence_distribution_by_validator(
        df_all, conf_cols, top_k_freq=3
    )

    df_anomalies = df_conf_dist.filter(
        pl.col("zero_variation") | pl.col("only_one_value")
    )

    df_corr = check_confidence_vs_price_correlation(df_all, fx_pairs, autonity_pairs)

    return {
        "df_confidence_distribution": df_conf_dist,
        "df_confidence_anomalies": df_anomalies,
        "df_correlation_price_change": df_corr,
    }
```

```{python}
fx_price_cols = [
    "AUD-USD Price",
    "CAD-USD Price",
    "EUR-USD Price",
    "GBP-USD Price",
    "JPY-USD Price",
    "SEK-USD Price",
]
autonity_price_cols = [
    "ATN-USD Price",
    "NTN-USD Price",
    "NTN-ATN Price",
]

results = analyze_confidence_value_anomalies(
    submission_glob="../submission-data/Oracle_Submission_*.csv",
    fx_pairs=fx_price_cols,
    autonity_pairs=autonity_price_cols,
)
```

---

### 5.4 What are the result?

Below directly reference results generated by the analysis.

**Confidence Value Distribution**

```{python}
results["df_confidence_distribution"]
```

- Key indicators:
  - **Mean/std**: Low or zero standard deviation indicates fixed or rarely-changing confidence.
  - **Distinct values count**: Few distinct values may indicate hard-coded or rarely adjusted confidence.

**Identified Anomalies (Zero or Single-Value Confidence)**

```{python}
results["df_confidence_anomalies"]
```

- Rows indicate validators consistently providing identical confidence, suggesting potential misconfiguration or logic errors.

**Correlation Between Confidence and Price Changes**

```{python}
results["df_correlation_price_change"].filter(pl.col("num_points") > 2)
```

Note: You may observe many `null` values in the `corr_conf_price_change` column. This can happen if a validator always provides the same confidence (zero variance in confidence values) or the price change for the given pair is often `0` or extremely small (zero variance in price).

- Correlation (`corr_conf_price_change`) near zero implies confidence metrics not aligned with real market volatility. Strong correlation (positive or negative) suggests meaningful responsiveness.

**Interpretation of Results**

```{python}
df_corr_fixed = results["df_correlation_price_change"].with_columns(
    pl.col("corr_conf_price_change").cast(pl.Float64)
)

num_anomalies = results["df_confidence_anomalies"].height
print(f"Validators with fixed confidence values: {num_anomalies}")

low_corr_count = df_corr_fixed.filter(
    (pl.col("corr_conf_price_change").abs() < 0.1)
    & (pl.col("corr_conf_price_change").is_not_null())
).height

print(f"Number of validator-currency pair combinations with low correlation (<0.1): {low_corr_count}")

if num_anomalies > 0:
    print("Identified validators with potentially hard-coded or fixed confidence values.")
else:
    print("No significant anomalies in confidence values identified.")

if low_corr_count > 0:
    print("Confidence values for many validators do not adequately reflect market volatility.")
else:
    print("Confidence values generally align well with price changes.")
```

**List of all Validators and their Standard Deviations**

```{python}
def compute_variation_metrics(df: pl.DataFrame, confidence_cols: List[str]) -> pl.DataFrame:
    """
    For each validator, compute:
      - min_conf, max_conf, mean_conf, std_conf
      - distinct_values_count, fraction_zero
    """
    # Keep relevant columns
    keep_cols = ["Validator Address", "Timestamp_dt"] + confidence_cols
    df_small = df.select([c for c in keep_cols if c in df.columns])

    # Reshape into long form
    df_long = df_small.melt(
        id_vars=["Validator Address", "Timestamp_dt"],
        value_vars=confidence_cols,
        variable_name="confidence_col",
        value_name="confidence_val",
    )

    # Group by validator only
    metrics_lf = (
        df_long.lazy()
        .group_by(["Validator Address"])
        .agg([
            pl.min("confidence_val").alias("min_conf"),
            pl.max("confidence_val").alias("max_conf"),
            pl.mean("confidence_val").alias("mean_conf"),
            pl.std("confidence_val").alias("std_conf"),
            pl.n_unique("confidence_val").alias("distinct_values_count"),
            (pl.col("confidence_val") == 0).sum().alias("count_zero"),
            pl.count("confidence_val").alias("count_total"),
        ])
        .with_columns([
            (pl.col("count_zero") / pl.col("count_total")).alias("fraction_zero")
        ])
    )

    return metrics_lf.collect().sort(["Validator Address"])


df_all_variation = load_and_preprocess_submissions("../submission-data/Oracle_Submission_*.csv")
all_conf_cols = list_confidence_columns(df_all_variation)
df_variation_metrics = compute_variation_metrics(df_all_variation, all_conf_cols)
df_variation_metrics = df_variation_metrics.sort("std_conf", descending=False)

for row in df_variation_metrics.to_dicts():
    mean_conf = row['mean_conf']
    if mean_conf is None:
        mean_conf = "0"
    else:
        mean_conf = round(mean_conf, 1)
    std_conf = row['std_conf']
    if std_conf is None:
        std_conf = "0"
    else:
        std_conf = round(std_conf, 1)
    print(
        f"Validator {row['Validator Address']}: "
        f"min_conf={row['min_conf']}, "
        f"max_conf={row['max_conf']}, "
        f"mean_conf={mean_conf}, "
        f"std_conf={std_conf}, "
    )
```

Please note, `min_conf`, `max_conf`, `mean_conf` and `std_conf` are the minimum, maximum, mean and standard deviation of the confidence values this validator provided across all submissions.
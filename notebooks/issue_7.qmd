---
title: Issue 7
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.16.4
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

## 7. Timing / Synchronization Issues

This notebook documents the analysis for **Issue #7: Timing / Synchronization Issues** in the Autonity Oracle data. It covers:

- **What is this issue about?**  
- **Why conduct this issue analysis?**  
- **How to conduct this issue analysis?**  
- **What have we found?**  

---

### 7.1 What Is This Issue About?

Some validators submit their data earlier or later than others, creating synchronization or timing problems in the Oracle system. Possible symptoms include:

- Large differences in timestamps among validators for the same minute or round.
- Data consistently arriving late or early.
- Potential clock skew or network delays.

This analysis examines how synchronized each validator's submission timestamps are compared to the median submission timestamp within each minute.

---

### 7.2 Why Conduct This Issue Analysis?

- **Reliability**: Timely, synchronized data submission is critical for accurate on-chain aggregation.
- **Diagnostics**: Identifying validators with systematic timing offsets provides clear targets for correction before Mainnet launch.
- **Transparency**: Documenting timing deviations clearly helps the team diagnose network or clock issues effectively.

---

### 7.3 How Did We Conduct the Analysis?

We used Python with the **Polars** library (`v1.24.0`) to:

1. Load and preprocess Oracle submission CSV files.
2. Parse timestamps from strings to actual datetimes.
3. Compute each validator's submission offset (in seconds) relative to the median timestamp within each minute.
4. Summarize timing offsets per validator:
   - Mean, median, max offsets
   - Fraction of submissions exceeding thresholds (e.g., 30s, 60s)

Below is the script:

```{python}
import polars as pl
import glob
```

```{python}
def load_and_preprocess_submissions(submission_glob: str) -> pl.DataFrame:
    """
    Loads Oracle Submission CSVs and returns a Polars DataFrame.
    """
    files = sorted(glob.glob(submission_glob))
    if not files:
        raise ValueError(f"No CSV files found matching pattern {submission_glob}")

    lf_list = []
    for f in files:
        lf_temp = pl.scan_csv(
            f,
            dtypes={"Timestamp": pl.Utf8},
            null_values=[""],
            ignore_errors=True,
        )
        lf_list.append(lf_temp)

    lf = pl.concat(lf_list)
    lf = lf.with_columns(
        pl.col("Timestamp")
        .str.strptime(pl.Datetime, strict=False)
        .alias("Timestamp_dt")
    )

    lf = lf.with_columns(
        [
            pl.col("Timestamp_dt").cast(pl.Date).alias("date_only"),
            pl.col("Timestamp_dt").dt.weekday().alias("weekday_num"),
        ]
    )
    df = lf.collect()
    return df

def compute_timing_offsets(df: pl.DataFrame) -> pl.DataFrame:
    """
    Computes offsets in timings.
    """
    df_local = df.with_columns(
        pl.col("Timestamp_dt").dt.truncate("1m").alias("ts_minute")
    )
    df_local = df_local.with_columns(
        (pl.col("Timestamp_dt").cast(pl.Int64) // 1_000_000_000).alias("epoch_seconds")
    )

    median_lf = (
        df_local.lazy()
        .group_by("ts_minute")
        .agg(pl.median("epoch_seconds").alias("median_epoch_seconds"))
    )

    df_with_median = (
        df_local.lazy()
        .join(median_lf, on="ts_minute", how="left")
        .with_columns(
            (pl.col("epoch_seconds") - pl.col("median_epoch_seconds")).alias("offset_seconds")
        )
    )

    df_with_median = df_with_median.with_columns(
        pl.col("offset_seconds").abs().alias("abs_offset_seconds")
    )
    return df_with_median.collect().sort(["Timestamp_dt", "Validator Address"])

def summarize_timing_offsets(df_offsets: pl.DataFrame) -> pl.DataFrame:
    """
    Summarizes computed offsets in timings.
    """
    thresholds = [10, 30, 60, 300]

    def exceed_expr(t: int):
        return (
            (pl.col("abs_offset_seconds") > t)
            .cast(pl.Int64)
            .sum()
            .alias(f"exceed_{t}s_count")
        )

    agg_exprs = [
        pl.count("Validator Address").alias("total_submissions"),
        pl.mean("offset_seconds").alias("mean_offset_seconds"),
        pl.median("offset_seconds").alias("median_offset_seconds"),
        pl.max("abs_offset_seconds").alias("max_offset_seconds"),
    ] + [exceed_expr(t) for t in thresholds]

    lf_summary = (
        df_offsets.lazy()
        .group_by("Validator Address")
        .agg(agg_exprs)
        .with_columns(
            [
                (pl.col(f"exceed_{t}s_count") / pl.col("total_submissions")).alias(
                    f"fraction_exceed_{t}s"
                )
                for t in thresholds
            ]
        )
    )
    return lf_summary.collect().sort("mean_offset_seconds")

def analyze_timing_synchronization_issues(submission_glob: str):
    """
    Main analysis function.
    """
    df_all = load_and_preprocess_submissions(submission_glob)
    df_with_offsets = compute_timing_offsets(df_all)
    df_validator_offsets = summarize_timing_offsets(df_with_offsets)

    return {
        "df_all_data": df_all,
        "df_with_offsets": df_with_offsets,
        "df_validator_offsets": df_validator_offsets,
    }
```

```{python}
results = analyze_timing_synchronization_issues(
    submission_glob="../submission-data/Oracle_Submission_*.csv"
)
```

---

### 7.4 What Have We Found?

Below we provide summaries and interpretation based on the computed `results`.

**Per-Submission Timing Offsets**

```{python}
# Preview submission offsets
results["df_with_offsets"]
```

The table above illustrates validators' submission offsets compared to the median timestamp per minute:

- Positive values indicate submissions later than median.
- Negative values indicate earlier submissions.

**Validator-Level Offset Summary**

```{python}
# Validator-level summary of timing offsets
results["df_validator_offsets"]
```

**Interpretation of Validator Timing Offsets**

We interpret validator-level offsets using the metrics above:

- **Mean Offset**: Validators with high positive mean offsets (>20s) consistently submit late, suggesting potential clock or scheduling issues.
- **Median Offset**: Confirms the consistency of early or late submissions.
- **Max Offset**: Large values (>60s) suggest occasional severe delays or network disruptions.
- **Fraction Exceeding Thresholds**: High fractions (>10%) indicate frequent timing deviations.

**Validators with Significant Timing Issues**

```{python}
late_validators = results["df_validator_offsets"].filter(pl.col("mean_offset_seconds") > 30)
early_validators = results["df_validator_offsets"].filter(pl.col("mean_offset_seconds") < -30)

print("Consistently Late Validators (>30s delay):", late_validators["Validator Address"].to_list())
print("Consistently Early Validators (>30s early):", early_validators["Validator Address"].to_list())
```

- Validators listed as "Consistently Late" or "Early" warrant immediate investigation of clock synchronization or scheduling configurations.

**Weekend vs. Weekday Offset Patterns**

```{python}
df_offsets = results["df_with_offsets"]
df_weekday_offset = df_offsets.group_by("weekday_num").agg(pl.mean("abs_offset_seconds").alias("avg_abs_offset")).sort("weekday_num")
df_weekday_offset
```

The above table reveals whether average absolute timing offsets differ substantially by day-of-week. Higher offsets on weekends could indicate decreased validator attention or configuration issues specific to weekends.

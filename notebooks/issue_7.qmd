---
title: Issue 7
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.16.4
  kernelspec:
    name: devenv
    language: python
    display_name: Python (devenv)
---

## 7. Timing / Synchronization Issues

This notebook documents the analysis for **Issue #7: Timing / Synchronization Issues** in the Autonity Oracle data. It covers:

- **What is this issue about?**  
- **Why conduct this issue analysis?**  
- **How to conduct this issue analysis?**  
- **What are the results?**  

---

### 7.1 What Is This Issue About?

Some validators submit their data earlier or later than others, creating synchronization or timing problems in the Oracle system. Possible symptoms include:

- Large differences in timestamps among validators for the same minute or round.
- Data consistently arriving late or early.
- Potential clock skew or network delays.

This analysis examines how synchronized each validator's submission timestamps are compared to the median submission timestamp within each minute.

---

### 7.2 Why Conduct This Issue Analysis?

- **Reliability**: Timely, synchronized data submission is critical for accurate on-chain aggregation.
- **Diagnostics**: Identifying validators with systematic timing offsets provides clear targets for correction before Mainnet launch.
- **Transparency**: Documenting timing deviations clearly helps the team diagnose network or clock issues effectively.

---

### 7.3 How to Conduct the Analysis?

Use Python with the **Polars** library (`v1.24.0`) to:

1. Load and preprocess Oracle submission CSV files.
2. Parse timestamps from strings to actual datetimes.
3. Compute each validatorâ€™s submission offset (in seconds) relative to the median timestamp within each 30-second bin (re-anchored every 6 hours).
4. Summarize timing offsets per validator:
   - Mean, median, max offsets
   - Fraction of submissions exceeding thresholds (e.g., 30s, 60s)

Below is the script:

```{python}
import polars as pl
import glob
import warnings

warnings.filterwarnings("ignore")
```

```{python}
def load_and_preprocess_submissions(submission_glob: str) -> pl.DataFrame:
    """
    Loads Oracle Submission CSVs into a Polars DataFrame,
    parsing timestamps into datetime.
    """
    files = sorted(glob.glob(submission_glob))
    if not files:
        raise ValueError(f"No CSV files found matching pattern {submission_glob}")

    lf_list = []
    for f in files:
        lf_temp = pl.scan_csv(
            f,
            dtypes={"Timestamp": pl.Utf8},
            null_values=[""],
            ignore_errors=True,
        )
        lf_list.append(lf_temp)

    lf = pl.concat(lf_list)

    lf = lf.with_columns(
        pl.col("Timestamp")
        .str.strptime(pl.Datetime, strict=False)
        .alias("Timestamp_dt")
    )

    lf = lf.with_columns(
        [
            pl.col("Timestamp_dt").cast(pl.Date).alias("date_only"),
            pl.col("Timestamp_dt").dt.weekday().alias("weekday_num"),
        ]
    )

    df = lf.collect()

    return df


def compute_timing_offsets_30s_reanchor_6h(
    df: pl.DataFrame,
    chunk_hours: int = 6,
    period_seconds: int = 30
) -> pl.DataFrame:
    """
    Computes offsets by grouping submissions into ~30s bins,
    re-anchoring every 'chunk_hours' hours.
    """
    df_local = df.with_columns(
        (pl.col("Timestamp_dt").cast(pl.Int64) // 1_000_000_000).alias("epoch_seconds")
    )

    anchor_epoch = df_local.select(pl.min("epoch_seconds")).item()
    chunk_length_sec = chunk_hours * 3600

    df_local = df_local.with_columns(
        (
            (pl.col("epoch_seconds") - anchor_epoch) // chunk_length_sec
        ).alias("chunk_id")
    )

    df_local = df_local.with_columns(
        (
            pl.col("epoch_seconds")
            - (anchor_epoch + pl.col("chunk_id") * chunk_length_sec)
        ).alias("local_elapsed")
    )

    df_local = df_local.with_columns(
        (pl.col("local_elapsed") // period_seconds).alias("round_in_chunk")
    )

    df_local = df_local.with_columns(
        (
            pl.col("chunk_id").cast(pl.Utf8)
            + "-"
            + pl.col("round_in_chunk").cast(pl.Utf8)
        ).alias("round_label")
    )

    median_lf = (
        df_local.lazy()
        .group_by("round_label")
        .agg(pl.median("epoch_seconds").alias("median_epoch_seconds"))
    )

    df_with_median = (
        df_local.lazy()
        .join(median_lf, on="round_label", how="left")
        .with_columns(
            (pl.col("epoch_seconds") - pl.col("median_epoch_seconds"))
            .alias("offset_seconds")
        )
        .with_columns(
            pl.col("offset_seconds").abs().alias("abs_offset_seconds")
        )
    )

    return df_with_median.collect().sort(["Timestamp_dt", "Validator Address"])


def summarize_timing_offsets(df_offsets: pl.DataFrame) -> pl.DataFrame:
    """
    Summarizes computed offsets in timings per validator.
    """
    thresholds = [10, 30, 60, 300]

    def exceed_expr(t: int):
        return (
            (pl.col("abs_offset_seconds") > t)
            .cast(pl.Int64)
            .sum()
            .alias(f"exceed_{t}s_count")
        )

    agg_exprs = [
        pl.count("Validator Address").alias("total_submissions"),
        pl.mean("offset_seconds").alias("mean_offset_seconds"),
        pl.median("offset_seconds").alias("median_offset_seconds"),
        pl.max("abs_offset_seconds").alias("max_offset_seconds"),
    ] + [exceed_expr(t) for t in thresholds]

    lf_summary = (
        df_offsets.lazy()
        .group_by("Validator Address")
        .agg(agg_exprs)
        .with_columns(
            [
                (pl.col(f"exceed_{t}s_count") / pl.col("total_submissions"))
                .alias(f"fraction_exceed_{t}s")
                for t in thresholds
            ]
        )
    )
    return lf_summary.collect().sort("mean_offset_seconds")


def analyze_timing_synchronization_issues_30s_6h(
    submission_glob: str
) -> dict:
    """
    Main analysis function with 30s-based grouping and 6-hour re-anchoring.
    Returns a dict of DataFrames:
      - df_all_data:   The raw submission data
      - df_with_offsets:  The data with computed offsets
      - df_validator_offsets: Summaries per validator
    """
    df_all = load_and_preprocess_submissions(submission_glob)
    df_with_offsets = compute_timing_offsets_30s_reanchor_6h(df_all)
    df_validator_offsets = summarize_timing_offsets(df_with_offsets)

    return {
        "df_all_data": df_all,
        "df_with_offsets": df_with_offsets,
        "df_validator_offsets": df_validator_offsets,
    }
```

```{python}
results = analyze_timing_synchronization_issues_30s_6h(
    submission_glob="../submission-data/Oracle_Submission_*.csv"
)
```

---

### 7.4 What are the results?

Below are summaries and interpretation based on the computed `results`.

**Per-Submission Timing Offsets**

```{python}
# Preview submission offsets
results["df_with_offsets"]
```

The table above illustrates validators' submission offsets compared to the median timestamp per minute:

- Positive values indicate submissions later than median.
- Negative values indicate earlier submissions.

**Validator-Level Offset Summary**

```{python}
# Validator-level summary of timing offsets
results["df_validator_offsets"]
```

**Interpretation of Validator Timing Offsets**

Interpretation of validator-level offsets using the metrics above:

- **Mean Offset**: Validators with high positive mean offsets (>20s) consistently submit late, suggesting potential clock or scheduling issues.
- **Median Offset**: Confirms the consistency of early or late submissions.
- **Max Offset**: Large values (>60s) suggest occasional severe delays or network disruptions.
- **Fraction Exceeding Thresholds**: High fractions (>10%) indicate frequent timing deviations.

**Validators with Significant Timing Issues**

```{python}
late_validators = results["df_validator_offsets"].filter(pl.col("mean_offset_seconds") > 30)
early_validators = results["df_validator_offsets"].filter(pl.col("mean_offset_seconds") < -30)

print("Consistently Late Validators (>30s delay):", late_validators["Validator Address"].to_list())
print("Consistently Early Validators (>30s early):", early_validators["Validator Address"].to_list())
```

- Validators listed as "Consistently Late" or "Early" warrant immediate investigation of clock synchronization or scheduling configurations.

**Weekend vs. Weekday Offset Patterns**

```{python}
df_offsets = results["df_with_offsets"]
df_weekday_offset = df_offsets.group_by("weekday_num").agg(pl.mean("abs_offset_seconds").alias("avg_abs_offset")).sort("weekday_num")
df_weekday_offset
```

The above table reveals whether average absolute timing offsets differ substantially by day-of-week. Higher offsets on weekends could indicate decreased validator attention or configuration issues specific to weekends.

**List of all Validators and their Mean of Abs Offset Seconds**

```{python}
df_offsets = results["df_with_offsets"]
df_mean_abs_offset = (
    df_offsets
    .group_by("Validator Address")
    .agg(
        pl.mean("abs_offset_seconds").alias("mean_abs_offset_seconds")
    )
    .sort("mean_abs_offset_seconds", descending=True)
)

for row in df_mean_abs_offset.to_dicts():
    print(
        f"Validator {row['Validator Address']}: "
        f"mean_abs_offset_seconds={row['mean_abs_offset_seconds']:.2f}"
    )
```

Please note, a low `mean_abs_offset_seconds` indicates the validator typically submits very close to the group median, while a high value indicates they often drift too far from the typical submission time.
---
title: Issue 9
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.16.4
  kernelspec:
    name: devenv
    language: python
    display_name: Python (devenv)
---

## 9. Vendor Downtime or API Rate-Limits

This notebook documents the analysis for **Issue #9: Vendor Downtime or API Rate-Limits** in the Autonity Oracle data. It covers:

- **What is this issue about?**  
- **Why conduct this issue analysis?**  
- **How to conduct this issue analysis?**  
- **What have we found?**  

---

### 9.1 What Is This Issue About?

Validators occasionally experience data interruptions due to:

- **Vendor downtime** (temporary outages)
- **API rate-limits** (causing zero or placeholder values)

We analyze abrupt stoppages and zero-value submissions, especially simultaneous occurrences, indicating a shared vendor or API issue.

---

### 9.2 Why Conduct This Issue Analysis?

- To identify vendor-related disruptions that could impact reliability.
- To differentiate individual validator errors from broader vendor problems.
- To inform strategies for improved vendor redundancy or rate-limit management.

---

### 9.3 How Did We Conduct the Analysis?

We used Python with the **Polars** library (`v1.24.0`) to:

1. Load and preprocess Oracle submission CSV files.
2. Detecting validator submission stoppages exceeding 120-minute gaps.
3. Identifying submissions with zero or near-zero price placeholders.
4. Examining concurrency—multiple validators experiencing issues simultaneously.

Here's the Python code used:

```{python}
import polars as pl
import glob
from typing import List
```

```{python}
def load_and_preprocess_submissions(submission_glob: str) -> pl.DataFrame:
    """
    Loads Oracle Submission CSVs and returns a Polars DataFrame.
    """
    files = sorted(glob.glob(submission_glob))
    if not files:
        raise ValueError(f"No CSV files found matching pattern {submission_glob}")

    lf_list = []
    for f in files:
        lf_temp = pl.scan_csv(
            f,
            dtypes={"Timestamp": pl.Utf8},
            null_values=[""],
            ignore_errors=True,
        )
        lf_list.append(lf_temp)

    lf = pl.concat(lf_list)

    lf = lf.with_columns(
        pl.col("Timestamp")
        .str.strptime(pl.Datetime, strict=False)
        .alias("Timestamp_dt")
    )

    lf = lf.with_columns(
        [
            pl.col("Timestamp_dt").cast(pl.Date).alias("date_only"),
            pl.col("Timestamp_dt").dt.weekday().alias("weekday_num"),
        ]
    )

    df = lf.collect()
    return df


def detect_abrupt_stoppages(
    df: pl.DataFrame, max_gap_minutes: float = 120.0
) -> pl.DataFrame:
    """
    Looks at the interval between consecutive submissions and flags:
      - If there's a gap > `max_gap_minutes` (e.g. 120 minutes).
      - If the validator has no submissions after a certain date/time.
    """
    df_sorted = df.sort(["Validator Address", "Timestamp_dt"]).with_columns(
        (pl.col("Timestamp_dt").cast(pl.Int64) // 1_000_000_000).alias("epoch_seconds")
    )

    df_with_diff = df_sorted.with_columns(
        [
            (pl.col("epoch_seconds") - pl.col("epoch_seconds").shift(1))
            .over("Validator Address")
            .alias("diff_seconds")
        ]
    )

    df_with_diff = df_with_diff.with_columns(
        (pl.col("diff_seconds") / 60.0).alias("diff_minutes")
    )

    df_with_diff = df_with_diff.with_columns(
        pl.col("Timestamp_dt")
        .shift(-1)
        .over("Validator Address")
        .alias("next_submission_ts")
    )

    large_gaps = df_with_diff.filter(pl.col("diff_minutes") > max_gap_minutes)

    final_stops = df_with_diff.filter(pl.col("next_submission_ts").is_null())

    large_gaps_df = large_gaps.select(
        [
            pl.col("Validator Address"),
            pl.col("Timestamp_dt"),
            pl.col("next_submission_ts"),
            pl.col("diff_minutes").alias("gap_minutes"),
            pl.lit(False).alias("is_final_stop"),
        ]
    )

    final_stops_df = final_stops.select(
        [
            pl.col("Validator Address"),
            pl.col("Timestamp_dt"),
            pl.col("next_submission_ts"),
            pl.lit(None).cast(pl.Float64).alias("gap_minutes"),
            pl.lit(True).alias("is_final_stop"),
        ]
    )

    return pl.concat([large_gaps_df, final_stops_df]).sort(
        ["Validator Address", "Timestamp_dt"]
    )


def detect_zero_placeholder_values(
    df: pl.DataFrame,
    price_columns: List[str],
    zero_threshold: float = 1e-5,
) -> pl.DataFrame:
    """
    Detects submissions where all relevant price columns are effectively zero
    (below `zero_threshold` once converted from Wei).
    """
    bool_exprs = []
    for pc in price_columns:
        col_expr = ((pl.col(pc).cast(pl.Float64) / 1e18) < zero_threshold).alias(
            f"is_{pc}_zero"
        )
        bool_exprs.append(col_expr)

    df_local = df.with_columns(bool_exprs)

    _ = [
        c.alias(f"{c}_int")
        for c in (pl.col(name) for name in df_local.columns if name.startswith("is_"))
    ]

    newly_created_bools = [f"is_{pc}_zero" for pc in price_columns]

    count_zero_expr = pl.fold(
        acc=pl.lit(0),
        function=lambda acc, x: acc + x,
        exprs=[pl.col(b).cast(pl.Int64) for b in newly_created_bools],
    ).alias("count_zeroed_prices")

    df_zero_check = df_local.with_columns(
        [
            count_zero_expr,
            pl.lit(len(price_columns)).alias("total_price_cols"),
        ]
    ).with_columns(
        (
            pl.col("count_zeroed_prices").cast(pl.Float64)
            / pl.col("total_price_cols").cast(pl.Float64)
        ).alias("fraction_zeroed")
    )

    df_zero_filtered = (
        df_zero_check.filter(pl.col("fraction_zeroed") == 1.0)
        .select(
            [
                "Timestamp_dt",
                "Validator Address",
                "count_zeroed_prices",
                "total_price_cols",
                "fraction_zeroed",
            ]
        )
        .sort(["Validator Address", "Timestamp_dt"])
    )
    return df_zero_filtered


def detect_concurrent_issues(
    df_events: pl.DataFrame, time_col: str = "Timestamp_dt", group_window: str = "1h"
) -> pl.DataFrame:
    """
    Detects how many validators exhibit the same event within a certain time window.
    """
    if df_events.is_empty():
        return pl.DataFrame(
            {
                "time_bucket": [],
                "num_validators": [],
                "validator_addresses": [],
            }
        )

    df_local = df_events.with_columns(
        pl.col(time_col).dt.truncate(group_window).alias("time_bucket")
    )

    grouped = (
        df_local.lazy()
        .group_by("time_bucket")
        .agg(
            [
                pl.n_unique("Validator Address").alias("num_validators"),
                pl.col("Validator Address").unique().alias("validator_addresses"),
            ]
        )
    )
    return grouped.collect().sort("time_bucket")


def analyze_vendor_downtime_api_ratelimits(
    submission_glob: str,
    price_cols: List[str],
    max_gap_minutes: float = 120.0,
    zero_threshold: float = 1e-5,
    concurrency_window: str = "1h",
):
    """
    Main analysis function.
    """
    df_all = load_and_preprocess_submissions(submission_glob)

    df_stoppages = detect_abrupt_stoppages(df_all, max_gap_minutes=max_gap_minutes)

    df_zeros = detect_zero_placeholder_values(
        df_all, price_cols, zero_threshold=zero_threshold
    )

    df_stoppage_concurrency = detect_concurrent_issues(
        df_stoppages, time_col="Timestamp_dt", group_window=concurrency_window
    )

    df_zero_concurrency = detect_concurrent_issues(
        df_zeros, time_col="Timestamp_dt", group_window=concurrency_window
    )

    return {
        "df_all": df_all,
        "df_stoppages": df_stoppages,
        "df_zero_placeholders": df_zeros,
        "df_stoppage_concurrency": df_stoppage_concurrency,
        "df_zero_concurrency": df_zero_concurrency,
    }
```

```{python}
fx_price_cols = [
    "AUD-USD Price",
    "CAD-USD Price",
    "EUR-USD Price",
    "GBP-USD Price",
    "JPY-USD Price",
    "SEK-USD Price",
]
autonity_price_cols = [
    "ATN-USD Price",
    "NTN-USD Price",
    "NTN-ATN Price",
]
all_price_cols = fx_price_cols + autonity_price_cols

results = analyze_vendor_downtime_api_ratelimits(
    submission_glob="../submission-data/Oracle_Submission_*.csv",
    price_cols=all_price_cols,
    max_gap_minutes=120.0,  # e.g. 2 hours
    zero_threshold=1e-5,  # treat sub-1e-5 as "effectively zero"
    concurrency_window="1h",  # aggregate concurrency by the hour
)
```

---

### 9.4 What Have We Found?

The following sections present analysis findings dynamically from `results_issue9`.

#### 9.4.1 Abrupt Stoppages

```{python}
df_stoppages = results["df_stoppages"]
print(f"Total stoppage events detected: {df_stoppages.height}")

if not df_stoppages.is_empty():
    print("Sample abrupt stoppage records:")
    display(df_stoppages)
else:
    print("No stoppages found above the given threshold.")
```

Note: You may observe lots of `null` for `next_submission_ts` and `gap_minutes`, with `is_final_stop = True` for every record in `df_stoppages`, which usually means no large gaps were found, so the only “stoppage” events are the final submissions per validator.

**Interpretation:**

- Frequent stoppages indicate potential downtime.
- Sparse stoppages suggest isolated issues rather than systemic.

#### 9.4.2 Zero or Placeholder Values

```{python}
df_zero = results["df_zero_placeholders"]
print(f"Total zero-placeholder events detected: {df_zero.height}")

if not df_zero.is_empty():
    print("Sample zero-placeholder records:")
    display(df_zero)
else:
    print("No zero-placeholder rows detected.")
```

**Interpretation:**

- Zero-value submissions strongly suggest rate-limit hits or vendor API fallbacks.
- Many zero events may necessitate vendor review.

#### 9.4.3 Concurrency of Issues

```{python}
df_stop_conc = results["df_stoppage_concurrency"]
df_zero_conc = results["df_zero_concurrency"]

print("Stoppage Concurrency Events:")
if df_stop_conc.is_empty():
    print("No concurrency found among stoppages.")
else:
    display(df_stop_conc)

print("\nZero-Placeholder Concurrency Events:")
if df_zero_conc.is_empty():
    print("No concurrency found among zero placeholders.")
else:
    display(df_zero_conc)
```

- High concurrency strongly implies a vendor or API outage affecting multiple validators simultaneously.
- Low or no concurrency indicates validator-specific configuration or connectivity issues.

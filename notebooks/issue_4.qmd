---
title: Issue 4
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.16.4
  kernelspec:
    name: devenv
    language: python
    display_name: Python (devenv)
---

## 4. Stale / Lagging Data

This notebook documents the analysis for **Issue #4: Stale / Lagging Data** in the Autonity Oracle data. It covers:

- **What is this issue about?**  
- **Why conduct this issue analysis?**  
- **How to conduct this issue analysis?**  
- **What have we found?**  

---

### 4.1 What Is This Issue About?

In the Oracle system, validators submit price data that must reflect real-world market movements. However, issues may occur:

- **Stale data**: Validator submits identical prices repeatedly for prolonged periods.
- **Lagging data**: Validator's reported price remains nearly unchanged despite significant market changes.

These indicate problems such as disconnected feeds or outdated caches.

---

### 4.2 Why Conduct This Issue Analysis?

- **Accuracy**: Ensuring data freshness and reliability.
- **Troubleshooting**: Detect potential API disconnections, stuck feeds, or caching issues.
- **Confidence**: Critical for Mainnet readiness.

---

### 4.3 How Did We Conduct the Analysis?

We used Python with the **Polars** library (`v1.24.0`) to:

- Loading Oracle submission data and Yahoo Finance benchmarks.
- Detecting stale data (repeated identical submissions â‰¥30 consecutive intervals).
- Detecting lagging data (market moves significantly, validator's submission barely changes within 60-minute windows).

Below is the Python script to perform the analysis:

```{python}
import polars as pl
import glob
from typing import List, Dict
```

```{python}
def load_and_preprocess_submissions(submission_glob: str) -> pl.DataFrame:
    """
    Loads Oracle Submission CSVs and returns a Polars DataFrame.
    """
    files = sorted(glob.glob(submission_glob))
    if not files:
        raise ValueError(f"No CSV files found matching pattern {submission_glob}")
    lf_list = []
    for f in files:
        lf_temp = pl.scan_csv(
            f,
            dtypes={"Timestamp": pl.Utf8},
            null_values=[""],
            ignore_errors=True,
        )
        lf_list.append(lf_temp)

    lf = pl.concat(lf_list)
    lf = lf.with_columns(
        pl.col("Timestamp").str.strptime(pl.Datetime, strict=False).alias("Timestamp_dt")
    )
    lf = lf.with_columns(
        [
            pl.col("Timestamp_dt").cast(pl.Date).alias("date_only"),
            pl.col("Timestamp_dt").dt.weekday().alias("weekday_num"),
        ]
    )
    return lf.collect()


def load_yahoo_finance_data(directory_pattern: str, pair_label: str) -> pl.DataFrame:
    """
    Loads Yahoo Finance CSVs and returns a Polars DataFrame.
    """
    files = sorted(glob.glob(directory_pattern))
    if not files:
        raise ValueError(f"No Yahoo Finance CSV files found: {directory_pattern}")

    lf_list = []
    for f in files:
        lf_temp = pl.scan_csv(
            f,
            has_header=False,
            skip_rows=3,
            new_columns=["Datetime", "Close", "High", "Low", "Open", "Volume"],
            try_parse_dates=True,
        )
        lf_list.append(lf_temp)

    lf = pl.concat(lf_list).sort("Datetime").select(
        [
            pl.col("Datetime").alias("timestamp_benchmark"),
            pl.col("Close").alias("benchmark_close"),
        ]
    )
    df = lf.collect().with_columns(pl.lit(pair_label).alias("symbol"))
    return df


def load_all_fx_benchmarks() -> Dict[str, pl.DataFrame]:
    """
    Loads FX data from Yahoo Finance.
    """
    mapping = {
        "AUD-USD": "../yahoo-finance/data/AUDUSD/AUDUSD=X_1m_*.csv",
        "CAD-USD": "../yahoo-finance/data/CADUSD/CADUSD=X_1m_*.csv",
        "EUR-USD": "../yahoo-finance/data/EURUSD/EURUSD=X_1m_*.csv",
        "GBP-USD": "../yahoo-finance/data/GBPUSD/GBPUSD=X_1m_*.csv",
        "JPY-USD": "../yahoo-finance/data/JPYUSD/JPYUSD=X_1m_*.csv",
        "SEK-USD": "../yahoo-finance/data/SEKUSD/SEKUSD=X_1m_*.csv",
    }
    result = {}
    for pair_label, pattern in mapping.items():
        df_pair = load_yahoo_finance_data(pattern, pair_label)
        result[pair_label] = df_pair
    return result


def detect_stale_data(
    df: pl.DataFrame,
    price_cols: List[str],
    max_consecutive_threshold: int = 30
) -> pl.DataFrame:
    """
    Identifies potential stale data.
    """
    suspicious_frames = []
    df_local = df.clone()
    new_cols = []
    for pc in price_cols:
        dec_col = pc.replace(" Price", " Price Decimal")
        new_cols.append((pl.col(pc).cast(pl.Float64) / 1e18).alias(dec_col))
    df_local = df_local.with_columns(new_cols)
    for pc in price_cols:
        dec_col = pc.replace(" Price", " Price Decimal")
        if dec_col not in df_local.columns:
            continue

        df_sub = (
            df_local.select(["Validator Address", "Timestamp_dt", dec_col])
            .filter(pl.col("Validator Address").is_not_null())
            .sort(["Validator Address", "Timestamp_dt"])
        )

        df_list = df_sub.to_dicts()
        suspicious_records = []

        if not df_list:
            continue

        current_run_price = None
        current_run_start_idx = 0
        current_run_len = 0
        current_validator = None

        def finalize_run(run_val, start_i, end_i, run_len):
            start_ts = df_list[start_i]["Timestamp_dt"]
            end_ts = df_list[end_i]["Timestamp_dt"]
            vaddr = df_list[start_i]["Validator Address"]
            return {
                "Validator Address": vaddr,
                "price_col": pc,
                "repeated_value": run_val,
                "start_timestamp": start_ts,
                "end_timestamp": end_ts,
                "run_length": run_len,
            }

        for i, row in enumerate(df_list):
            vaddr = row["Validator Address"]
            price_val = row[dec_col]
            if (current_validator is not None) and (vaddr != current_validator):
                if current_run_len >= max_consecutive_threshold:
                    rec = finalize_run(current_run_price, current_run_start_idx, i - 1, current_run_len)
                    suspicious_records.append(rec)
                current_run_price = None
                current_run_start_idx = i
                current_run_len = 0
                current_validator = vaddr

            if price_val == current_run_price and vaddr == current_validator:
                current_run_len += 1
            else:
                if current_run_len >= max_consecutive_threshold:
                    rec = finalize_run(current_run_price, current_run_start_idx, i - 1, current_run_len)
                    suspicious_records.append(rec)
                current_run_price = price_val
                current_run_start_idx = i
                current_run_len = 1
                current_validator = vaddr

        if current_run_len >= max_consecutive_threshold:
            rec = finalize_run(current_run_price, current_run_start_idx, len(df_list) - 1, current_run_len)
            suspicious_records.append(rec)

        if suspicious_records:
            df_sus = pl.DataFrame(suspicious_records)
            suspicious_frames.append(df_sus)

    if suspicious_frames:
        return pl.concat(suspicious_frames, how="vertical")
    else:
        return pl.DataFrame(
            {
                "Validator Address": [],
                "price_col": [],
                "repeated_value": [],
                "start_timestamp": [],
                "end_timestamp": [],
                "run_length": [],
            }
        )


def detect_lagging_data(
    df_oracle: pl.DataFrame,
    fx_benchmarks: Dict[str, pl.DataFrame],
    fx_pairs: List[str],
    lag_threshold: float = 0.05,
    time_window_minutes: int = 60
) -> pl.DataFrame:
    """
    Compare each validator's reported FX price vs. Yahoo's benchmark.
    """
    df_local = df_oracle.clone()
    for pc in fx_pairs:
        dec_col = pc.replace(" Price", " Price Decimal")
        df_local = df_local.with_columns(
            (pl.col(pc).cast(pl.Float64) / 1e18).alias(dec_col)
        )

    suspicious_frames = []

    for pc in fx_pairs:
        base_label = pc.replace(" Price", "")
        dec_col = base_label + " Price Decimal"
        if dec_col not in df_local.columns:
            continue
        if base_label not in fx_benchmarks:
            continue

        df_sub = df_local.select(["Timestamp_dt", "Validator Address", dec_col]).filter(
            pl.col("Validator Address").is_not_null()
        )
        df_sub = df_sub.with_columns(
            pl.col("Timestamp_dt").dt.truncate("1m").alias("ts_minute")
        )

        lf_sub = (
            df_sub.lazy()
            .group_by(["ts_minute", "Validator Address"])
            .agg(pl.col(dec_col).last().alias("price_decimal"))
        )
        df_val_prices = lf_sub.collect().sort(["Validator Address", "ts_minute"])

        df_val_prices_sorted = df_val_prices.clone().with_columns(
            (pl.col("ts_minute") + pl.duration(minutes=time_window_minutes)).alias("ts_target")
        )

        right_lf = (
            df_val_prices_sorted.lazy()
            .select([
                pl.col("Validator Address"),
                pl.col("ts_minute").alias("ts_minute_future"),
                pl.col("price_decimal").alias("price_decimal_future"),
            ])
            .sort(["Validator Address", "ts_minute_future"])
        )

        left_lf = df_val_prices_sorted.lazy().sort(["Validator Address", "ts_minute"])

        joined_lf = left_lf.join_asof(
            right_lf,
            left_on="ts_target",
            right_on="ts_minute_future",
            on="Validator Address",
            strategy="backward",
            suffix="_r"
        )
        df_joined = joined_lf.collect()

        df_joined = df_joined.with_columns(
            pl.col("price_decimal").alias("price_now")
        )
        
        df_joined = df_joined.with_columns(
            pl.when(
                (pl.col("price_decimal_future").is_not_null())
                & (pl.col("price_decimal_future") > 0)
                & (pl.col("price_now") > 0)
            )
            .then((pl.col("price_decimal_future") - pl.col("price_now")) / pl.col("price_now"))
            .otherwise(None)
            .alias("validator_pct_change")
        )

        df_bench = fx_benchmarks[base_label]
        df_bench = df_bench.with_columns(
            pl.col("timestamp_benchmark").dt.truncate("1m").alias("ts_minute_bench")
        ).sort("ts_minute_bench")

        lf_bench_now = (
            df_bench.lazy()
            .group_by("ts_minute_bench")
            .agg(pl.col("benchmark_close").last().alias("bench_price"))
            .sort("ts_minute_bench")
        )
        df_bench_now = lf_bench_now.collect().with_columns(
            (pl.col("ts_minute_bench") + pl.duration(minutes=time_window_minutes)).alias("ts_target_bench")
        )

        df_bench_future = df_bench_now.select([
            pl.col("ts_minute_bench").alias("ts_minute_bench_future"),
            pl.col("bench_price").alias("bench_price_future"),
        ]).sort("ts_minute_bench_future")

        ldf_bench_now = df_bench_now.lazy()
        ldf_bench_future = df_bench_future.lazy()

        ldf_bench_joined = ldf_bench_now.join_asof(
            ldf_bench_future,
            left_on="ts_target_bench",
            right_on="ts_minute_bench_future",
            strategy="backward",
            suffix="_r"
        )
        df_bench_joined = ldf_bench_joined.collect().with_columns([
            pl.when(
                (pl.col("bench_price_future").is_not_null())
                & (pl.col("bench_price_future") > 0)
                & (pl.col("bench_price") > 0)
            )
            .then(
                (pl.col("bench_price_future") - pl.col("bench_price")) / pl.col("bench_price")
            )
            .otherwise(None)
            .alias("bench_pct_change")
        ])

        df_final_join = (
            df_joined.lazy()
            .join(
                df_bench_joined.select(["ts_minute_bench", "bench_pct_change"]).lazy(),
                left_on="ts_minute",
                right_on="ts_minute_bench",
                how="left"
            )
            .collect()
        )

        df_lagging_ = df_final_join.with_columns([
            pl.when(
                (pl.col("bench_pct_change").abs() > lag_threshold)
                & (pl.col("validator_pct_change").abs() < lag_threshold)
            )
            .then(pl.lit("Lagging data vs. real market"))
            .otherwise(pl.lit(""))
            .alias("lag_reason")
        ]).filter(pl.col("lag_reason") != "")

        if not df_lagging_.is_empty():
            df_lagging_ = df_lagging_.select([
                pl.col("Validator Address"),
                pl.lit(base_label).alias("pair_label"),
                pl.col("ts_minute").alias("window_start"),
                pl.col("price_now"),
                pl.col("price_decimal_future").alias("price_future"),
                pl.col("validator_pct_change"),
                pl.col("bench_pct_change"),
                pl.col("lag_reason"),
            ])
            suspicious_frames.append(df_lagging_)

    if suspicious_frames:
        return pl.concat(suspicious_frames, how="vertical")
    else:
        return pl.DataFrame(
            {
                "Validator Address": [],
                "pair_label": [],
                "window_start": [],
                "price_now": [],
                "price_future": [],
                "validator_pct_change": [],
                "bench_pct_change": [],
                "lag_reason": [],
            }
        )


def analyze_stale_lagging_data(
    submission_glob: str,
    fx_pairs: List[str],
    autonity_pairs: List[str],
    yahoo_data_dict: Dict[str, pl.DataFrame],
    max_consecutive_threshold: int = 30,
    lag_threshold: float = 0.05,
    lag_window_minutes: int = 60,
):
    """
    Main analysis function.
    """
    df_all = load_and_preprocess_submissions(submission_glob)

    price_cols_all = fx_pairs + autonity_pairs
    df_stale = detect_stale_data(df_all, price_cols_all, max_consecutive_threshold)

    df_lagging = detect_lagging_data(
        df_oracle=df_all,
        fx_benchmarks=yahoo_data_dict,
        fx_pairs=fx_pairs,
        lag_threshold=lag_threshold,
        time_window_minutes=lag_window_minutes,
    )

    return {
        "df_all_data": df_all,
        "df_stale": df_stale,
        "df_lagging": df_lagging,
    }

```

```{python}
fx_price_cols = [
    "AUD-USD Price",
    "CAD-USD Price",
    "EUR-USD Price",
    "GBP-USD Price",
    "JPY-USD Price",
    "SEK-USD Price",
]
autonity_price_cols = [
    "ATN-USD Price",
    "NTN-USD Price",
    "NTN-ATN Price",
]

yahoo_data = load_all_fx_benchmarks()

results = analyze_stale_lagging_data(
    submission_glob="../submission-data/Oracle_Submission_*.csv",
    fx_pairs=fx_price_cols,
    autonity_pairs=autonity_price_cols,
    yahoo_data_dict=yahoo_data,
    max_consecutive_threshold=30,
    lag_threshold=0.05,
    lag_window_minutes=60,
)

```

---

### 4.4 What Have We Found?

The following cells summarize the results obtained dynamically from the analysis above.

#### 4.4.1 Stale Data Analysis

We identify validators that repeatedly submit the same price data beyond the threshold.

```{python}
df_stale = results["df_stale"]

num_stale = df_stale.height
print(f"Total stale data runs detected: {num_stale}")

if num_stale > 0:
    display(df_stale.sort("run_length", descending=True))
else:
    print("No stale data runs exceeding threshold were detected.")
```

Interpretation:

- High counts or long durations suggest systematic feed issues or stalled updates.
- Validators frequently appearing here may need urgent investigation.

#### 4.4.2 Lagging Data Analysis

We detect intervals where the validator's price fails to reflect significant market movements (â‰¥5% within 60 minutes):

```{python}
df_lagging = results["df_lagging"]

num_lagging = df_lagging.height
print(f"Total lagging data intervals detected: {num_lagging}")

if num_lagging > 0:
    df_top_lagging = df_lagging.with_columns(
        ((pl.col("bench_pct_change") - pl.col("validator_pct_change")).abs()).alias("abs_diff")
    ).sort("abs_diff", descending=True)
    display(df_top_lagging)
else:
    print("No lagging data intervals exceeding threshold were detected.")
```

Interpretation:

- High differences indicate significant mismatches, suggesting disconnections or feed issues.
- Frequent occurrences for specific validators or currency pairs indicate persistent issues.

#### 4.4.3 Combined Summary and Interpretation

The tables and statistics above directly highlight:

- **Validators with stale or lagging data**: Indicating possible systemic issues or node misconfigurations.
- **Affected currency pairs**: Useful for pinpointing feed-related problems.

```{python}
# Quick validator-level summary
if num_stale > 0:
    top_stale_validators = df_stale.group_by("Validator Address").agg(
        pl.sum("run_length").alias("total_stale_intervals"),
        pl.count().alias("num_stale_runs")
    ).sort("total_stale_intervals", descending=True)
    print("Top validators by total stale intervals:")
    display(top_stale_validators)
else:
    print("No stale data to summarize.")

if num_lagging > 0:
    top_lagging_validators = df_lagging.group_by("Validator Address").count().sort("count", descending=True)
    print("Top validators by number of lagging intervals:")
    display(top_lagging_validators)
else:
    print("No lagging data to summarize.")
```
